---
title: 贝叶斯分类器
date: 2018-11-07 17:40:33
categories:
- Machine Learning
tags:
- Theory
- Bayes
- Gibbs Sampling
mathjax: true
---

**贝叶斯无敌**

参考：

>西瓜书第7章 贝叶斯分类器

## 1. 贝叶斯决策论

贝叶斯决策论是概率框架下实施决策的基本方法。对于分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。

以多分类任务为例，假设有$N$种可能的类别标记，即$Y =\{ c_1, c_2,...,c_N \}$，$\lambda_{ij}$是将一个真实标记为$c_j$的样本误分类为$c_i$所产生的损失。那么在样本$\boldsymbol{x}$上的条件风险为

$$
R(c_i|\boldsymbol{x}) = \sum^N_{j=1}\lambda_{ij}P(c_j|\boldsymbol{x})
$$

我们的任务是寻找一个判定准则：$h:X \rightarrow Y$以最小化总体风险

$$
R(h) = \mathbb{E}_\boldsymbol{x}[R(h(\boldsymbol{x})|\boldsymbol{x})]
$$

<!-- more -->

显然，为了最小化总体风险，只需要在每个样本上选择那个能使条件风险$R(c|\boldsymbol{x})$最小的类别标记，即

$$
h^*(\boldsymbol{x}) = \underset{c \in Y}{\arg\min} R(c|\boldsymbol{x})
$$

举个例子

{% asset_img gmm02.png 混合高斯分布 %}

对于上图来说，假如我们已知点的分布属于两个高斯分布，对应两个类别，我们如何判断图中某点的类别呢？为了最小化总体风险，选择该点在两个高斯分布中概率最高的那个对应的种类。在这个过程中我们只计算了该点的两个高斯分布概率，这就是贝叶斯决策论的基本思想。

如果目标是最小化分类错误率，则$\lambda_{ij}$可写为

$$
\lambda_{ij} = \left\{\begin{matrix}
0, \quad if \quad i=j\\ 
1, \quad otherwise
\end{matrix}\right.
$$

此时条件风险

$$
R(c|\boldsymbol{x}) = 1-P(c|\boldsymbol{x})
$$

于是，最小化分类错误率的贝叶斯最优分类器为

$$
h^*(\boldsymbol{x}) = \underset{c\in Y}{\arg \max} P(c|\boldsymbol{x})
$$

即对每个样本选择能使后验概率$P(c|\boldsymbol{x})$最大的分类标记。

对应上面图中描述的，我们事先知道数据的分布才能很容易解决这个问题，但是实际上，数据分布通常难以直接获得，即$P(c|\boldsymbol{x})$后验概率难以直接获取。
从这个角度来看，机器学习所要实现的是基于有限的样本尽可能准确地估计出后验概率。
大体来说，主要有两种策略：

* 判别式模型：给定$\boldsymbol{x}$，可通过直接建模$P(c|\boldsymbol{x})$来预测$c$，决策树、BP神经网络、SVM等；
* 生成式模型：先对联合概率分布$P(\boldsymbol{x}, c)$建模，然后再由此获得$P(c|\boldsymbol{x})$。

对生成式模型来说，基于贝叶斯定理

$$
P(c|\boldsymbol{x}) = \frac{P(c)P(\boldsymbol{x}|c)}{P(\boldsymbol{x})}
$$

其中$P(c)$是类先验概率；$P(\boldsymbol{x}|c)$是样本相对于类标记$c$的类条件概率，或称为似然；$P(\boldsymbol{x})$是用于归一化的证据因子（与类别标记无关，一般无需计算）。

类先验概率$P(c)$表达了样本空间各类样本所占的比例，根据大数定律，可以通过各类样本出现的频率来进行估计。

对类条件概率$P(\boldsymbol{x}|c)$来说，它涉及到关于$\boldsymbol{x}$所有属性的联合概率，直接根据样本出现的频率来估计会很困难。例如，假设样本的$d$个属性都是二值的，则样本空间将有$2^d$种可能取值，在现实应用中，这个值往往大于训练样本数$m$，也就是说，很多样本取值没有在训练集种出现，不可直接用频率估计$P(\boldsymbol{x}|c)$。

## 2. 极大似然估计

估计类条件概率的一般策略是

1. 假设样本属于某个类别；
2. 基于假设的类别对概率分布的参数进行估计；
3. 根据更新后的参数重新计算样本的类别

重复2和3直到收敛。

事实上，概率模型的训练过程就是参数估计过程。

令$D_c$表示训练集中第$c$类样本组成的集合，假设这些样本是独立同分布的，则参数$\boldsymbol{\theta}_c$对于数据集$D_c$的似然是

$$
P(D_c|\boldsymbol{\theta}_c) = \prod_{\boldsymbol{x}\in D_c}P(\boldsymbol{x}|\boldsymbol{\theta}_c)
$$

对$\boldsymbol{\theta}_c$进行极大似然估计，就是去寻找能最大化似然$P(D_c|\boldsymbol{\theta}_c)$的参数$\boldsymbol{\theta}_c$，即在参数所有可能取值中找到能使数据出现可能性最大的参数。

连乘容易造成下溢，通常使用对数似然

$$
LL(\boldsymbol{\theta}_c) = \log P(D_c|\boldsymbol{\theta}_c)
\\
=\sum_{\boldsymbol{x}\in D_c}\log P(\boldsymbol{x}|\boldsymbol{\theta}_c)
$$

显然极大似然估计为

$$
\hat{\boldsymbol{\theta}}_c = \underset{\boldsymbol{\theta}_c}{\arg\max} LL(\boldsymbol{\theta}_c)
$$

例如，上图中的高斯分布，假设概率密度函数$p(\boldsymbol{x}|c)\sim N(\boldsymbol{\mu}_c,\boldsymbol{\sigma}^2_c)$，则参数的极大似然估计为

$$
\hat{\boldsymbol{\mu}}_c = \frac{1}{|D_c|}\sum_{\boldsymbol{x}\in D_c}\boldsymbol{x}
\\
\hat{\boldsymbol{\sigma}}^2_c =\frac{1}{|D_c|}\sum_{\boldsymbol{x}\in D_c}(\boldsymbol{x} - \hat{\boldsymbol{\mu}}_c)(\boldsymbol{x} - \hat{\boldsymbol{\mu}}_c)^T
$$

我们能做出如此假设的前提是，在可视化的条件下我们发现数据很大程度是属于两个高斯分布的。然而事实上，我们无法知道数据的分布，可能数据的维度很高，可能数据量很大，如果“假设”出错，那么我们的结果可能会是失败的。

## 3. 朴素贝叶斯分类器

朴素贝叶斯分类是为了解决类条件概率$P(\boldsymbol{x}|c)$无法直接获取的问题，采用了属性条件独立性假设：对已知类别，假设所有属性相互独立，即假设每个属性独立地对分类结果发生影响。

基于上述假设

$$
P(c|\boldsymbol{x}) = \frac{P(c)P(\boldsymbol{x}|c)}{P(\boldsymbol{x})} = \frac{P(c)}{P(\boldsymbol{x})} \prod^d_{i=1}P(x_i|c)
$$

$d$为属性数目，$x_i$为样本在第$i$个属性上的取值。

对于所有类别来说，基于上式的贝叶斯判定准则为

$$
h_{nb}(\boldsymbol{x}) = \underset{c\in Y}{\arg\max}P(c)\prod^d_{i=1}P(x_i|c)
$$

这就是朴素贝叶斯分类器的表达式。

令$D_c$表示训练集$D中第$c$类样本组成的集合，若有充足的独立同分布样本，则

$$
P(c) = \frac{|D_c|}{|D|}
$$

对离散属性而言，令$D_{c,x_i}$表示$D_c$中在第$i$个属性上取值为$x_i$的样本组成的集合，则

$$
P(x_i|c) = \frac{|D_{c, x_i}|}{|D_c|}
$$

显然，连乘过程中如果出现了0，那么整个概率都变成了0，我们需要进行修正，令$N$表示训练集$D$中可能的类别数，$N_i$表示第$i$个属性可能的取值数

$$
\hat{P}(c) = \frac{|D_c|+1}{|D|+N}
\\
\hat{P}(x_i|c) = \frac{|D_{c, x_i}|+1}{|D_c|+N}
$$

## 4. 半朴素贝叶斯分类器

朴素贝叶斯分类器基于属性条件独立性假设，但是现实任务中这个假设往往很难成立。考虑一部分属性间的相互依赖信息，这就是半朴素贝叶斯分类器的基本思想。独依赖估计ODE是半朴素贝叶斯分类器最常用的一种策略，即假设每个属性在类别之外最多仅依赖于一个其他属性

$$
P(c|\boldsymbol{x}) \propto P(c)\prod^d_{i=1}P(x_i|c, pa_i)
$$

其中$pa_i$为属性$x_i$所依赖的属性，称为父属性。如何确定父属性？

---

最直接的做法是假设所有属性都依赖于同一个属性，称为超父，通过交叉验证等模型选择方法确定超父属性，由此形成了SPODE方法。

{% asset_img spode.jpg %}

---

TAN则是在最大带权生成树算法的基础上，通过以下步骤将属性间依赖关系约简

1. 计算任意两个属性之间的条件互信息$I(x_i,x_j|y) = \sum_{x_i,x_j|y}\log \frac{P(x_i,x_j|c)}{P(x_i|c)P(x_j|c)}$
2. 以属性为结点构建完全图，任意两个结点之间边的权重设为$I(x_i, x_j|y)$；
3. 构建此完全图的最大带权生成树，挑选根变量，将边置为有向；
4. 加入类别结点$y$，增加从$y$到每个属性的有向边。

通过最大生成树算法，TAN实际上仅保留了强相关属性之间的依赖性。

---

AODE是一种基于集成学习机制、更为强大的独依赖分类器。AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够数据支撑的SPODE集成起来作为最终结果

$$
P(c|\boldsymbol{x}) \propto \underset{|D_{x_i}|\geqslant m'}{\sum^d_{i=1}} P(c, x_i)\prod^d_{j=1}P(x_j|c, x_j)
$$

其中$D_{x_i}$是在第$i$个属性上取值为$x_i$的样本的集合，$m'$为阈值常数。

$$
\hat{P}(c,x_i) = \frac{|D_{c, x_i}|+1}{|D|+N\times N_i}
\\
\hat{P}(x_j|c, x_i) = \frac{|D_{c, x_i, x_j}|+1}{|D_{c,x_i}|+N_j}
$$

$D_{c,x_i}$是类别为$c$且在第$i$个属性上取值为$x_i$的样本集合，$D_{c, x_i,x_j}$是类别为$c$且在第$i$和$j$个属性上取值分别为$x_i$和$x_j$的样本集合。

---

同样的，假设父属性为$k$个，即高阶依赖，那么所需要的训练样本数将以指数级增加。

## 5. 贝叶斯网

贝叶斯网亦称为信念网，它借助有向无环图来刻画属性之间的依赖关系，并使用条件概率表来描述属性的联合概率分布。




## 6. EM算法


