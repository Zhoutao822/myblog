---
title: 语音识别-HMM
date: 2018-11-16 15:33:47
categories:
- speech recognition
tags:
- theory
- HMM
mathjax: true
---

参考：《解析深度学习-语音识别实践》第3章 隐马尔可夫模型及其变体

## 1. 马尔可夫链

马尔可夫链是一种离散状态的马尔可夫序列，也是一般性马尔可夫序列的特殊形式。
马尔可夫链的状态空间具有离散和有限性：$q_t \in \{ s^{(j)},j = 1,2,...,N\}$。每一个离散值都与马尔可夫链中的一个状态相关。

一个马尔可夫链$\boldsymbol{q}_1^T = q_1, q_2,...,q_T$，可被转移概率完全表示，定义为

$$
P(q_t=s^{(j)}|q_{t-1}=s^{(i)}) = a_{ij}(t) \quad i,j=1,2,...,N
$$

以及初始状态分布概率。如果这些转移概率与时间$t$无关，则得到齐次马尔可夫链。

（齐次），马尔可夫链的转移概率通常能方便地表示为矩阵形式：

$$
\boldsymbol{A} = [a_{ij}], \quad 其中a_{ij} \geqslant 0 \quad \forall i,j ; \sum^N_{j=1}a_{ij}=1 \quad \forall i
$$

$\boldsymbol{A}$称为马尔可夫链的转移矩阵。给定马尔可夫链的转移概率，则状态输出概率

$$
p_j(t) = P[q_t = s^{(j)}]
$$

很容易得到，递归计算

$$
p_i(t+1) = \sum^N_{j=1} a_{ji}p_j(t), \quad \forall i
$$

如果马尔可夫链的状态占有分布式渐进收敛：$p_i(t) \rightarrow \pi(q^{(i)})$，当$t \rightarrow \infty$，我们称$p(s^{(i)})$为马尔可夫链的一个稳态分布。对有稳态分布的马尔可夫链来说，他的转移概率$a_{ij}$必须满足：

$$
\bar{\pi}(s^{(i)}) = \sum^N_{j=1}a_{ji}\bar{\pi}(s^{(j)}), \quad \forall i
$$

马尔可夫链的稳态分布在马尔可夫链蒙特卡洛（MCMC）方法中起着重要作用。这些方法用来模拟（即采样）任意复杂的分布函数，使其能执行很多复杂的统计推断和学习任务，否则这些任务运算困难。MCMC方法的理论基础是马尔可夫链到它的稳态分布$\bar{\pi}(s^{(i)})$的渐进收敛。也就是说，无论初始分布如何，马尔可夫链之于$\bar{\pi}(s^{(i)})$是渐进无偏的。因此，为了从任意的复合分布$p(s)$中采样，可以通过设计合适的转移概率构造一个马尔可夫链，使它的稳态分布为$\bar{\pi}(s) = p(s)$。

三种马尔可夫链的性质：

1. 马尔可夫链的状态时长是一个指数或几何级分布：$p_i(d)=C(a_{ii})^{d-1}$，其中归一化常数为$C = 1 - a_{ii}$；
2. 平均状态时长为

$$
\bar{d}_i = \sum^{\infty}_{d=1}dp_i(d) = \sum^{\infty}_{d=1}(1-a_{ii})(a_{ii})^{d-1} = \frac{1}{1-a_{ii}}
$$

3. 对任意一个服从马尔可夫链的观察序列，若它对应有限长度状态序列$\boldsymbol{q}^T_1$，则其概率很容易计算，是所有马尔可夫链的转移概率的乘积：$P(\boldsymbol{q}^T_1) = \bar{\pi}_{q_1}\prod^{T-1}_{t=1}a_{q_tq_{t+1}}$，其中$\bar{\pi}_{s_1}$使当$t=1$时的初始状态输出概率。

## 2. 序列与模型

马尔可夫链的每一种状态与一种输出（观察值或事件）一一对应，没有随机性。

隐马尔可夫序列在各个状态引入一种随机性，用一个观测的概率分布与每一个状态对应，而不是一个确定的事件或观察值。

{% asset_img hmm.png 隐马尔可夫模型%}

即我们观察到的是$x_i$，而观测值仅由隐藏的状态$y_i$决定，而当前时刻的状态$y_i$仅由前一时刻的状态$y_{i-1}$决定。

### 2.1 隐马尔可夫模型的性质

1. 齐次马尔可夫链的转移概率矩阵$\boldsymbol{A}=[a_{ij}] \quad i,j = 1,2,...,N$，其中共有$N$个状态

$$
a_{ij} = P(q_t = j | q_{t-1}=i) \quad i,j = 1,2,...,N
$$

2. 马尔可夫链的初始概率：$\pi = [\pi_i] \quad i= 1,2,...,N$，其中$\pi_i = P(q_1 = i)$。
3. 观察概率分布为$P(\boldsymbol{o}_t|s^{(i)}) \quad i=1,2,...,N$。若$\boldsymbol{o}_t$是离散的，每个状态对应的概率分布用来描述观察$\{ \boldsymbol{v}_1, \boldsymbol{v}_2,...,\boldsymbol{v}_K\}$的概率：

$$
b_i(k) = P(\boldsymbol{o}_t = \boldsymbol{v}_t|q_t = i) \quad i=1,2,...,N
$$

若观察概率分布是连续的，那么概率密度函数PDF中的参数$\Lambda_i$代表HMM状态$i$的特性。

---
在语音处理问题中，我们用HMM下的PDF来描述连续观察向量（$\boldsymbol{o}_t \in \mathbb{R}^D$）的概率分布，其中多元混合高斯分布是最成功、应用最广泛的PDF：

$$
b_i(\boldsymbol{o}_t) = \sum^M_{m=1}\frac{c_{i,m}}{(2\pi)^{D/2}|\boldsymbol{\Sigma}_{i,m}|^{1/2}}\exp[-\frac{1}{2}(\boldsymbol{o}_t - \boldsymbol{\mu}_{i,m})^T\boldsymbol{\Sigma}^{-1}_{i,m}(\boldsymbol{o}_t - \boldsymbol{\mu}_{i,m})]
$$

在混合高斯HMM中，参数集$\Lambda_i$包括混合权重成分$c_{i,m}$，高斯分布均值向量$\boldsymbol{\mu}_{i,m} \in \mathbb{R}^D$与协方差矩阵$\boldsymbol{\Sigma}_{i,m} \in \mathbb{R}^{D \times D}$。

有了模型参数后，高斯HMM可以看作是一个观察值序列$\boldsymbol{o}_t，t =1,2,...,T$的生成器。在$t$时刻，数据根据公式

$$
\boldsymbol{o}_t = \boldsymbol{\mu}_i + \boldsymbol{r}_t(\boldsymbol{\Sigma}_i)
$$

生成，其中时刻$t$的状态$i$取决于马尔可夫链的演变，受$a_{ij}$影响，且

$$
\boldsymbol{r}_t(\boldsymbol{\Sigma}_i) = N(0, \boldsymbol{\Sigma}_i)
$$
是均值为0、依赖序号$i$的IID（独立同分布）的高斯剩余序列。

---

有一个对平稳状态的HMM的简单扩展，可以使其观察序列不再是状态限制下的IID。修改常量$\boldsymbol{\mu}_i$，使其随时间而变化：

$$
\boldsymbol{o}_t = \boldsymbol{g}_t(\Lambda_i) + \boldsymbol{r}_t(\boldsymbol{\Sigma}_i)
$$

在状态$i$下，确定性的时间变化轨迹函数$\boldsymbol{g}_t(\Lambda_i)$中的参数是独立的。这便是高斯趋势HMM，这是一种特殊的非平稳状态的HMM，其中一阶统计量（均值）是随时间变化的。

### 2.2 隐马尔可夫模型似然度的计算

设$\boldsymbol{q}_1^T = (q_1, ..., q_T)$是GMM-HMM中的一个有限长度状态序列，$P(\boldsymbol{o}_1^T, \boldsymbol{q}^T_1)$是观察序列$\boldsymbol{o}_1^T = (\boldsymbol{o}_1,...,\boldsymbol{o}_T)$和状态序列$\boldsymbol{q}^T_1$的联合概率。令$P(\boldsymbol{o}_1^T|\boldsymbol{q}_1^T)$表示哎状态序列$\boldsymbol{q}_1^T$的条件下生成观察序列$\boldsymbol{o}_1^T$的概率。

在GMM-HMM中：

$$
P(\boldsymbol{o}_1^T|\boldsymbol{q}_1^T) = \prod^T_{t=1}b_i(\boldsymbol{o}_t)
\\
= \prod^T_{t=1}\sum^M_{m=1}\frac{c_{i,m}}{(2\pi)^{D/2}|\boldsymbol{\Sigma}_{i,m}|^{1/2}}\exp[-\frac{1}{2}(\boldsymbol{o}_t - \boldsymbol{\mu}_{i,m})^T\boldsymbol{\Sigma}^{-1}_{i,m}(\boldsymbol{o}_t - \boldsymbol{\mu}_{i,m})]
$$

另一方面，状态序列$\boldsymbol{q}_1^T$的概率为转移概率乘积

$$
P(\boldsymbol{q}_1^T) = \pi_{q_1}\prod^{T-1}_{t=1}a_{q_tq_{t+1}}
$$

为了记号上的简便，考虑初始状态分布的概率为1（$\pi_{q_1} = 1$）

联合概率$P(\boldsymbol{o}_1^T, \boldsymbol{q}_1^T)$可以通过上式乘积得到

$$
P(\boldsymbol{o}_1^T, \boldsymbol{q}_1^T) = P(\boldsymbol{o}_1^T|\boldsymbol{q}_1^T) P(\boldsymbol{q}_1^T)
$$

原则上可以通过累加状态序列下的联合概率计算总体观察序列似然度

$$
P(\boldsymbol{o}_1^T) = \sum_{\boldsymbol{q}_1^T}P(\boldsymbol{o}_1^T, \boldsymbol{q}_1^T)
$$

然而，在长度为$T$下运算是指数级的复杂度，所以不可行。使用前向算法计算，复杂度与$T$是线性的。

### 2.3 计算似然度

首先定义马尔可夫链每个状态$i$下的前向概率

$$
\alpha_t(i) = P(q_t=i,\boldsymbol{o}_1^t), \quad t = 1,...,T
$$

与后向概率

$$
\beta_t(i) = P(\boldsymbol{o}^T_{t+1}|q_t=i), \quad t = 1,...,T
$$

前向概率和后向概率递归计算方法：

$$
\alpha_t(j) = \sum^N_{i=1}\alpha_{t-1}(i)a_{ij}b_j(\boldsymbol{o}_t), \quad t = 2,3,...,T; \quad j = 1,2,...,N
\\
\beta_t(i) = \sum^N_{j=1}\beta_{t+1}(j)a_{ij}b_j(\boldsymbol{o}_{t+1}), \quad t = T-1, T-2,...,1; \quad i =1,2,...,N
$$

$\alpha$递归式初始值为：

$$
\alpha_1(i) = P(q_1=i,\boldsymbol{o}_1) = P(q_1=i)P(\boldsymbol{o}_1|q_1)=\pi_ib_i(\boldsymbol{o}_1), \quad i = 1,2,...,N
$$

令$\beta$递归式初始值为：

$$
\beta_T(i) = 1, \quad i =1,2,...,N
$$

我们的目标是计算$P(\boldsymbol{o}_1^T)$，先对于每个状态$i$与$t = 1,2,...,T$，计算

$$
P(q_t = i,\boldsymbol{o}_1^T) = P(q_t=i, \boldsymbol{o}_1^t, \boldsymbol{o}^T_{t+1})
\\
= P(q_t = i, \boldsymbol{o}_1^t)P(\boldsymbol{o}^T_{t+1}|\boldsymbol{o_1^t, q_t=i})
\\
= P(q_t = i, \boldsymbol{o}_1^t)P(\boldsymbol{o}^T_{t+1}|q_t=i)
\\
= \alpha_t(i)\beta_t(i)
$$

这样，$P(\boldsymbol{o}_1^T)$可以按照公式计算

$$
P(\boldsymbol{o}_1^T) = \sum^N_{i=1}P(q_t=i,\boldsymbol{o}_1^T) = \sum^N_{i=1}\alpha_t(i)\beta_t(i)
$$

将$t=T$代入上式，可以得出

$$
P(\boldsymbol{o}_1^T) = \sum^N_{i=1}\alpha_T(i)
$$

---

前向概率递归

$$
\alpha_t(j) = P(q_t=j,\boldsymbol{o}_1^t)
\\
= \sum^N_{i=1}P(q_{t-1} = i, q_t = j,\boldsymbol{o}_1^{t-1}, \boldsymbol{o}_t)
\\
= \sum^N_{i=1}P(q_t=j,\boldsymbol{o}_t|q_{t-1} = i, \boldsymbol{o}_1^{t-1})P(q_{t-1} = i, \boldsymbol{o}_1^{t-1})
\\
= \sum^N_{i=1}P(q_t=j,\boldsymbol{o}_t|q_{t-1} = i)\alpha_{t-1}(i)
\\
= \sum^N_{i=1}P(\boldsymbol{o}_t|q_t=j,q_{t-1}=i)P(q_t=j|q_{t-1} = i)\alpha_{t-1}(i)
\\
= \sum^N_{i=1}b_j(\boldsymbol{o}_t)a_{ij}\alpha_{t-1}(i)
$$


后向概率递归

$$
\beta_t(i) = P(\boldsymbol{o}_{t+1}^T|q_t=i)
\\
= \frac{P(\boldsymbol{o}_{t+1}^T, q_t=i)}{P(q_t=i)}
\\
=\frac{\sum^N_{j=1}P(\boldsymbol{o}^T_{t+1}, q_t=i,q_{t+1}=j)}{P(q_t=i)}
\\
= \frac{\sum^N_{j=1}P(\boldsymbol{o}^T_{t+1}| q_t=i,q_{t+1}=j)P(q_t=i,q_{t+1}=j)}{P(q_t=i)}
\\
= \sum^N_{j=1}P(\boldsymbol{o}^T_{t+1}|q_{t+1}=j)\frac{P(q_t=i,q_{t+1}=j)}{P(q_t=i)}
\\
= \sum^N_{j=1} P(\boldsymbol{o}^T_{t+2}, \boldsymbol{o}_{t+1}|q_{t+1} = j)a_{ij}
\\
=\sum^N_{j=1}P(\boldsymbol{o}^T_{t+2}|q_{t+1} = j)P(\boldsymbol{o}_{t+1}|q_{t+1} = j)a_{ij}
\\
= \sum^N_{j=1}\beta_{t+1}(j)b_j(\boldsymbol{o}_{t+1})a_{ij}
$$

## 3. EM算法及其在学习HMM参数中的应用



## 4. 用于解码HMM状态序列的维特比算法



## 5. 隐马尔可夫模型和生成语音识别模型的变体

