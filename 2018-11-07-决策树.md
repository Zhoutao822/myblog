---
title: 决策树
date: 2018-11-07 16:32:20
categories:
- machine learning
tags:
- theory
- decision tree
mathjax: true
---

**树即是人，人即是树**

参考：

> 西瓜书第4章 决策树

> [决策树系列（一）——基础知识回顾与总结](http://www.cnblogs.com/yonghao/p/5061873.html)

## 1. 基本流程

在日常生活中其实我们都可能在使用决策树算法，只是你没有这个概念，比如说，在这个看脸的时代，你在妹子心中的状态可以用下图描述

{% asset_img tree.png 简单决策树 %}

这样就完成了一个三分类决策树，每一个判断节点都是一个属性，每个节点的分支都是该属性的属性值。

---
**决策树学习基本算法**

输入：训练集$D = \{ (\boldsymbol{x}_1, y_1), (\boldsymbol{x}_2, y_2), ..., (\boldsymbol{x}_m, y_m) \}$；

过程：函数$TreeGenerate(D, A)$

1：生成结点$node$；

2：$if$ $D$中样本全属于同一类别$C$ $then$

3：    将$node$标记为$C$类叶结点；$return$

4：$end$ $if$

5：$if$ $A = \text{\O}$ $OR$ $D$ 中样本在$A$上的取值相同 $then$

6：    将$node$标记为叶结点，其类别标记为$D$中样本数最多的类；$return$

7：$end$ $if$

8：从$A$中选择最优化分属性$a_*$；

9：$for$ $a_*$的每一个值$a_*^v$ $do$

10：    为$node$生成一个分支；令$D_v$表示$D$中在$a_*$上取值为$a_*^v$的样本子集；

11：    $if$ $D_v$为空 $then$

12：        将分支结点标记为叶结点，其标记类别为$D$中样本最多的类；$return$

13：    $else$

14：        以$TreeGenerate(D_v, A \setminus  \{a_* \})$为分支结点

15：    $end$ $if$

16：$end$ $for$

输出：以$node$为根结点的一棵决策树

---

递归生成决策树有三种情形会导致递归返回：

1. 当前结点包含的样本全属于同一类别，无需划分；
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；
3. 当前结点包含的样本集合为空，不能划分。

## 2. 划分选择

决策树学习的关键在第8行，即如何选择最优划分属性。一般来说，我们希望分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”也来越高（如何衡量“纯度”，有不同的算法）。

### 2.1 信息增益

假定当前样本集合$D$中第$k$类样本所占的比例为$p_k(k = 1,2,...,|\mathbb{Y}|)$，则$D$的信息熵定义为

$$
Ent(D) = -\sum^{|\mathbb{Y}|}_{k=1}p_k\log_2p_k
$$

$Ent(D)$的值越小，则$D$的纯度越高。

假定离散属性$a$有$V$个可能的取值$\{ a^1, a^2, ...,a^V \}$，若使用$a$来对样本集合$D$进行划分，则会产生$V$个分支结点，其中第$v$个分支结点包含了$D$中所有在属性$a$上取值为$a^v$的样本，记为$D^v$。则可以计算用属性$a$对$D$进行划分所获得的信息增益

$$
Gaib(D, a) = Ent(D) - \sum^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)
$$

一般而言，信息增益越大，则使用属性$a$来进行划分所获得的纯度提升越大。因此，在第8行使用属性$a_* = \underset{a \in A}{\arg \max Gain(D, a)}$。这就是$ID3$决策树学习算法的划分准则。

### 2.2 增益率

利用信息增益作为划分准则可能会带来另一问题，即对可取值数目较多的属性有所偏好，比如说若一个属性有10个属性值，而另一个属性只有3个属性值，此时经过计算很大程度上10个属性值的属性带来的信息增益会大于另一个。为减少这种偏好可能带来的不利影响，$C4.5$决策树算法使用增益率作为划分准则

$$
Gain_ratio(D, a) = \frac{Gain(D,a)}{IV(a)}
\\
IV(a) = -\sum^V_{v=1}\frac{|D^v|}{|D|} \log_2\frac{|D^v|}{|D|}
$$

$IV(a)$称为属性$a$的固有值，若属性$a$的取值数目越多，则$IV(a)$的值通常会越大。

与此同时$C4.5$算法具体流程并不是直接选择增益率最高的属性，而是先从候选属性中选择信息增益高于平均水平的属性，再从中选择增益率高的。

#### 2.3 基尼指数

$CART$决策树（可用于分类和回归）使用基尼指数选择划分属性

$$
Gini(D) = \sum^{|\mathbb{Y}|}_{k=1}\sum_{k' \neq k}p_kp_{k'}
\\
= 1- \sum^{|\mathbb{Y}|}_{k=1}p_k^2
$$

$Gini(D)$反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(D)$越小，则数据集$D$纯度越高。

则对应属性$a$的基尼指数定义为

$$
Gini\_index(D, a) = \sum^V_{v=1}\frac{|D^v|}{|D|}Gini(D^v)
$$

我们选择使得划分后基尼指数最小的属性作为最优属性，即$a_* = \underset{a \in A}{\arg \min Gini\_index(D, a)}$

## 3. 剪枝处理

剪枝是对付过拟合的主要手段，主要方式是删除一些分支，基本策略有两种，预剪枝和后剪枝。
* 预剪枝是在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分当前结点并标记为叶结点；
* 后剪枝是先从数据集生成一棵完整的决策树，然后自底向上对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将子树替换为叶结点。

如何判断泛化性能，我们使用验证集进行评估。

### 3.1 预剪枝

{% asset_img precut.png 预剪枝 %}
 
1. 根据划分准则选择最优划分属性；
2. 计算不对当前最优属性划分的条件下验证集的准确率；
3. 计算对当前最优属性划分后验证集的准确率；
4. 比较上面两步的值，若划分后的准确率不大于划分前，则放弃使用该属性划分。

问题：

* 预剪枝是基于贪心算法的，必然存在继续划分可能带来泛化性能提升的可能性，使用预剪枝导致永远无法展开这些分支；
* 预剪枝降低过拟合的风险，减少了训练开销。

### 3.2 后剪枝

{% asset_img backcut.png 后剪枝 %}

1. 根据训练集生成完整的决策树；
2. 计算当前决策树在验证集上的准确率；
3. 自底向上考虑某个属性，计算将当前属性替换为叶结点（取最多的类别作为叶结点标记）后的验证集准确率；
4. 比较上面两步的值，若剪枝后的准确率不大于剪枝前的，则放弃使用该属性划分。

问题：

* 后剪枝保留的分支较预剪枝多，所以欠拟合风险小，泛化性能优于预剪枝；
* 自底向上的过程导致训练开销远大于预剪枝。

## 4. 连续与缺失值

### 4.1 连续值处理

对于连续值，最简单的策略是采用二分法，即将属性$a$的属性值基于$t$划分为两个阵营$D_t^-$和$D_t^+$，如何确定$t$的值才能使该划分为最优化分，显然，$t$的取值在$[a^i, a^{i+1})$之间都是等效的，那么我们可以参照插队的方式取$t$的所有可能值，再从中选取最优点。

对于有$n$个属性值的连续属性$a$，$t$的候选划分点集和

$$
T_a = \{ \frac{a^i + a^{i+1}}{2}|1 \leqslant i \leqslant n-1 \}
$$

通过对信息增益表达式的改造

$$
Gain(D, a) = \underset{t \in T_a}{\max} Gain(D, a, t)
\\
= \underset{t \in T_a}{\max} Ent(D) - \sum_{\lambda \in \{ -,+ \}}\frac{|D^{\lambda}_t|}{|D|}Ent(D_t^{\lambda})
$$

需注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性。

### 4.2 缺失值处理

样本不完整，那么该样本应该怎样划分呢。


## 5. 多变量决策树


