---
title: 深度学习-线性因子模型
date: 2019-01-03 21:21:11
categories:
- Deep Learning
tags:
- Theory
- PCA
- ICA
- Probabilistic PCA
- SFA
- Sparse Coding
- Factor Analysis
mathjax: true
---


相较于线性模型描述输入$\boldsymbol{x}$与输出$\boldsymbol{y}$的关系，线性因子模型考虑的是输入$\boldsymbol{x}$与隐变量$\boldsymbol{h}$的关系，也就是说线性因子模型的目的是发掘出输入变量$\boldsymbol{x}$的统计特征，利用隐变量实现降维或输入优化等目的。

线性因子模型，顾名思义，我们假设$\boldsymbol{x}$是$\boldsymbol{h}$的线性变换以及添加噪声来生成的

$$
\boldsymbol{x} = \boldsymbol{Wh} + \boldsymbol{b} + noise
$$

其中解释性因子$\boldsymbol{h}$从一个分布中抽取

$$
\boldsymbol{h} \sim p(\boldsymbol{h})
$$

其中$p(\boldsymbol{h})$是一个因子分布，满足$p(\boldsymbol{h}) = \prod_ip(h_i)$，噪声通常是对角化的且服从高斯分布。

{% asset_img linear.png %}

<!-- more -->

## 1. 概率PCA和因子分析

概率PCA、因子分析和其他线性因子模型是上述等式的特殊情况，并且仅在对观测到$\boldsymbol{x}$之前的噪声分布和潜变量$\boldsymbol{h}$先验的选择上有所不同。

### 1.1 因子分析

在因子分析中，潜变量的先验是一个方差为单位矩阵的高斯分布

$$
\boldsymbol{h} \sim N(\boldsymbol{h};\boldsymbol{0},\boldsymbol{I})
$$

同时假定在给定$\boldsymbol{h}$的条件下观察值$x_i$是条件独立的。具体来说，我们可以假设噪声是从对角协方差矩阵的高斯分布中抽出的，协方差矩阵为$\boldsymbol{\psi} = diag(\boldsymbol{\sigma}^2)$，其中$\boldsymbol{\sigma}^2 = [\sigma_1^2, \sigma_2^2, ...,\sigma_n^2]^T$表示一个向量，每个元素表示一个变量的方差。

因此潜变量的作用是捕获不同观测变量$x_i$之间的依赖关系。

$$
\boldsymbol{x} \sim N(\boldsymbol{x};\boldsymbol{b}, \boldsymbol{W}\boldsymbol{W}^T + \boldsymbol{\psi})
$$

### 1.2 概率PCA

对因子分析模型作轻微修改，使条件方差$\sigma_i^2$等于同一个值。在这种情况下，$\boldsymbol{x}$的协方差简化为$\boldsymbol{W}\boldsymbol{W}^T + \sigma^2\boldsymbol{I})$，这里的$\sigma^2$是一个标量。

$$
\boldsymbol{x} \sim N(\boldsymbol{x};\boldsymbol{b}, \boldsymbol{W}\boldsymbol{W}^T + \sigma^2\boldsymbol{I})
$$

或等价

$$
\boldsymbol{x} = \boldsymbol{Wh} + \boldsymbol{b} + \sigma \boldsymbol{z}
$$

其中$\boldsymbol{z} \sim N(\boldsymbol{z}; \boldsymbol{0}, \boldsymbol{I})$是高斯噪声，`Tipping and Bishop`提出了一种迭代的EM算法来估计参数$\boldsymbol{W}$和$\sigma^2$。

概率PCA模型利用了一种观察现象：除了一些微小残余的重构误差（至多为$\sigma^2$），数据中的大多数变量可以由潜变量$\boldsymbol{h}$描述。当$\sigma \rightarrow 0$，概率PCA退化为PCA，概率PCA所定义的密度函数在$d$维的$\boldsymbol{W}$的列空间周围非常尖锐。这导致模型会为没有在一个超平面附近聚集的数据分配非常低的概率。

## 2. 独立成分分析

独立成分分析ICA是一种建模线性因子的方法，旨在将观察到的信号分离成许多潜在信号，这些潜在信号通过压缩和叠放可以恢复成观察数据。这些信号是完全独立，而不是仅仅彼此不相关。

潜在因子$\boldsymbol{h}$的先验$p(\boldsymbol{h})$，必须由用户提前给出并固定。接着模型确定性地生成$\boldsymbol{x} = \boldsymbol{Wh}$。我们可以通过非线性变化来确定$p(\boldsymbol{x})$。然后通过一般的方法比如最大化似然进行学习。

ICA的所有变种均要求$p(\boldsymbol{h})$是非高斯的。因为若$p(\boldsymbol{h})$具有高斯分量的独立先验，对于许多$\boldsymbol{W}$值，我们可以在$p(\boldsymbol{x})$上获得相同的分布，那么$\boldsymbol{W}$存在多解，意味着不可解。在用户明确指定分布的最大似然方法中，一个典型的选择是使用$p(h_i) = \frac{\partial}{\partial h_i}\sigma(h_i)$。这些非高斯分布的典型选择在0附近具有比高斯分布更高的峰值，因此我们也可以看到独立成分分析经常用于学习稀疏特征。

## 3. 慢特征分析

慢特征分析SFA是使用来自时间信号的信息学习不变特征的线性因子模型。

慢特征分析的想法源于所谓的**慢性原则（slowness principle）**。其基本思想是，与场景中起描述作用的单个量度相比，场景的重要特性通常变化得非常缓慢。例如，在计算机视觉中，单个像素值可以非常快速地改变。如果斑马从左到右移动穿过图像并且它的条纹穿过对应的像素时，该像素将迅速从黑色变为白色，并再次恢复成黑色。通过比较，指示斑马是否在图像中的特征将不发生改变，并且描述斑马位置的特征将缓慢地改变。因此，我们可能希望将模型正则化，从而能够学习到那些随时间变化较为缓慢的特征。

一般来说，我们可以将慢性原则应用于可以使用梯度下降训练的任何可微分模型。为了引入慢性原则，我们可以向代价函数添加以下项

$$
\lambda \sum_t L(f(\boldsymbol{x}^{(t+1)}), f(\boldsymbol{x}^{(t)}))
$$

其中$\lambda$是确定慢度正则化强度的超参数项，$t$是样本时间序列的索引，$f$是需要正则化的特征提取器，$L$是测量$f(x(t))$和$f(x(t+1))$之间的距离的损失函数。$L$的一个常见选择是均方误差。

SFA算法先将$f(\boldsymbol{x};\theta)$定义为线性变换，然后求解如下优化问题

$$
\underset{\boldsymbol{\theta}}{\min} \mathbb{E}_t(f(\boldsymbol{x}^{(t+1)})_i - f(\boldsymbol{x}^{(t)})_i)^2
$$

并满足下面的约束

$$
\mathbb{E}_tf(\boldsymbol{x}^{(t)})_i = 0
\\
\mathbb{E}_t[f(\boldsymbol{x}^{(t)})_i^2] = 1
$$

学习特征具有零均值的约束对于使问题具有唯一解是必要的；否则我们可以向所有特征值添加一个常数，并获得具有相等慢度目标值的不同解。特征具有单位方差的约束对于防止所有特征趋近于0的病态解是必要的。

与 PCA 类似，SFA 特征是有序的，其中学习第一特征是最慢的。要学习多个特征，我们还必须添加约束

$$
\forall i < j, \quad \mathbb{E}_t[f(\boldsymbol{x}^{(t)})_if(\boldsymbol{x}^{(t)})_j] = 0
$$

这要求学习的特征必须彼此线性去相关。没有这个约束，所有学习到的特征将简单地捕获一个最慢的信号。可以想象使用其他机制，如最小化重构误差，也可以迫使特征多样化。但是由于 SFA 特征的线性，这种去相关机制只能得到一种简单的解。SFA 问题可以通过线性代数软件获得闭式解。

深度 SFA 也已经被用于学习用在对象识别和姿态估计的特征。

## 4. 稀疏编码

稀疏编码是一个线性因子模型，像大多数其他线性因子模型一样，它使用了线性的解码器加上噪声的方式获得一个$\boldsymbol{x}$的重构，稀疏编码模型通常假设线性因子有一个各向同性精度为$\beta$的高斯噪声

$$
p(\boldsymbol{x}|\boldsymbol{h}) = N(\boldsymbol{x};\boldsymbol{Wh} + \boldsymbol{b}, \frac{1}{\beta}\boldsymbol{I})
$$

分布$p(\boldsymbol{h})$通常选取为一个峰值很尖锐且接近0的分布。常见的选择包括可分解的`Laplace、Cauchy`或者可分解的`Student-t`分布。例如，以稀疏惩罚系数$\lambda$为参数的`Laplace`先验可以表示为

$$
p(h_i) = Laplace(h_i;0,\frac{2}{\lambda}) = \frac{\lambda}{4}e^{-\frac{1}{2}\lambda|h_i|}
$$

相应的，`Student-t`先验分布可以表示为

$$
p(h_i) \propto \frac{1}{(1 + \frac{h_i^2}{v})^{\frac{v+1}{2}}}
$$

使用最大似然的方法来训练稀疏编码模型是不可行的。相反，为了在给定编码的情况下更好地重构数据，训练过程在编码数据和训练解码器之间交替进行。

编码器是一个优化算法，在这个优化问题中，我们寻找单个最可能的编码值：

$$
\boldsymbol{h}^* = f(\boldsymbol{x}) = \underset{\boldsymbol{h}}{\arg \max} p(\boldsymbol{h}|\boldsymbol{x})
$$

优化问题如下

$$
\underset{\boldsymbol{h}}{\arg \max} p(\boldsymbol{h}|\boldsymbol{x})
\\
= \underset{\boldsymbol{h}}{\arg \max} \log p(\boldsymbol{h}|\boldsymbol{x})
\\
= \underset{\boldsymbol{h}}{\arg \min} \lambda ||\boldsymbol{h}||_1 + \beta ||\boldsymbol{x} - \boldsymbol{Wh}||^2_2
$$

由于$L_1$范数，这个过程将产生稀疏的$\boldsymbol{h}^*$。

为了训练模型而不仅仅是进行推断，我们交替迭代关于$\boldsymbol{h}$和$\boldsymbol{W}$的最小化过程。在本文中，我们将$\beta$视为超参数。我们通常将其设置为 1，因为它在此优化问题的作用与$\lambda$类似，没有必要使用两个超参数。

## 5. PCA的流形解释

线性因子模型，包括 PCA 和因子分析，可以理解为学习一个流形。

我们可以将概率 PCA 定义为高概率的薄饼状区域，即一个高斯分布，沿着某些轴非常窄，就像薄饼沿着其垂直轴非常平坦，但沿着其他轴是细长的，正如薄饼在其水平轴方向是很宽的一样。

PCA 可以理解为将该薄饼与更高维空间中的线性流形对准。这种解释不仅适用于传统 PCA，而且适用于学习矩阵$\boldsymbol{W}$和$\boldsymbol{V}$的任何线性自编码器，其目的是使重构的$\boldsymbol{x}$尽可能接近于原始的$\boldsymbol{x}$。

{% asset_img pca.png %}

编码器表示为

$$
\boldsymbol{h} = f(\boldsymbol{x}) = \boldsymbol{W}^T(\boldsymbol{x} - \boldsymbol{\mu})
$$

编码器计算$\boldsymbol{h}$的低维表示，解码器负责重构

$$
\hat{\boldsymbol{x}} = g(\boldsymbol{h}) = \boldsymbol{b} + \boldsymbol{Vh}
$$

能够最小化重构误差

$$
\mathbb{E}[||\boldsymbol{x} - \hat{\boldsymbol{x}}||^2]
$$

的线性编码器和解码器的选择对应着$\boldsymbol{V} = \boldsymbol{W}, \boldsymbol{\mu} = \boldsymbol{b} = \mathbb{E}[\boldsymbol{x}]$，$\boldsymbol{W}$的列形成一组标准正交基，这组基生成的子空间与协方差矩阵$\boldsymbol{C}$

$$
\boldsymbol{C} = \mathbb{E}[(\boldsymbol{x} - \boldsymbol{\mu})(\boldsymbol{x} - \boldsymbol{\mu})^T]
$$

的主特征向量所生成的子空间相同。在PCA中，$\boldsymbol{W}$的列是按照对应特征值幅度大小排序所对应的特征向量。

我们还可以发现$\boldsymbol{C}$的特征值$\lambda_i$对应了$\boldsymbol{x}$在特征向量$\boldsymbol{v}^{(i)}$方向上的方差。如果$\boldsymbol{x} \in \mathbb{R}^D, \boldsymbol{h} \in \mathbb{R}^d$并且满足$d < D$，则最佳的重构误差是

$$
\min \mathbb{E}[||\boldsymbol{x} - \hat{\boldsymbol{x}}||^2] = \sum^D_{i = d+1} \lambda_i
$$

因此，如果协方差矩阵的秩为$d$，则特征值$\lambda_{d+1}$到$\lambda_D$都为0，并且重构误差为0。

此外，我们还可以证明上述解可以通过在给定正交矩阵$\boldsymbol{W}$的情况下最大化$\boldsymbol{h}$元素的方差而不是最小化重构误差来获得。





