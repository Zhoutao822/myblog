---
title: 深度学习-正则化
date: 2018-11-29 09:07:16
categories:
- deep learning
tags:
- theory
- regularization
mathjax: true
---

参考：

> [《深度学习》第7章 深度学习中的正则化](https://github.com/exacity/deeplearningbook-chinese)

本章部分内容与`深度学习-实际问题`交叉。

机器学习的一个核心问题是设计一个不仅在训练数据上表现好，并且能在新输入上泛化好的算法。在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价），这些策略被统称为正则化。

<!-- more -->

## 1. 参数范数惩罚

对优化目标$J$添加一个参数惩罚项$\Omega(\theta)$，正则化后的目标函数记为

$$
\hat{J}(\theta;\boldsymbol{X, y}) = J(\theta;\boldsymbol{X, y}) + \alpha\Omega(\theta)
$$

其中$\alpha \in [0, \infty)$是权衡范数惩罚项$\Omega$和目标函数$J$相对贡献的超参数。显然，$\alpha$为0表示没有正则化，$\alpha$越大正则化惩罚越大。






## 2. 作为约束的范数惩罚



## 3. 正则化和欠约束问题


## 4. 数据集增强


## 5. 噪声鲁棒性


## 6. 半监督学习


## 7. 多任务学习


## 8. 提前终止


## 9. 参数绑定和参数共享


## 10. 稀疏表示


## 11. Bagging和其他集成方法

## 12. Dropout


## 13. 对抗训练


## 14. 切面距离、正切传播和流形正切分类器

