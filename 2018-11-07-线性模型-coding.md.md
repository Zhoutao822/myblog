---
title: 线性模型-coding
date: 2018-11-07 22:00:56
categories:
- machine learning
tags:
- code
- linear model
mathjax: true
---

## 1. 数据集说明

### 1.1 线性回归数据集-Boston房价

* 数据来源：sklearn.datasets.load_boston（tensorflow.keras.datasets.boston_housing理论上应该是一模一样的）；
* 数据集形状：总计506个样本，每个样本由14个属性表示，一般将最后一个房价作为target，所有属性值均为number，详情可调用load_boston()['DESCR']了解每个属性的具体含义；
* 数据集划分：随机选出20%数据作为测试集，不做验证集要求；
* 性能度量：MSE或者RMSE均可以。

### 1.2

## 2. 线性回归

### 2.1 公式法

**我们先不考虑特征工程，仅将所有特征放入线性回归模型中**。

> 首先导入需要的第三方库

```python
from sklearn.datasets import load_boston #数据集
from sklearn import preprocessing #归一化处理
from sklearn.model_selection import train_test_split #数据集划分
import pandas as pd #观察数据集
import matplotlib.pyplot as plt #绘制图表
import numpy as np #数据处理
pd.set_option('precision', 2) #设置pandas显示数据保留两位小数
```

> 然后看看数据的大致范围与一些统计信息

```python
boston = load_boston() #加载数据，load_boston()返回的是一个字典
print(boston['DESCR']) #打印数据集描述信息

filepath = boston['filename'] #调用load_boston()会下载数据集csv文件到本地，通过filename获取路径
df = pd.read_csv(filepath, skiprows=0, header=1) #通过pandas读取csv文件，由于sklearn下载的csv文件第0行是样例数和属性数，第1行是属性名称，从第2行开始才是数据，所以设置skiprows跳过第0行，设置header特征行为1
df.describe() #显示数据集统计信息
```

![]()

> 数据集划分

```python
data = boston['data'] #data对应前13列，即特征列，获取到的数据类型为np.array
target = boston['target'] #target对应最后一列，即目标列

x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, shuffle=True) #调用train_test_split划分数据集，指定test_size为0.2，指定shuffle为True，在划分前打乱数据集

# 注释掉的部分是对数据进行归一化处理，减去均值，再除以标准差，这个伏笔
# scaler = preprocessing.StandardScaler().fit(x_train)
# x_train_scale = scaler.transform(x_train)
# x_test_scale = scaler.transform(x_test)
# x_train_scale = np.column_stack((x_train_scale, np.ones(len(x_train_scale))))
# x_test_scale = np.column_stack((x_test_scale, np.ones(len(x_test_scale))))

x_train = np.column_stack((x_train, np.ones(len(x_train)))) #在公式法中我们还要增加一列全1为偏差bias
x_test = np.column_stack((x_test, np.ones(len(x_test))))
```

> 计算预测值与损失

```python
def standLR(x, y):
    '''
        根据公式计算参数w（已经包括bias）
    '''
    xMat = np.mat(x) #将np.array数据转成矩阵便于后续计算
    yMat = np.mat(y).T #对应一列

    xTx = xMat.T * xMat #.T实现矩阵转置
    if np.linalg.det(xTx) == 0.0: #如果矩阵行列式为0说明矩阵不可逆
        print('矩阵不可逆，请使用其他方法！！')
        return
    w = xTx.I * xMat.T * yMat #计算w，w的形状是一列
    return w

def predict(x, w):
    return np.mat(x) * w #根据w计算预测值，预测值也是一列

def mse(pre, y):
    m = y.shape[0]
    yMat = np.mat(y).T
    loss = np.sum(np.square(pre - yMat)) / m #计算MSE，也可以开方获取RMSE
    return loss

w = standLR(x_train, y_train)
pre = predict(x_test, w)
loss = mse(pre, y_test)
print('MSE for testSet is: {:.3f}'.format(loss))

#绘制预测值与真实值，以y=x为标准，越接近这条线越准确
plt.figure(figsize=(4, 4))
plt.plot([0, 60], [0, 60])
plt.scatter(pre.A, y_test)
plt.show()
```

> MSE for testSet is: 23.541 ，每次结果都不一定相同

![]()


* 当使用了归一化后的特征列数据进行求解时，我们最后得到的预测性能和没有使用归一化的是几乎一样的，但是这是不是意味着归一化没有用处呢，当然不是；
* 在不考虑特征工程的情形下，我们仅通过线性回归能得到的完美解$w$，在测试集上的损失是23.541，我们要想提高性能减少损失必须考虑特征工程；
* 当样例数和特征数增大时，矩阵计算需要大量内存，这个方法不合适；
* 在标准线性回归中，我们没有考虑单个样本损失的权重，比如测试点与某些样本点距离很近，那么这些近距离的样本点的损失对测试点就应当更重要，所以它们的损失权重应该较大，而那些远离测试点的样本点，其权重应当较小，基于这个理论，我们使用局部加权线性回归测试一下。

### 2.2 局部加权线性回归LWLR



### 2. 梯度下降

## 3. 二分类sigmoid



## 4. 多分类softmax



## 5. LDA降维