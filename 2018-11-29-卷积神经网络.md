---
title: 卷积神经网络
date: 2018-11-29 09:14:00
categories:
- Deep Learning
tags:
- Theory
- CNN
mathjax: true
---

参考：

> [《深度学习》第9章 卷积网络](https://github.com/exacity/deeplearningbook-chinese)

## 1. 卷积运算

$$
s(t) = \int x(a)w(t-a)da
$$

其中$t$是时间，$x(t)$是原始输入，$w(a)$是加权函数，$a$表示测量结果距当前时刻$t$的时间间隔。连起来就是，根据加权函数$w$，我们对原始输入$x$进行加权积分以求得一个更加平滑的输出$s$，积分区间一般为$[t-a, t]$，一般对时间来说，不会取$t$之后的区间，因为无法预测未来。

这种运算就叫做**卷积**，通常用$*$表示为

$$
s(t) = (x * w)(t)
$$

在卷积网络的术语中，函数$x$叫做输入，函数$w$叫做核函数，输出$s$有时被叫做特征映射。

实际上时间会被离散化，假设函数$x, w$都定义在整数时刻$t$上，我们只能取离散形式的卷积：

$$
s(t) = (x * w)(t) = \sum^\infty_{a = -\infty} x(a)w(t-a)
$$

在机器学习任务上，我们会在图像处理上使用卷积，例如把二维图像$I$作为输入，使用二维的核$K$：

$$
S(i,j) = (I * K)(i, j) = \sum_m\sum_n I(m,n)K(i-m,j-n)
$$

卷积是可交换的，所以上式等价于

$$
S(i,j) = (K * I)(i, j) = \sum_m\sum_n I(i-m,j-n)K(m,n)
$$

核矩阵$K$的维度相对$I$很小，使用核翻转，所以第二个式子更常用，也更好理解。

{% asset_img cnn0.png 卷积运算 %}

卷积通常对应着一个非常稀疏的矩阵，这是因为核的大小通常远小于输入图像的大小。

## 2. 动机

{% asset_img cnn3.jpg %}

卷积运算通过三个重要的思想来帮助改进机器学习系统：**稀疏交互**、**参数共享**、**等变表示**。另外卷积提供了一种处理大小可变的输入的方法。

1. **稀疏交互**：传统的DNN使用矩阵乘法建立输入与输出的连接关系。其中参数矩阵中每一个单独的参数都描述了一个输入单元与一个输出单元间的交互。这意味着每一个输出单元与每一个输入单元都产生交互。卷积网络具有稀疏交互的特征，即仅使用一个非常小的核来检测输入中有意义的特征，例如图像的边缘。具体的表现为，每一个卷积层定义多个filter（滤波器），每个filter检测输入的一种有意义的特征，比如可能有横线、竖线、圆角等等，在下一个卷积层进一步的提取特征形成卷积神经网络（还包括了操作）。那么我们仅需要保存这些filter的参数就相当于保存了整个神经网络，对比DNN，$m$个输入和$n$个输出，参数有$m \times n$且算法时间复杂度为$O(m \times n)$；稀疏连接限制每一个输出只有$k$个连接，参数有$k \times n$且算法时间复杂度为$O(k \times n)$，在实际应用中$k$比$m$小几个数量级，就能在机器学习任务中取得好的表现。
2. **参数共享**：是指在一个模型的多个函数中使用相同的参数。作为参数共享的同义词，我们可以说一个网络含有**绑定的权重**，在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。这样$k$的数量级才能小于$m$。
3. **等变表示**：对于卷积，参数共享的特殊形式使得神经网络层具有对平移等变的性质。对于函数$f(x),g(x)$，若满足$f(g(x)) = g(f(x))$，则$f(x)$对于变换$g$具有等变性。例如令$g$是任意平移函数，那么先卷积再平移得到的结果与先平移再卷积得到的结果相同。但是另一些变换不是天然等变的，比如图像的缩放或者旋转变换，需要其他的一些机制来处理。

## 3. 池化

{% asset_img cnn2.jpg %}

卷积神经网络的一个典型层包括三层。第一级是并行计算的多个卷积产生一组线性激活响应；第二级中，每一个线性激活响应会通过一个非线性激活函数，例如ReLU；第三级中，使用池化函数进一步调整这一层的输出。

池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。
例如，最大池化函数给出相邻矩形区域内的最大值。其他常用的池化函数包括相邻矩形区域内的平均值、$L_2$范数以及基于据中心像素距离的加权平均函数。

不管采用什么样的池化函数，当输入作出少量平移时，池化能够帮助输入的表示近似不变。对于平移的不变性是指当我们对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变。**局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时**。例如，当判定一张图像中是否包含人脸时，我们并不需要知道眼睛的精确像素位置，我们只需要知道有一只眼睛在脸的左边，有一只在右边就行了。但在一些其他领域，保存特征的具体位置却很重要。例如当我们想要寻找一个由两条边相交而成的拐角时，我们就需要很好地保存边的位置来判定它们是否相交。

使用池化可以看作是增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性。当这个假设成立时，池化可以极大地提高网络的统计效率。

用多个学得的过滤器和一个最大池化单元可以学得对旋转变换的不变性。

因为池化综合了全部邻居的反馈，这使得池化单元少于探测单元成为可能，我们可以通过综合池化区域的$k$个像素的统计特征而不是单个像素来实现。这种方法提高了网络的计算效率，因为下一层少了约$k$倍的输入。

## 4. 卷积与池化作为一种无限强的先验

我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。这个先验也要求除了那些处在隐藏单元的小的空间连续的接受域内的权重以外，其余的权重都为零。总之，我们可以把卷积的使用当作是对网络中一层的参数引入了一个无限强的先验概率分布。这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性。类似的，使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。

一个关键的洞察是卷积和池化可能导致欠拟合。与任何其他先验类似，卷积和池化只有当先验的假设合理且正确时才有用。如果一项任务依赖于保存精确的空间信息，那么在所有的特征上使用池化将会增大训练误差。

另一个关键洞察是当我们比较卷积模型的统计学习表现时，只能以基准中的其他卷积模型作为比较的对象。其他不使用卷积的模型即使我们把图像中的所有像素点都置换后依然有可能进行学习。

## 5. 基本卷积函数的变体

有时会希望跳过核中的一些位置来降低计算的开销（相应的代价是提取特征没有先前那么好了）。我们可以把这一过程看作是对全卷积函数输出的下采样(downsampling)。如果我们只想在输出的每个方向上每间隔$s$个像素进行采样。我们把 s 称为下采样卷积的 步幅（stride）。当然也可以对每个移动方向定义不同的步幅。

在任何卷积网络的实现中都有一个重要性质，那就是能够隐含地对输入$V$用零进行填充 (pad) 使得它加宽。如果没有这个性质，表示的宽度在每一层就会缩减，缩减的幅度是比核少一个像素这么多。

三种零填充情况：

1. 第一种是无论怎样都不使用零填充的极端情况，并且卷积核只允许访问那些图像中能够完全包含整个核的位置，这称为有效（valid），卷积输出的大小在每一层都会缩减。如果输入的图像宽度是$m$，核的宽度是$k$，那么输出的宽度就会变成$m − k + 1$；
2. 第二种特殊的情况是只进行足够的零填充来保持输出和输入具有相同的大小。这称为相同（same）卷积，这可能会导致边界像素存在一定程度的欠表示；
3. 第三种极端情况产生了，称为 全（full）卷积。它进行了足够多的零填充使得每个像素在每个方向上恰好被访问了$k$次，最终输出图像的宽度为$m + k − 1$，在这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致学得一个在卷积特征映射的所有位置都表现不错的单核更为困难。

通常零填充的最优数量（对于测试集的分类正确率）处于“有效卷积”和“相同卷积”之间的某个位置。

### 5.1 非共享卷积

非共享卷积并不是真正的卷积，只是利用局部连接的网络层。在这种情况下，我们的多层感知机对应的邻接矩阵是相同的，但每一个连接都有它自己的权重。它和具有一个小核的离散卷积运算很像，但并不横跨位置来共享参数。

当我们知道每一个特征都是一小块空间的函数并且相同的特征不会出现在所有的空间上时，局部连接层是很有用的。例如，如果我们想要辨别一张图片是否是人脸图像时，我们只需要去寻找嘴是否在图像下半部分即可。

### 5.2 平铺卷积

平铺卷积对卷积层和局部连接层进行了折衷。这里并不是对每一个空间位置的权重集合进行学习，我们学习一组核使得当我们在空间移动时它们可以循环利用。这意味着在近邻的位置上拥有不同的过滤器，就像局部连接层一样，但是对于这些参数的存储需求仅仅会增长常数倍，这个常数就是核的集合的大小，而不是整个输出的特征映射的大小。

我们一般也会在进行非线性运算前，对每个输出加入一些偏置项。这样就产生了如何在偏置项中共享参数的问题。对于局部连接层，很自然地对每个单元都给定它特有的偏置，对于平铺卷积，也很自然地用与核一样的平铺模式来共享参数。对于卷积层来说，通常的做法是在输出的每一个通道上都设置一个偏置，这个偏置在每个卷积映射的所有位置上共享。然而，如果输入是已知的固定大小，也可以在输出映射的每个位置学习一个单独的偏置。分离这些偏置可能会稍稍降低模型的统计效率，但同时也允许模型来校正图像中不同位置的统计差异。例如，当使用隐含的零填充时，图像边缘的探测单元接收到较少的输入，因此需要较大的偏置。

## 6. 结构化输出

卷积神经网络可以用于输出高维的结构化对象，而不仅仅是预测分类任务的类标签或回归任务的实数值。通常这个对象只是一个张量，由标准卷积层产生。例如，模型可以产生张量$\boldsymbol{S}$，其中$\boldsymbol{S}_{i,j,k}$是网络的输入像素$(j, k)$属于类$i$的概率。这允许模型标记图像中的每个像素，并绘制沿着单个对象轮廓的精确掩模。

对图像逐个像素标记的一种策略是先产生图像标签的原始猜测，然后使用相邻像素之间的交互来修正该原始猜测。重复这个修正步骤数次对应于在每一步使用相同的卷积，该卷积在深层网络的最后几层之间共享权重。这使得在层之间共享参数的连续的卷积层所执行的一系列运算，形成了一种特殊的循环神经网络。

一旦对每个像素都进行了预测，我们就可以使用各种方法来进一步处理这些预测，以便获得图像在区域上的分割。一般的想法是假设大片相连的像素倾向于对应着相同的标签。图模型可以描述相邻像素间的概率关系。或者，卷积网络可以被训练来最大化地近似图模型的训练目标。

## 7. 数据类型

卷积网络使用的数据通常包含多个通道，每个通道是时间上或空间中某一点的不同观测量。

到目前为止，我们仅讨论了训练和测试数据中的每个样例都有相同的空间维度的情况。卷积网络的一个优点是它们还可以处理具有可变的空间尺度的输入。这些类型的输入不能用传统的基于矩阵乘法的神经网络来表示。这为卷积网络的使用提供了令人信服的理由，即使当计算开销和过拟合都不是主要问题时。

{% asset_img cnn1.png %}

注意，使用卷积处理可变尺寸的输入，仅对输入是因为包含对同种事物的不同量的观察 (时间上不同长度的记录，空间上不同宽度的观察等) 而导致的尺寸变化这种情况才有意义。如果输入是因为它可以选择性地包括不同种类的观察而具有可变尺寸，使用卷积是不合理的。例如，如果我们正在处理大学申请，并且我们的特征包括成绩等级和标准化测试分数，但不是每个申请人都进行了标准化测试，则使用相同的权重来对成绩特征和测试分数特征进行卷积是没有意义的。

## 8. 高效的卷积算法

卷积等效于使用傅立叶变换将输入与核都转换到频域、执行两个信号的逐点相乘，再使用傅立叶逆变换转换回时域。对于某些问题的规模，这种算法可能比离散卷积的朴素实现更快。

将$d$维的核表示为$d$个一维向量，等价于组合$d$个一维卷积，那么多线程运算可以起到作用。

设计更快的执行卷积或近似卷积，而不损害模型准确性的方法，是一个活跃的研究领域。甚至仅提高前向传播效率的技术也是有用的，因为在商业环境中，通常部署网络比训练网络还要耗资源。

## 9. 随机或无监督的特征

通常，卷积网络训练中最昂贵的部分是学习特征。输出层的计算代价通常相对不高，因为在通过若干层池化之后作为该层输入的特征的数量较少。当使用梯度下降执行监督训练时，每步梯度计算需要完整地运行整个网络的前向传播和反向传播。减少卷积网络训练成本的一种方式是使用那些不是由监督方式训练得到的特征。

三种策略可以不通过监督训练而得到卷积核：

1. 简单随机初始化；
2. 手动设计，例如检测指定方向或尺度边缘的核；
3. 无监督标准，使用$k$均值聚类算法应用于小图像块，然后使用每个学得的中心作为卷积核。

随机过滤器经常在卷积网络中表现得出乎意料得好说明，由卷积和随后的池化组成的层，当赋予随机权重时，自然地变得具有频率选择性和平移不变性。

一个中间方法是学习特征，但是使用那种不需要在每个梯度计算步骤中都进行完整的前向和反向传播的方法。与多层感知机一样，我们使用贪心逐层预训练，单独训练第一层，然后一次性地从第一层提取所有特征，之后用那些特征单独训练第二层，以此类推。
