---
title: 语音识别-RBM和DBN
date: 2018-11-29 09:20:44
categories:
- Speech Recognition
tags:
- Theory
- RBM
- DBN
- Pretraining
mathjax: true
---

参考：

> 《解析深度学习-语音识别实践》第5章 高级模型初始化技术
> [《深度学习》第20章 深度生成模型](https://github.com/exacity/deeplearningbook-chinese)

**语音识别过程：切除首尾端静音->声音划分为帧->对每一帧提取MFCC特征->若干帧对应一个状态->三个状态组成一个音素->若干音素组成单词**

## 1. 受限玻尔兹曼机

在介绍受限玻尔兹曼机RBM之前，先了解一下玻尔兹曼机。

### 1.1 玻尔兹曼机

玻尔兹曼机最初作为一种广义的“联结主义”引入，用来学习二值向量上的任意概率分布。也就是说，我们的目标是计算一组二值向量数据（只包含0和1）的概率分布情况。

首先在$d$维二值随机向量$\boldsymbol{x} \in \{0,1 \}^{d \times 1}$上定义玻尔兹曼机。玻尔兹曼机是一种基于能量的模型（玄学？），意味着我们可以使用能量函数定义联合概率分布：

$$
P(\boldsymbol{x}) = \frac{\exp (-E(\boldsymbol{x}))}{Z}
$$

其中$E(\boldsymbol{x})$是能量函数，$Z$是确保概率和为1的配分函数。玻尔兹曼机的能量函数如下：

$$
E(\boldsymbol{x}) = -\boldsymbol{x}^T\boldsymbol{U}\boldsymbol{x} - \boldsymbol{b}^T\boldsymbol{x}
$$

其中$\boldsymbol{U}$是模型参数的权重矩阵，$\boldsymbol{b}$是偏置向量。

在一般情况下，根据观察到的样本的情况，我们可以很容易计算上面的表达式，但是它限制了观察到的变量和权重矩阵描述的变量之间相互作用的类型。具体来说，这意味着一个单元的概率由
其他单元值的线性模型（逻辑回归）给出。

当不是所有变量都能被观察到时，玻尔兹曼机作用更强。我们将单元分为两个子集：可见单元$\boldsymbol{v}$和隐藏单元$\boldsymbol{h}$。则能量函数变为

$$
E(\boldsymbol{v}, \boldsymbol{h}) = -\boldsymbol{v}^T\boldsymbol{R}\boldsymbol{v} - \boldsymbol{v}^T\boldsymbol{W}\boldsymbol{h}-\boldsymbol{h}^T\boldsymbol{S}\boldsymbol{h} - \boldsymbol{b}^T\boldsymbol{v} - \boldsymbol{c}^T\boldsymbol{h}
$$

玻尔兹曼机的学习算法通常基于最大似然，当基于最大似然的学习规则训练时，连接两个单元的特定权重的更新仅取决于这两个单元在不同分布下收集的统计信息：$P_{model}(\boldsymbol{v})$和$\hat{P}_{data}(\boldsymbol{v})P_{model}(\boldsymbol{h}|\boldsymbol{v})$。网络的其余部分参与塑造这些统计信息，但权重可以在完全不知道网络其余部分的情况下更新。

<!-- more -->

### 1.2 受限玻尔兹曼机

{% asset_img rbm0.png 受限玻尔兹曼机 %}

RBM是包含一层可观察变量和单层潜变量的无向概率图模型，它是一个二分图，观察层或潜层中的任何单元之间不允许存在连接。令观察层由一组$n_v$个二值随机变量组成，我们统称为向量$\boldsymbol{v} \in \{0, 1\}^{n_v \times 1}$。我们将$n_h$个二值随机变量的潜在或隐藏层记为$\boldsymbol{h} \in \{0, 1\}^{n_h \times 1}$，$\boldsymbol{b}\in \{0, 1\}^{n_v \times 1}$和$\boldsymbol{c} \in \{0, 1\}^{n_h \times 1}$分别是可见层和隐藏层的偏置向量。其能量函数为

$$
E(\boldsymbol{v}, \boldsymbol{h}) = -\boldsymbol{b}^T\boldsymbol{v} - \boldsymbol{c}^T\boldsymbol{h} - \boldsymbol{v}^T\boldsymbol{W}\boldsymbol{h}
$$

若$\boldsymbol{v} \in \mathbb{R}^{n_v \times1}$，即可见层取实值，则能量函数变为

$$
E(\boldsymbol{v}, \boldsymbol{h}) = \frac{1}{2}(\boldsymbol{v} - \boldsymbol{b})^T(\boldsymbol{v} - \boldsymbol{b}) - \boldsymbol{c}^T\boldsymbol{h} - \boldsymbol{v}^T\boldsymbol{W}\boldsymbol{h}
$$

其联合概率分布为

$$
P(\boldsymbol{v}, \boldsymbol{h}) = \frac{\exp (-E(\boldsymbol{v}, \boldsymbol{h}))}{Z}
\\
Z = \sum_\boldsymbol{v}\sum_\boldsymbol{h}\exp \{ -E(\boldsymbol{v},\boldsymbol{h}) \}
$$

显然，若能穷举状态，则计算$Z$是很方便的，但是问题在于很难穷举所有状态，因此需要更好的方法计算$Z$。

### 1.3 条件分布

要注意的是，我们的目标是挖掘观测变量的潜在信息，需要求解的是观测变量分布概率，然而$P(\boldsymbol{v})$是难解的，但基于隐藏变量的条件分布的采样和计算是相对简单的。

$$
P(\boldsymbol{h}|\boldsymbol{v}) = \frac{P(\boldsymbol{h}, \boldsymbol{v})}{P(\boldsymbol{v})}
\\
= \frac{1}{P(\boldsymbol{v})} \frac{1}{Z} \exp (\boldsymbol{b}^T\boldsymbol{v} + \boldsymbol{c}^T\boldsymbol{h} + \boldsymbol{v}^T\boldsymbol{W}\boldsymbol{h})
\\
= \frac{1}{Z'}\exp (\boldsymbol{c}^T\boldsymbol{h} + \boldsymbol{v}^T\boldsymbol{W}\boldsymbol{h}), \quad Z' = \frac{\partial Z}{\partial \boldsymbol{h}}
\\
=\frac{1}{Z'}\exp (\sum^{n_h}_{j=1}c_jh_j + \sum^{n_h}_{j=1}\boldsymbol{v}^T\boldsymbol{W}_{:,j}h_j)
\\
=\frac{1}{Z'} \prod^{n_h}_{j=1}\exp (c_jh_j + \boldsymbol{v}^T\boldsymbol{W}_{:,j}h_j)
\\
= \prod^{n_h}_{j=1}P(h_j|\boldsymbol{v})
$$

又因为$h_j$是二值的，所以

$$
P(h_j = 1|\boldsymbol{v}) = \frac{\tilde{P}(h_j = 1|\boldsymbol{v})}{\tilde{P}(h_j = 0|\boldsymbol{v}) + \tilde{P}(h_j = 1|\boldsymbol{v})}
\\
= \frac{\exp(c_j + \boldsymbol{v}^T\boldsymbol{W}_{:,j})}{\exp(0) + \exp(c_j + \boldsymbol{v}^T\boldsymbol{W}_{:,j})}
\\
=\sigma(c_j + \boldsymbol{v}^T\boldsymbol{W}_{:,j})
$$

$\sigma$是Sigmoid函数。

扩展一下

$$
P(\boldsymbol{h} = \boldsymbol{1}|\boldsymbol{v}) = \sigma(\boldsymbol{c} + \boldsymbol{W}^T\boldsymbol{v}) 
$$

由此得到关于隐藏层的完全条件分布表达为因子形式

$$
P(\boldsymbol{h}|\boldsymbol{v}) = \prod^{n_h}_{j=1}\sigma((2\boldsymbol{h}-1)\odot(\boldsymbol{c} + \boldsymbol{W}^T\boldsymbol{v}))_j
$$

$\boldsymbol{v}$和$\boldsymbol{h}$是类似的，可以得到

$$
P(\boldsymbol{v}|\boldsymbol{h}) = \prod^{n_v}_{i=1}\sigma((2\boldsymbol{v}-1)\odot(\boldsymbol{b} + \boldsymbol{W}\boldsymbol{h}))_i
$$

对高斯可见层神经元，条件概率$P(\boldsymbol{h} = \boldsymbol{1}|\boldsymbol{v})$相同，而$P(\boldsymbol{v}|\boldsymbol{h})$为

$$
P(\boldsymbol{v}|\boldsymbol{h}) = N(\boldsymbol{v};\boldsymbol{b} + \boldsymbol{W}\boldsymbol{h}, \boldsymbol{I})
$$

### 1.4 受限玻尔兹曼机的属性

讲了这么多，RBM有什么用呢，一个RBM可以用来学习输入集合的概率分布。我们的目标是获得可见层变量的概率分布，也就是说可以将RBM作为其他模型的前置组件，经过RBM我们获得了输入数据的概率分布，依据这些信息，再构建一个模型进行训练，这样的模型训练效果应当更好。

首先定义一个自由能量的量

$$
F(\boldsymbol{v}) = - \log (\sum_{\boldsymbol{h}}\exp(-E(\boldsymbol{v}, \boldsymbol{h})))
$$

使用$F(\boldsymbol{v})$，我们可以把边缘概率$P(\boldsymbol{v})$写成

$$
P(\boldsymbol{v}) = \sum_{\boldsymbol{h}}P(\boldsymbol{v}, \boldsymbol{h})
\\
= \sum_\boldsymbol{h}\frac{\exp(-E(\boldsymbol{v}, \boldsymbol{h}))}{Z}
\\
=\frac{\exp(-F(\boldsymbol{v}))}{\sum_v\exp(-F(v))}
$$

若可见层取实值，则边缘概率密度为

$$
p_0(\boldsymbol{v}) = \frac{\exp(-\frac{1}{2}(\boldsymbol{v} - \boldsymbol{b})^T(\boldsymbol{v} - \boldsymbol{b}))}{Z_0}
$$

当RBM不包含隐藏层神经元时，这是一个均值为$\boldsymbol{b}$，方差为1的高斯分布。

若隐藏层神经元数量为$n$，我们可以得到

$$
p_n(\boldsymbol{v}) = \frac{\sum_\boldsymbol{h}\exp(-E_n(\boldsymbol{v}, \boldsymbol{h}))}{Z_n}
\\
=\frac{\prod^n_{i=1}\sum^1_{h_i=0}\exp(c_ih_i + h_i\boldsymbol{v}^T\boldsymbol{W}_{:,i})}{Z_n}
\\
=\frac{\prod^{n-1}_{i=1}\sum^1_{h_i=0}\exp(c_ih_i + h_i\boldsymbol{v}^T\boldsymbol{W}_{:,i})(1+ \exp(c_n+ \boldsymbol{v}^T\boldsymbol{W}_{:,n}))}{Z_n}
\\
= p_{n-1}(\boldsymbol{v})\frac{Z_{n-1}}{Z_n}(1+ \exp(c_n+ \boldsymbol{v}^T\boldsymbol{W}_{:,n}))
\\
= p_{n-1}(\boldsymbol{v})\frac{Z_{n-1}}{Z_n}+ p_{n-1}(\boldsymbol{v})\frac{Z_{n-1}}{Z_n}\exp(c_n+ \boldsymbol{v}^T\boldsymbol{W}_{:,n})
$$

上式是一个递归式，假如我们加入新的隐层神经元为第$n$个。我们将$p_{n-1}(\boldsymbol{v})\frac{Z_{n-1}}{Z_n}+ p_{n-1}(\boldsymbol{v})\frac{Z_{n-1}}{Z_n}\exp(c_n+ \boldsymbol{v}^T\boldsymbol{W}_{:,n})$分为两部分，前一部分是$n-1$个隐层神经元的混合高斯分布缩放，后一部分是增加新的隐层神经元导致的$n-1$个隐层神经元的混合高斯分布偏移，偏移量由$\boldsymbol{W}_{:,n}$决定，因此可以将$p_n(\boldsymbol{v})$看作是两个混合高斯模型的叠加。**由于此式是递归式**，也就意味着最终概率分布包含的高斯成分数量$N_n$与隐层神经元个数$n$相比是指数形式增长的（大概是$N_n = 2^n$）。

因此，RBM把可见层输入表示成了一个由多个方差为1的高斯分量组成的混合高斯模型，这些高斯分量的个数是指数级的。与GMM相比，RBM使用了更多的混合分量。然而，GMM可以为不同的高斯分量使用不同的方差来表示这个分布。结论就是高斯-伯努利RBM可以像混合高斯模型一样表示实值数据的分布，RBM可以替换GMM。

由于RBM的隐层神经元为二值向量，因此很适合用在词袋模型的任务中，比如根据文章内容，判断文章主题，可见层为文章内容，隐层神经元表示主题。因此可以用于推荐系统。

### 1.5 训练受限玻尔兹曼机

训练RBM使用随机梯度下降SGD来极小化负对数似然NLL

$$
J_{NLL}(\boldsymbol{W}, \boldsymbol{b}, \boldsymbol{c};\boldsymbol{v}) = -\log P(\boldsymbol{v}) = F(\boldsymbol{v}) + \log \sum_v\exp(-F(v))
$$

更新的参数$(\boldsymbol{W}, \boldsymbol{b}, \boldsymbol{c})$，$\eta$为学习率，更新方式相同

$$
\boldsymbol{W}_{t+1} \leftarrow \boldsymbol{W}_t - \eta \bigtriangleup \boldsymbol{W}_t 
$$

而且考虑惯性系数$\gamma$

$$
\bigtriangleup\boldsymbol{W}_t = \gamma\bigtriangleup\boldsymbol{W}_{t-1} + (1-\gamma)\frac{1}{M_b}\sum^{M_b}_{m=1}\bigtriangledown_{\boldsymbol{W}_t}J_{NLL}(\boldsymbol{W}, \boldsymbol{b}, \boldsymbol{c};\boldsymbol{v}^m)
$$

这里$M_b$是batch大小，$\bigtriangledown J$是负对数似然对参数的梯度，其他两个参数$\boldsymbol{b},\boldsymbol{c}$与上式相同。

与DNN不同，RBM的对数似然梯度不适合精确计算。负对数似然对于任意模型参数的导数的一般形式为

$$
\bigtriangledown_\theta J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle \frac{\partial E(\boldsymbol{v, h})}{\partial\theta} \right \rangle_{data} - \left \langle \frac{\partial E(\boldsymbol{v, h})}{\partial\theta} \right \rangle_{model}]
$$

$\left \langle x \right \rangle_{data}$和$\left \langle x \right \rangle_{model}$分别是从数据和最终模型中估计$x$的期望值。特别地，对于可见层神经元-隐层神经元的权重，有

$$
\bigtriangledown_{w_{ji}} J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle v_ih_j \right \rangle_{data} - \left \langle v_ih_j \right \rangle_{model}]
$$

第一个期望是训练数据中可见层神经元$v_i$和隐层神经元$h_j$同时取1的频率，第二个期望是以最终模型定义的分布来求得的。当隐层神经元未知时，第二个期望的计算时间是与高斯成分的数量相关，也就是指数级的，因此需要换一种方法。

最有效的是对比散度算法CD。对可见层神经元-隐藏层神经元权重的梯度的一步对比散度近似是

$$
\bigtriangledown_{w_{ji}} J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle v_ih_j \right \rangle_{data} - \left \langle v_ih_j \right \rangle_{\infty}]
\\
\approx - [\left \langle v_ih_j \right \rangle_{data} - \left \langle v_ih_j \right \rangle_{1}]
$$

这里$\left \langle \cdot \right \rangle_{\infty}$和$\left \langle \cdot \right \rangle_{1}$分别表示在吉布斯采样器运行了无穷次和一次之后得到的采样上估计的期望。

采样过程和对比散度算法：

* 第一步，吉布斯采样器通过一个数据样本初始化；
* 接着，依据之前证明的后验概率$P(\boldsymbol{h}|\boldsymbol{v})$由可见层采样生成一个隐藏层采样；
* 根据RBM类型是伯努利-伯努利RBM还是高斯-伯努利RBM，使用不同的公式定义的后验概率$P(\boldsymbol{v}|\boldsymbol{h})$，基于隐藏层采样继续生成一个可见层采样；
* 重复上述过程。

如果吉布斯采样器运行无穷次，则真实期望$\left \langle v_ih_j \right \rangle_{model}$可以从老化阶段之后生成的采样中估计

$$
\left \langle v_ih_j \right \rangle_{model} \approx \frac{1}{N} \sum^{N_{burn}+N}_{n=N_{burn}+1} v_i^nh_j^n
$$

这里$N_{burn}$是达到老化阶段所需的步数，$N$是老化之后的采样次数（可能是巨大的）。然而运行很多步吉布斯采样器是低效的。我们可以只运行一次，用一个非常粗略的近似$\left \langle v_ih_j \right \rangle_{1}$来估计$\left \langle v_ih_j \right \rangle_{model}$

然而$\left \langle v_ih_j \right \rangle_{1}$具有很大的方差。为了减小方差，我们可以基于以下公式估计$\left \langle v_ih_j \right \rangle_{model}$，这里取等号是因为二值向量

$$
\boldsymbol{h}^0 \sim P(\boldsymbol{h}|\boldsymbol{v}^0)
\\
\boldsymbol{v}^1 = \mathbb{E}(\boldsymbol{v}|\boldsymbol{h}^0) = P(\boldsymbol{v}|\boldsymbol{h}^0)
\\
\boldsymbol{h}^1 = \mathbb{E}(\boldsymbol{h}|\boldsymbol{v}^1) = P(\boldsymbol{h}|\boldsymbol{v}^1)
$$

这里$\sim$表示从中采样，$\boldsymbol{v}^0$是训练集的一个采样，我们采用平均场逼近方法直接生成采样$\boldsymbol{v}^1,\boldsymbol{h}^1$。换句话说，这些采样可以取实数值。同样的技巧也可以应用在

$$
\left \langle v_ih_j \right \rangle_{data} \approx \left \langle v_ih_j \right \rangle_{0} = v_i^0\mathbb{E}_j(\boldsymbol{h}|\boldsymbol{v}^0) = v_i^0P_j(\boldsymbol{h}|\boldsymbol{v}^0)
$$

在伯努利-伯努利RBM中，模型参数$\boldsymbol{b},\boldsymbol{c}$的更新规则可以简单地替换合适梯度导出。

$$
\bigtriangledown_{\boldsymbol{W}}J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle \boldsymbol{h}\boldsymbol{v}^T \right \rangle_{data} - \left \langle \boldsymbol{h}\boldsymbol{v}^T \right \rangle_{model}]
\\
\bigtriangledown_{\boldsymbol{b}}J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle \boldsymbol{v} \right \rangle_{data} - \left \langle \boldsymbol{v} \right \rangle_{model}]
\\
\bigtriangledown_{\boldsymbol{c}}J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle \boldsymbol{h} \right \rangle_{data} - \left \langle \boldsymbol{h} \right \rangle_{model}]
$$

CD算法也可以用来训练高斯-伯努利RBM。唯一的区别是，在高斯-伯努利RBM中，使用高斯分布的后验分布$P(\boldsymbol{v}|\boldsymbol{h})$的期望值$\mathbb{E}(\boldsymbol{v}|\boldsymbol{h})$。

> 使用对比散度算法训练RBM
> 
> 1.procedure TrainRBMWithCD(M个样本，N是CD数，吉布斯采样器迭代次数)
> 
> > 2.随机初始化$(\boldsymbol{W}_0,\boldsymbol{b}_0,\boldsymbol{c}_0)$
> 
> > 3.$while$ 停止训练准则未达到 $do$ （达到最大迭代次数或训练准则提升很小就停止）
> 
> > > 4.随机选择一个$M_b$个样本的小批量$\boldsymbol{O}$
> 
> > > 5.$\boldsymbol{V}^0 \leftarrow \boldsymbol{O}$
> 
> > > 6.$\boldsymbol{H}^0 \leftarrow P(\boldsymbol{H}|\boldsymbol{V}^0)$
> 
> > > 7.$\bigtriangledown_{\boldsymbol{W}}J \leftarrow \boldsymbol{H}^0(\boldsymbol{V}^0)^T$
> 
> > > 8.$\bigtriangledown_{\boldsymbol{b}}J \leftarrow sumrow(\boldsymbol{V}^0)$
> 
> > > 9.$\bigtriangledown_{\boldsymbol{c}}J \leftarrow sumrow(\boldsymbol{H}^0)$
> 
> > > 10.$for$ $n \leftarrow 0; n < N; n \leftarrow n + 1$ $do$
> 
> > > > 11.$\boldsymbol{H}^n \leftarrow \mathbb{I}(\boldsymbol{H}^n > rand(0, 1)) \quad$      采样，$\mathbb{I}$是指示函数
> 
> > > > 12.$\boldsymbol{V}^{n+1} \leftarrow P(\boldsymbol{V}|\boldsymbol{H}^n)$
> 
> > > > 13.$\boldsymbol{H}^{n+1} \leftarrow P(\boldsymbol{H}|\boldsymbol{V}^{n+1})$
> 
> > > 14.$end$ $for$
> 
> > > 15.$\bigtriangledown_{\boldsymbol{W}}J \leftarrow \bigtriangledown_{\boldsymbol{W}}J - \boldsymbol{H}^N(\boldsymbol{V}^N)^T$
> 
> > > 16.$\bigtriangledown_{\boldsymbol{b}}J \leftarrow \bigtriangledown_{\boldsymbol{b}}J - sumrow(\boldsymbol{V}^0)$
> 
> > > 17.$\bigtriangledown_{\boldsymbol{c}}J \leftarrow \bigtriangledown_{\boldsymbol{c}}J - sumrow(\boldsymbol{H}^0)$
> 
> > > 18.$\boldsymbol{W}_{t+1} \leftarrow \boldsymbol{W}_t + \frac{\eta}{M_b}\bigtriangleup\boldsymbol{W}_t$
> 
> > > 19.$\boldsymbol{b}_{t+1} \leftarrow \boldsymbol{b}_t + \frac{\eta}{M_b}\bigtriangleup\boldsymbol{b}_t$
> 
> > > 20.$\boldsymbol{c}_{t+1} \leftarrow \boldsymbol{c}_t + \frac{\eta}{M_b}\bigtriangleup\boldsymbol{c}_t$
> 
> > 21.$end$ $while$
> 
> > 22.返回 $rbm = (\boldsymbol{W}, \boldsymbol{b}, \boldsymbol{c})$
> 
> 23.$end$ $procedure$

## 2. 深度置信网络预训练

预训练的目的是为了初始化某个深度模型，比如DNN的初始值；这是由于神经网络对初始值敏感，不同的初始值可能会导致梯度下降到局部极小，但是通过预训练初始化的DNN在梯度下降过程中更容易达到全局最小。

{% asset_img dbn.png 深度置信网络 %}

深度置信网络（深度信念网络）DBN是第一批成功应用到深度架构训练的非卷积模型之一。DBN是从RBM衍生出的深度模型，首先我们知道一个RBM是由一个可见层和一个隐藏层组成的，那么这个隐藏层可以看作是更上一层的RBM的可见层，依此类推，我们就构建出一个深度置信网络DBN，RBM与RBM之间的权重是共享的。在这个网络中，顶层是一个无向图RBM，而下面的层次构成了一个有向图生成模型。

对于这样一个DBN，我们的训练过程是：对每个数据向量$\boldsymbol{v}$，我们先计算一个隐藏层神经元期望激活值的向量（它等价于概率）$\boldsymbol{h}$；我们把这些隐藏层期望值作为训练数据来训练一个新的RBM（更上一层）。这样，每个RBM的权重都可以用来从前一层的输出中提取特征。一旦我们停止训练，我们就拥有了一个DBN所有层权重的初始值，而这个DBN隐藏层的层数刚好等于我们训练的RBM的数量。这个DBN可以进一步通过wake-sleep算法模型进行精细调整。

具有$l$个隐藏层的DBN包含$l$个权重矩阵：$\boldsymbol{W}^{(1)},...,\boldsymbol{W}^{(l)}$。同时也包含$l+1$个偏置向量：$\boldsymbol{b}^{(0)},...,\boldsymbol{b}^{(l)}$，其中$\boldsymbol{b}^{(0)}$是可见层的偏置。DBN表示的概率分布由下式给出：

$$
P(\boldsymbol{h}^{(l)},\boldsymbol{h}^{(l-1)}) \propto (\boldsymbol{b}^{(l)^T}\boldsymbol{h}^{(l)} + \boldsymbol{b}^{(l-1)^T}\boldsymbol{h}^{(l-1)} + \boldsymbol{h}^{(l-1)^T}\boldsymbol{W}^{(l)}\boldsymbol{h}^{(l)})
\\
P(h_i^{(k)} = 1|\boldsymbol{h}^{(k+1)}) = \sigma(b_i^{(k)} + \boldsymbol{W}_{:,i}^{(k+1)^T}\boldsymbol{h}^{(k+1)}), \quad \forall i,\forall k \in 1,...,l-2
\\
P(v_i = 1|\boldsymbol{h}^{(1)}) = \sigma(b_i^{(0)} + \boldsymbol{W}_{:,i}^{(1)^T}\boldsymbol{h}^{(1)}), \quad \forall i
$$

在实值可见单元的情况下，替换

$$
\boldsymbol{v} \sim N(\boldsymbol{v};\boldsymbol{b}^{(0)} + \boldsymbol{W}^{(1)^T}\boldsymbol{h}^{(1)}, \boldsymbol{\beta}^{-1})
$$

为了便于处理，$\boldsymbol{\beta}$为对角形式。

训练过程表示：

* 先使用对比散度CD训练第一层RBM（可见层+第一隐藏层）以最大化$\mathbb{E}_{\boldsymbol{v}\sim p_{data}}\log p(\boldsymbol{v})$；
* 训练第二层RBM（第一隐藏层+第二隐藏层）为近似最大化$\mathbb{E}_{\boldsymbol{v}\sim p_{data}}\mathbb{E}_{\boldsymbol{h}^{(1)} \sim p^{(1)}(\boldsymbol{h}^{(1)}|\boldsymbol{v})}\log p^{(2)}(\boldsymbol{h}^{(1)})$，其中$p^{(1)}$是第一个RBM的概率分布，$p^{(2)}$是第二个RBM的概率分布。

上述过程可以无限重复，从而向DBN添加任意多层，这个过程可被视为提高数据在DBN下似然概率的变分下界。

我们会发现DBN计算过程中使用到了Sigmoid函数，十分类似由Sigmoid神经元构成的DNN。从这种视角看由Sigmoid神经元构成的DNN，我们发现DBN的权重可以用作DNN的初始权重。DBN和DNN之间的唯一区别实在DNN中使用了标注。基于此，在多分类DNN中，当预训练结束后，我们需要添加一个随机初始化的softmax输出层，并用反向传播算法鉴别性地精细调整网络中地所有权重。

使用DBN初始化DNN可能潜在地提高DNN在测试数据集上的性能的原因：

1. DNN是高度非线性且非凸的，初始化点可能很大程度地影响最终模型；
2. 预训练阶段使用的生成性准则与反向传播阶段使用的鉴别性准则不同，BP阶段相当于对模型进行了正则化；
3. 预训练可以使用大量未标注数据，提供了半监督学习的机会。

**实验证明，生成性预训练除了耗时以外，通常是有帮助的，在训练数据很小的时候格外有效。**

如果只使用一个隐藏层，DBN的预训练并不重要，预训练在有两个隐藏层的时候最有效。随着隐藏层数量的增加，预训练效果通常会减弱。这是因为DBN在预训练时使用了两个近似：

1. 在训练下一层的时候使用平均场逼近来生成目标；
2. 学习模型参数的时候使用了近似的对比散度算法。

显然，若使用DBN初始化基于ReLU的DNN，效果将大打折扣，因为两者并无联系。

## 3. 降噪自动编码器预训练

自动编码器是基于**没有标注**的训练数据集$\mathbb{S} = \{ (\boldsymbol{v}^m) | 1 \leqslant m \leqslant M \}$，找到一个$N_h$维隐藏层表示$\boldsymbol{h} = f(\boldsymbol{v})\in \mathbb{R}^{N_h \times 1}$，通过它可以使用最小均方误差MSE把初始化的$N_v$维信号$\boldsymbol{v}$重建为$\tilde{\boldsymbol{v}} = g(\boldsymbol{h})$。

$$
J_{MSE}(\boldsymbol{W, b};\mathbb{S}) = \frac{1}{M}\sum^M_{m=1}\frac{1}{2}||\tilde{\boldsymbol{v}}^m - \boldsymbol{v}^m||^2
$$

理论上$f$和$g$可以是任意函数。与主成分分析PCA不同的是，自动编码器有获取输入分布的多重模态属性的潜力。降噪自动编码器只有一个隐藏层。

与DBN不同的是，DBN是主动获取训练数据的统计规律，自动编码器是让其自由发展，尽可能地减小重建误差，在这个过程中，编码器获得了从输入数据到隐藏层的统计特征，解码器获得了重建目标的能力。

那么问题来了，当隐藏层表示的维度高于输入特征的维度时，自动编码器就存在一个潜在的问题。如果除了最小化重建误差以外没有其他限制，自动编码器可能只学习到恒等函数，而没有提取出任何训练数据集中出现的统计规律。

这个问题的解决方式：

* 对隐藏层添加稀疏性限制，强制使隐藏层大部分节点为0；
* 学习过程中添加随机扰动，在降噪自动编码器中使用这种方法，它强制隐藏层去发掘更多鲁棒特征，以及通过从一个损坏的版本重建输入以阻止它只学到恒等函数。

损坏输入的方式，最简单的机制是随机选择输入条目（一半条目）把它们设置为0。
一个降噪自动编码器做了两件事情：保存输入中信息，并撤销随即损坏的影响。
注意到在RBM的对比散度训练过程中，采样步骤本质上执行的就是对输入的随机损坏过程。

类似与使用RBM，降噪自动编码器也可以用来预训练一个DNN，步骤类似，一层一层训练叠加。

## 4. 鉴别性预训练

基于DBN以及降噪自动编码的预训练都是生成性预训练技术，其关键在于依靠其他判别准则生成初始化参数，鉴别性预训练DPT的训练方式完全不同。

最明显的一种是逐层BP，首先使用标注鉴别性训练一个单隐层的DNN，直到全部收敛；接着在第一隐层和输出层之间插入一个新的随机初始化的第二隐层，同样的训练至完全收敛；这样继续插入训练的过程直到得到所需数量的隐藏层。在绝大多数的条件下，逐层BP性能优于逐层贪心算法，这是由于逐层贪心不考虑之前的数据，它只对当前隐藏层负责。

但是，逐层BP的缺点是一些隐藏节点可能在训练收敛后会处于饱和状态，因此当新的隐藏层加入时很难对其进行进一步更新。这个限制可以通过每次加入新的隐藏层时，不让模型训练至收敛来缓解。一个典型的启发式方法是我们只使用要达到收敛所用数据的$\frac{1}{L}$来执行DPT，其中$L$是最终模型的总层数，在DPT中，其目标是调整权重使其接近一个较好的局部最优点，因此，DPT最好在可获得大量训练数据的时候使用。

## 5. 混合预训练

生成性预训练和鉴别性预训练各有千秋。生成性预训练没有和任务特定的目标函数绑定，它有助于减轻过拟合但不保证有助于鉴别性的模型精细化调整（BP）；鉴别性预训练直接最小化目标函数，然而如果没有规划好，低层权重可能向最终目标调整过多，而没有考虑到接下来添加的隐藏层。

为了解决上述问题，可以采用混合预训练方法。

$$
J_{HYB}(\boldsymbol{W, b};\mathbb{S}) = J_{DISC}(\boldsymbol{W, b};\mathbb{S}) + \alpha J_{GEN}(\boldsymbol{W, b};\mathbb{S})
$$

$\alpha$是插值权重。对于分类任务，鉴别性准则可以是交叉熵，对于回归任务，鉴别性准则可以是MSE。对于RBM，生成性准则可以是负对数似然，对于自动编码器，生成性准则可以是重建误差。

## 6. 采用丢弃法的预训练

dropout可以提升泛化性能，那么这启发我们使用dropout进行预训练。
使用0.3到0.5的dropout率，然后通过10到20轮训练数据来预训练一个DNN，接着把dropout设置为0继续训练DNN。这样初始化的DNN的错误率比RBM预训练的DNN相对降低了3%。
