---
title: 语音识别-RBM和DBN
date: 2018-11-29 09:20:44
categories:
- speech recognition
tags:
- theory
- RBM
- DBN
- auto encoder
mathjax: true
---

参考：

> 《解析深度学习-语音识别实践》第5章 高级模型初始化技术
> [《深度学习》第20章 深度生成模型](https://github.com/exacity/deeplearningbook-chinese)

## 1. 受限玻尔兹曼机

在介绍受限玻尔兹曼机RBM之前，先了解一下玻尔兹曼机。

### 1.1 玻尔兹曼机

玻尔兹曼机最初作为一种广义的“联结主义”引入，用来学习二值向量上的任意概率分布。也就是说，我们的目标是计算一组二值向量数据（只包含0和1）的概率分布情况。

首先在$d$维二值随机向量$\boldsymbol{x} \in \{0,1 \}^{d \times 1}$上定义玻尔兹曼机。玻尔兹曼机是一种基于能量的模型（玄学？），意味着我们可以使用能量函数定义联合概率分布：

$$
P(\boldsymbol{x}) = \frac{\exp (-E(\boldsymbol{x}))}{Z}
$$

其中$E(\boldsymbol{x})$是能量函数，$Z$是确保概率和为1的配分函数。玻尔兹曼机的能量函数如下：

$$
E(\boldsymbol{x}) = -\boldsymbol{x}^T\boldsymbol{U}\boldsymbol{x} - \boldsymbol{b}^T\boldsymbol{x}
$$

其中$\boldsymbol{U}$是模型参数的权重矩阵，$\boldsymbol{b}$是偏置向量。

在一般情况下，根据观察到的样本的情况，我们可以很容易计算上面的表达式，但是它限制了观察到的变量和权重矩阵描述的变量之间相互作用的类型。具体来说，这意味着一个单元的概率由
其他单元值的线性模型（逻辑回归）给出。

当不是所有变量都能被观察到时，玻尔兹曼机作用更强。我们将单元分为两个子集：可见单元$\boldsymbol{v}$和隐藏单元$\boldsymbol{h}$。则能量函数变为

$$
E(\boldsymbol{v}, \boldsymbol{h}) = -\boldsymbol{v}^T\boldsymbol{R}\boldsymbol{v} - \boldsymbol{v}^T\boldsymbol{W}\boldsymbol{h}-\boldsymbol{h}^T\boldsymbol{S}\boldsymbol{h} - \boldsymbol{b}^T\boldsymbol{v} - \boldsymbol{c}^T\boldsymbol{h}
$$

玻尔兹曼机的学习算法通常基于最大似然，当基于最大似然的学习规则训练时，连接两个单元的特定权重的更新仅取决于这两个单元在不同分布下收集的统计信息：$P_{model}(\boldsymbol{v})$和$\hat{P}_{data}(\boldsymbol{v})P_{model}(\boldsymbol{h}|\boldsymbol{v})$。网络的其余部分参与塑造这些统计信息，但权重可以在完全不知道网络其余部分的情况下更新。

### 1.2 受限玻尔兹曼机

{% asset_img rbm0.png 受限玻尔兹曼机 %}

RBM是包含一层可观察变量和单层潜变量的无向概率图模型，它是一个二分图，观察层或潜层中的任何单元之间不允许存在连接。令观察层由一组$n_v$个二值随机变量组成，我们统称为向量$\boldsymbol{v} \in \{0, 1\}^{n_v \times 1}$。我们将$n_h$个二值随机变量的潜在或隐藏层记为$\boldsymbol{h} \in \{0, 1\}^{n_h \times 1}$，$\boldsymbol{b}\in \{0, 1\}^{n_v \times 1}$和$\boldsymbol{c} \in \{0, 1\}^{n_h \times 1}$分别是可见层和隐藏层的偏置向量。其能量函数为

$$
E(\boldsymbol{v}, \boldsymbol{h}) = -\boldsymbol{b}^T\boldsymbol{v} - \boldsymbol{c}^T\boldsymbol{h} - \boldsymbol{v}^T\boldsymbol{W}\boldsymbol{h}
$$

若$\boldsymbol{v} \in \mathbb{R}^{n_v \times1}$，即可见层取实值，则能量函数变为

$$
E(\boldsymbol{v}, \boldsymbol{h}) = \frac{1}{2}(\boldsymbol{v} - \boldsymbol{b})^T(\boldsymbol{v} - \boldsymbol{b}) - \boldsymbol{c}^T\boldsymbol{h} - \boldsymbol{v}^T\boldsymbol{W}\boldsymbol{h}
$$

其联合概率分布为

$$
P(\boldsymbol{v}, \boldsymbol{h}) = \frac{\exp (-E(\boldsymbol{v}, \boldsymbol{h}))}{Z}
\\
Z = \sum_\boldsymbol{v}\sum_\boldsymbol{h}\exp \{ -E(\boldsymbol{v},\boldsymbol{h}) \}
$$

显然，若能穷举状态，则计算$Z$是很方便的，但是问题在于很难穷举所有状态，因此需要更好的方法计算$Z$。

### 1.3 条件分布

要注意的是，我们的目标是挖掘观测变量的潜在信息，需要求解的是观测变量分布概率，然而$P(\boldsymbol{v})$是难解的，但基于隐藏变量的条件分布的采样和计算是相对简单的。

$$
P(\boldsymbol{h}|\boldsymbol{v}) = \frac{P(\boldsymbol{h}, \boldsymbol{v})}{P(\boldsymbol{v})}
\\
= \frac{1}{P(\boldsymbol{v})} \frac{1}{Z} \exp (\boldsymbol{b}^T\boldsymbol{v} + \boldsymbol{c}^T\boldsymbol{h} + \boldsymbol{v}^T\boldsymbol{W}\boldsymbol{h})
\\
= \frac{1}{Z'}\exp (\boldsymbol{c}^T\boldsymbol{h} + \boldsymbol{v}^T\boldsymbol{W}\boldsymbol{h}), \quad Z' = \frac{\partial Z}{\partial \boldsymbol{h}}
\\
=\frac{1}{Z'}\exp (\sum^{n_h}_{j=1}c_jh_j + \sum^{n_h}_{j=1}\boldsymbol{v}^T\boldsymbol{W}_{:,j}h_j)
\\
=\frac{1}{Z'} \prod^{n_h}_{j=1}\exp (c_jh_j + \boldsymbol{v}^T\boldsymbol{W}_{:,j}h_j)
\\
= \prod^{n_h}_{j=1}P(h_j|\boldsymbol{v})
$$

又因为$h_j$是二值的，所以

$$
P(h_j = 1|\boldsymbol{v}) = \frac{\tilde{P}(h_j = 1|\boldsymbol{v})}{\tilde{P}(h_j = 0|\boldsymbol{v}) + \tilde{P}(h_j = 1|\boldsymbol{v})}
\\
= \frac{\exp(c_j + \boldsymbol{v}^T\boldsymbol{W}_{:,j})}{\exp(0) + \exp(c_j + \boldsymbol{v}^T\boldsymbol{W}_{:,j})}
\\
=\sigma(c_j + \boldsymbol{v}^T\boldsymbol{W}_{:,j})
$$

$\sigma$是Sigmoid函数。

扩展一下

$$
P(\boldsymbol{h} = \boldsymbol{1}|\boldsymbol{v}) = \sigma(\boldsymbol{c} + \boldsymbol{W}^T\boldsymbol{v}) 
$$

由此得到关于隐藏层的完全条件分布表达为因子形式

$$
P(\boldsymbol{h}|\boldsymbol{v}) = \prod^{n_h}_{j=1}\sigma((2\boldsymbol{h}-1)\odot(\boldsymbol{c} + \boldsymbol{W}^T\boldsymbol{v}))_j
$$

$\boldsymbol{v}$和$\boldsymbol{h}$是类似的，可以得到

$$
P(\boldsymbol{v}|\boldsymbol{h}) = \prod^{n_v}_{i=1}\sigma((2\boldsymbol{v}-1)\odot(\boldsymbol{b} + \boldsymbol{W}\boldsymbol{h}))_i
$$

对高斯可见层神经元，条件概率$P(\boldsymbol{h} = \boldsymbol{1}|\boldsymbol{v})$相同，而$P(\boldsymbol{v}|\boldsymbol{h})$为

$$
P(\boldsymbol{v}|\boldsymbol{h}) = N(\boldsymbol{v};\boldsymbol{b} + \boldsymbol{W}\boldsymbol{h}, \boldsymbol{I})
$$

### 1.4 受限玻尔兹曼机的属性

讲了这么多，RBM有什么用呢，一个RBNM可以用来学习输入集合的概率分布。我们的目标是获得可见层变量的概率分布，也就是说可以将RBM作为其他模型的前置组件，经过RBM我们获得了输入数据的概率分布，依据这些信息，再构建一个模型进行训练，这样的模型训练效果应当更好。

首先定义一个自由能量的量

$$
F(\boldsymbol{v}) = - \log (\sum_{\boldsymbol{h}}\exp(-E(\boldsymbol{v}, \boldsymbol{h})))
$$

使用$F(\boldsymbol{v})$，我们可以把边缘概率$P(\boldsymbol{v})$写成

$$
P(\boldsymbol{v}) = \sum_{\boldsymbol{h}}P(\boldsymbol{v}, \boldsymbol{h})
\\
= \sum_\boldsymbol{h}\frac{\exp(-E(\boldsymbol{v}, \boldsymbol{h}))}{Z}
\\
=\frac{\exp(-F(\boldsymbol{v}))}{\sum_v\exp(-F(v))}
$$

若可见层取实值，则边缘概率密度为

$$
p_0(\boldsymbol{v}) = \frac{\exp(-\frac{1}{2}(\boldsymbol{v} - \boldsymbol{b})^T(\boldsymbol{v} - \boldsymbol{b}))}{Z_0}
$$

当RBM不包含隐藏层神经元时，这是一个均值为$\boldsymbol{b}$，方差为1的高斯分布。

若隐藏层神经元数量为$n$，我们可以得到

$$
p_n(\boldsymbol{v}) = \frac{\sum_\boldsymbol{h}\exp(-E_n(\boldsymbol{v}, \boldsymbol{h}))}{Z_n}
\\
=\frac{\prod^n_{i=1}\sum^1_{h_i=0}\exp(c_ih_i + h_i\boldsymbol{v}^T\boldsymbol{W}_{:,i})}{Z_n}
\\
=\frac{\prod^{n-1}_{i=1}\sum^1_{h_i=0}\exp(c_ih_i + h_i\boldsymbol{v}^T\boldsymbol{W}_{:,i})(1+ \exp(c_n+ \boldsymbol{v}^T\boldsymbol{W}_{:,n}))}{Z_n}
\\
= p_{n-1}(\boldsymbol{v})\frac{Z_{n-1}}{Z_n}(1+ \exp(c_n+ \boldsymbol{v}^T\boldsymbol{W}_{:,n}))
\\
= p_{n-1}(\boldsymbol{v})\frac{Z_{n-1}}{Z_n}+ p_{n-1}(\boldsymbol{v})\frac{Z_{n-1}}{Z_n}\exp(c_n+ \boldsymbol{v}^T\boldsymbol{W}_{:,n})
$$

上式是一个递归式，假如我们加入新的隐层神经元为第$n$个。我们将$p_{n-1}(\boldsymbol{v})\frac{Z_{n-1}}{Z_n}+ p_{n-1}(\boldsymbol{v})\frac{Z_{n-1}}{Z_n}\exp(c_n+ \boldsymbol{v}^T\boldsymbol{W}_{:,n})$分为两部分，前一部分是$n-1$个隐层神经元的混合高斯分布缩放，后一部分是增加新的隐层神经元导致的$n-1$个隐层神经元的混合高斯分布偏移，偏移量由$\boldsymbol{W}_{:,n}$决定，因此可以将$p_n(\boldsymbol{v})$看作是两个混合高斯模型的叠加。**由于此式是递归式**，也就意味着最终概率分布包含的高斯成分数量$N_n$与隐层神经元个数$n$相比是指数形式增长的（大概是$N_n = 2^n$）。

因此，RBM把可见层输入表示成了一个由多个方差为1的高斯分量组成的混合高斯模型，这些高斯分量的个数是指数级的。与GMM相比，RBM使用了更多的混合分量。然而，GMM可以为不同的高斯分量使用不同的方差来表示这个分布。结论就是高斯-伯努利RBM可以像混合高斯模型一样表示实值数据的分布，RBM可以替换GMM。

由于RBM的隐层神经元为二值向量，因此很适合用在词袋模型的任务中，比如根据文章内容，判断文章主题，可见层为文章内容，隐层神经元表示主题。因此可以用于推荐系统。

### 1.5 训练受限玻尔兹曼机

训练RBM使用随机梯度下降SGD来极小化负对数似然NLL

$$
J_{NLL}(\boldsymbol{W}, \boldsymbol{b}, \boldsymbol{c};\boldsymbol{v}) = -\log P(\boldsymbol{v}) = F(\boldsymbol{v}) + \log \sum_v\exp(-F(v))
$$

更新的参数$(\boldsymbol{W}, \boldsymbol{b}, \boldsymbol{c})$，$\eta$为学习率，更新方式相同

$$
\boldsymbol{W}_{t+1} \leftarrow \boldsymbol{W}_t - \eta \bigtriangleup \boldsymbol{W}_t 
$$

而且考虑惯性系数$\gamma$

$$
\bigtriangleup\boldsymbol{W}_t = \gamma\bigtriangleup\boldsymbol{W}_{t-1} + (1-\gamma)\frac{1}{M_b}\sum^{M_b}_{m=1}\bigtriangledown_{\boldsymbol{W}_t}J_{NLL}(\boldsymbol{W}, \boldsymbol{b}, \boldsymbol{c};\boldsymbol{v}^m)
$$

这里$M_b$是batch大小，$\bigtriangledown J$是负对数似然对参数的梯度，其他两个参数$\boldsymbol{b},\boldsymbol{c}$与上式相同。

与DNN不同，RBM的对数似然梯度不适合精确计算。负对数似然对于任意模型参数的导数的一般形式为

$$
\bigtriangledown_\theta J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle \frac{\partial E(\boldsymbol{v, h})}{\partial\theta} \right \rangle_{data} - \left \langle \frac{\partial E(\boldsymbol{v, h})}{\partial\theta} \right \rangle_{model}]
$$

$\left \langle x \right \rangle_{data}$和$\left \langle x \right \rangle_{model}$分别是从数据和最终模型中估计$x$的期望值。特别地，对于可见层神经元-隐层神经元的权重，有

$$
\bigtriangledown_{w_{ji}} J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle v_ih_j \right \rangle_{data} - \left \langle v_ih_j \right \rangle_{model}]
$$

第一个期望是训练数据中可见层神经元$v_i$和隐层神经元$h_j$同时取1的频率，第二个期望是以最终模型定义的分布来求得的。当隐层神经元未知时，第二个期望的计算时间是与高斯成分的数量相关，也就是指数级的，因此需要换一种方法。

最有效的是对比散度算法CD。对可见层神经元-隐藏层神经元权重的梯度的一步对比散度近似是

$$
\bigtriangledown_{w_{ji}} J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle v_ih_j \right \rangle_{data} - \left \langle v_ih_j \right \rangle_{\infty}]
\\
\approx - [\left \langle v_ih_j \right \rangle_{data} - \left \langle v_ih_j \right \rangle_{1}]
$$

这里$\left \langle \cdot \right \rangle_{\infty}$和$\left \langle \cdot \right \rangle_{1}$分别表示在吉布斯采样器运行了无穷次和一次之后得到的采样上估计的期望。

采样过程和对比散度算法：

* 第一步，吉布斯采样器通过一个数据样本初始化；
* 接着，依据之前证明的后验概率$P(\boldsymbol{h}|\boldsymbol{v})$由可见层采样生成一个隐藏层采样；
* 根据RBM类型是伯努利-伯努利RBM还是高斯-伯努利RBM，使用不同的公式定义的后验概率$P(\boldsymbol{v}|\boldsymbol{h})$，基于隐藏层采样继续生成一个可见层采样；
* 重复上述过程。

如果吉布斯采样器运行无穷次，则真实期望$\left \langle v_ih_j \right \rangle_{model}$可以从老化阶段之后生成的采样中估计

$$
\left \langle v_ih_j \right \rangle_{model} \approx \frac{1}{N} \sum^{N_{burn}+N}_{n=N_{burn}+1} v_i^nh_j^n
$$

这里$N_{burn}$是达到老化阶段所需的步数，$N$是老化之后的采样次数（可能是巨大的）。然而运行很多步吉布斯采样器是低效的。我们可以只运行一次，用一个非常粗略的近似$\left \langle v_ih_j \right \rangle_{1}$来估计$\left \langle v_ih_j \right \rangle_{model}$

然而$\left \langle v_ih_j \right \rangle_{1}$具有很大的方差。为了减小方差，我们可以基于以下公式估计$\left \langle v_ih_j \right \rangle_{model}$，这里取等号是因为二值向量

$$
\boldsymbol{h}^0 \sim P(\boldsymbol{h}|\boldsymbol{v}^0)
\\
\boldsymbol{v}^1 = \mathbb{E}(\boldsymbol{v}|\boldsymbol{h}^0) = P(\boldsymbol{v}|\boldsymbol{h}^0)
\\
\boldsymbol{h}^1 = \mathbb{E}(\boldsymbol{h}|\boldsymbol{v}^1) = P(\boldsymbol{h}|\boldsymbol{v}^1)
$$

这里$\sim$表示从中采样，$\boldsymbol{v}^0$是训练集的一个采样，我们采用平均场逼近方法直接生成采样$\boldsymbol{v}^1,\boldsymbol{h}^1$。换句话说，这些采样可以取实数值。同样的技巧也可以应用在

$$
\left \langle v_ih_j \right \rangle_{data} \approx \left \langle v_ih_j \right \rangle_{0} = v_i^0\mathbb{E}_j(\boldsymbol{h}|\boldsymbol{v}^0) = v_i^0P_j(\boldsymbol{h}|\boldsymbol{v}^0)
$$

在伯努利-伯努利RBM中，模型参数$\boldsymbol{b},\boldsymbol{c}$的更新规则可以简单地替换合适梯度导出。

$$
\bigtriangledown_{\boldsymbol{W}}J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle \boldsymbol{h}\boldsymbol{v}^T \right \rangle_{data} - \left \langle \boldsymbol{h}\boldsymbol{v}^T \right \rangle_{model}]
\\
\bigtriangledown_{\boldsymbol{b}}J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle \boldsymbol{v} \right \rangle_{data} - \left \langle \boldsymbol{v} \right \rangle_{model}]
\\
\bigtriangledown_{\boldsymbol{c}}J_{NLL}(\boldsymbol{W, b, c;v}) = - [\left \langle \boldsymbol{h} \right \rangle_{data} - \left \langle \boldsymbol{h} \right \rangle_{model}]
$$

CD算法也可以用来训练高斯-伯努利RBM。唯一的区别是，在高斯-伯努利RBM中，使用高斯分布的后验分布$P(\boldsymbol{v}|\boldsymbol{h})$的期望值$\mathbb{E}(\boldsymbol{v}|\boldsymbol{h})$。

> 使用对比散度算法训练RBM
> 
> 1.procedure TrainRBMWithCD(M个样本，N是CD数，吉布斯采样器迭代次数)
> 
> > 2.随机初始化$(\boldsymbol{W}_0,\boldsymbol{b}_0,\boldsymbol{c}_0)$
> 
> > 3.$while$ 停止训练准则未达到 $do$ （达到最大迭代次数或训练准则提升很小就停止）
> 
> > > 4.随机选择一个$M_b$个样本的小批量$\boldsymbol{O}$
> 
> > > 5.$\boldsymbol{V}^0 \leftarrow \boldsymbol{O}$
> 
> > > 6.$\boldsymbol{H}^0 \leftarrow P(\boldsymbol{H}|\boldsymbol{V}^0)$
> 
> > > 7.$\bigtriangledown_{\boldsymbol{W}}J \leftarrow \boldsymbol{H}^0(\boldsymbol{V}^0)^T$
> 
> > > 8.$\bigtriangledown_{\boldsymbol{b}}J \leftarrow sumrow(\boldsymbol{V}^0)$
> 
> > > 9.$\bigtriangledown_{\boldsymbol{c}}J \leftarrow sumrow(\boldsymbol{H}^0)$
> 
> > > 10.$for$ $n \leftarrow 0; n < N; n \leftarrow n + 1$ $do$
> 
> > > > 11.$\boldsymbol{H}^n \leftarrow \mathbb{I}(\boldsymbol{H}^n > rand(0, 1)) \quad$      采样，$\mathbb{I}$是指示函数
> 
> > > > 12.$\boldsymbol{V}^{n+1} \leftarrow P(\boldsymbol{V}|\boldsymbol{H}^n)$
> 
> > > > 13.$\boldsymbol{H}^{n+1} \leftarrow P(\boldsymbol{H}|\boldsymbol{V}^{n+1})$
> 
> > > 14.$end$ $for$
> 
> > > 15.$\bigtriangledown_{\boldsymbol{W}}J \leftarrow \bigtriangledown_{\boldsymbol{W}}J - \boldsymbol{H}^N(\boldsymbol{V}^N)^T$
> 
> > > 16.$\bigtriangledown_{\boldsymbol{b}}J \leftarrow \bigtriangledown_{\boldsymbol{b}}J - sumrow(\boldsymbol{V}^0)$
> 
> > > 17.$\bigtriangledown_{\boldsymbol{c}}J \leftarrow \bigtriangledown_{\boldsymbol{c}}J - sumrow(\boldsymbol{H}^0)$
> 
> > > 18.$\boldsymbol{W}_{t+1} \leftarrow \boldsymbol{W}_t + \frac{\eta}{M_b}\bigtriangleup\boldsymbol{W}_t$
> 
> > > 19.$\boldsymbol{b}_{t+1} \leftarrow \boldsymbol{b}_t + \frac{\eta}{M_b}\bigtriangleup\boldsymbol{b}_t$
> 
> > > 20.$\boldsymbol{c}_{t+1} \leftarrow \boldsymbol{c}_t + \frac{\eta}{M_b}\bigtriangleup\boldsymbol{c}_t$
> 
> > 21.$end$ $while$
> 
> > 22.返回 $rbm = (\boldsymbol{W}, \boldsymbol{b}, \boldsymbol{c})$
> 
> 23.$end$ $procedure$

## 2. 深度置信网络预训练


## 3. 降噪自动编码器预训练


## 4. 鉴别性预训练


## 5. 混合预训练


## 6. 采用丢弃法的预训练
