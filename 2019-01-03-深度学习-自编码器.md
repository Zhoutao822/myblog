---
title: 深度学习-自编码器
date: 2019-01-03 21:25:43
categories:
- Deep Learning
tags:
- Theory
- DAE
- CAE
mathjax: true
---

参考：

> [《深度学习》第14章 自编码器](https://github.com/exacity/deeplearningbook-chinese)

自编码器autoencoder是神经网络的一种，经过训练后能尝试将输入复制到输出。

自编码器可以看作两个部分：一个由函数$\boldsymbol{h} = f(\boldsymbol{x})$表示的编码器和一个生成重构的解码器$\boldsymbol{r}=g(\boldsymbol{h})$。但是如果一个自编码器只是简单地学会处处设置为$g(f(\boldsymbol{x})) = \boldsymbol{x}$，那么这个自编码器毫无意义。相反我们不应该将自编码器设计成输入到输出完全相等。通常需要加入一些约束，使得自编码器的输出与训练数据的输入相似而不相同。

{% asset_img 0.png %}

<!-- more -->

## 1. 欠完备自编码器

通过限制$\boldsymbol{h}$的维度比$\boldsymbol{x}$小，这种编码维度小于输入维度的自编码器称为**欠完备**自编码器。显然，欠完备自编码器可以实现两个功能：降维和学习数据分布中最显著的特征。

损失函数为

$$
L(\boldsymbol{x}, g(f(\boldsymbol{x})))
$$

当解码器是线性的且$L$是均方误差，欠完备的自编码器会学习出与PCA相同的生成子空间。但是若编码器和解码器被赋予过大的容量，自编码器会执行简单的复制任务而捕捉不到任何有关数据分布的有用信息。

## 2. 正则自编码器

若隐藏编码的维输允许与输入相等，或隐藏编码维输大于输入的过完备情况下，编码器和解码器很有可能仅仅是复制数据而学习不到数据分布的有用信息。

正则自编码器通过使用带约束的损失函数避免上述问题的产生。

### 2.1 稀疏自编码器

稀疏自编码器在训练时增加编码层的稀疏惩罚$\Omega(\boldsymbol{h})$：

$$
L(\boldsymbol{x}, g(f(\boldsymbol{x}))) + \Omega(\boldsymbol{h})
$$

上式如何理解，首先我们知道贝叶斯近似推断认为，正则化的惩罚对应于模型参数的先验概率分布，也就说最大化对数似然对应最大化$p(\boldsymbol{\theta}|\boldsymbol{x})$，相当于最大化$\log p(\boldsymbol{x}|\boldsymbol{\theta}) + \log p(\boldsymbol{\theta})$。$\log p(\boldsymbol{x}|\boldsymbol{\theta})$即通常的数据似然项，参数的对数先验项$\log p(\boldsymbol{\theta})$则包含了对$\boldsymbol{\theta}$特定值的偏好。但是正则自编码器不适用这样的解释是因为正则项取决于数据（损失$L$与惩罚$\Omega$面向对象不同）。虽然我们任可以认为正则项隐式地表达了对函数的偏好。

从$\boldsymbol{h}$的角度来看，我们可以将$\boldsymbol{h}$作为潜变量，而整个稀疏自编码器框架是对潜变量$\boldsymbol{h}$的生成模型的近似最大似然训练。对于具有明确联合分布$p_{model}(\boldsymbol{x}, \boldsymbol{h}) = p_{model}(\boldsymbol{h})p_{model}(\boldsymbol{x}|\boldsymbol{h})$的模型，可以将$p_{model}(\boldsymbol{h})$视为模型关于潜变量的先验分布，表示模型看到$\boldsymbol{x}$的信念先验。对数似然可分解为

$$
\log p_{model}(\boldsymbol{x}) = \log \sum_{\boldsymbol{h}}p_{model}(\boldsymbol{h}, \boldsymbol{x})
$$

我们可以认为自编码器使用一个高似然值$\boldsymbol{h}$的点估计近似这个总和。但是$\boldsymbol{h}$是参数编码器的输出，从这个角度，我们根据这个选择的$\boldsymbol{h}$，最大化如下

$$
\log p_{model}(\boldsymbol{h},\boldsymbol{x}) = \log p_{model}(\boldsymbol{h}) + \log p_{model}(\boldsymbol{x}|\boldsymbol{h})
$$

$\log p_{model}(\boldsymbol{h})$项能被稀疏诱导。如Laplace先验

$$
p_{model}(h_i) = \frac{\lambda}{2} e^{-\lambda |h_t|}
$$

对应于绝对值稀疏惩罚。将对数先验表示为绝对值惩罚，得到

$$
\Omega(\boldsymbol{h}) = \lambda \sum_i |h_i|
\\
-\log p_{model}(\boldsymbol{h}) = \sum_i(\lambda|h_i| - \log \frac{\lambda}{2}) = \Omega(\boldsymbol{h}) + const
$$

这里的常数项只跟$\lambda$有关，通常将$\lambda$视为超参数，因此可以丢弃不影响参数学习的常数项。

从稀疏性导致$p_{model}(\boldsymbol{h})$学习成近似最大似然的结果看，稀疏惩罚完全不是一个正则项。这仅仅影响模型关于潜变量的分布。这个观点提供了训练自编码器的另一个动机：这是近似训练生成模型的一种途径。这也给出了为什么自编码器学到的特征是有用的另一个解释：它们描述的潜变量可以解释输入。

### 2.2 去噪自编码器

去噪自编码器DAE最小化

$$
L(\boldsymbol{x}, g(f(\tilde{\boldsymbol{x}})))
$$

其中$\tilde{\boldsymbol{x}}$是被某种噪声损坏的$\boldsymbol{x}$的副本。因此去噪自编码器必须撤销这些损坏，而不是简单地复制输入。

### 2.3 惩罚导数作为正则

类似稀疏自编码器中的惩罚项$\Omega$

$$
L(\boldsymbol{x}, g(f(\boldsymbol{x}))) + \Omega(\boldsymbol{h}, \boldsymbol{x})
$$

但是$\Omega$的形式不同

$$
\Omega(\boldsymbol{h}, \boldsymbol{x}) = \lambda\sum_i ||\bigtriangledown_{\boldsymbol{x}}h_i||^2
$$

这迫使模型学习一个在$\boldsymbol{x}$变化小时目标也没有太大变化的函数。因为这个惩罚只对训练数据适用，它迫使自编码器学习可以反映训练数据分布信息的特征。

这样正则化的自编码器被称为收缩自编码器（contractive autoencoder, CAE）。这种方法与去噪自编码器、流形学习和概率模型存在一定理论联系。

## 3. 表示能力、层的大小和深度

自编码器通常只有单层的编码器和解码器，但这不是必然的。实际上深度编码器和解码器能提供更多优势。

万能近似定理保证至少有一层隐藏层且隐藏单元足够多的前馈神经网络能以任意精度近似任意函数（在很大范围里），这是非平凡深度（至少有一层隐藏层）的一个主要优点。这意味着具有单隐藏层的自编码器在数据域内能表示任意近似数据的恒等函数。但是，从输入到编码的映射是浅层的。这意味这我们不能任意添加约束，比如约束编码稀疏。深度自编码器（编码器至少包含一层额外隐藏层）在给定足够多的隐藏单元的情况下，能以任意精度近似任何从输入到编码的映射。

深度可以指数地降低表示某些函数的计算成本。深度也能指数地减少学习一些函数所需的训练数据量。

实验中，深度自编码器能比相应的浅层或线性自编码器产生更好的压缩效率。

训练深度自编码器的普遍策略是训练一堆浅层的自编码器来贪心地预训练相应的深度架构。所以即使最终目标是训练深度自编码器，我们也经常会遇到浅层自编码器。

## 4. 随机编码器和解码器

在给定隐藏编码$\boldsymbol{h}$的条件下，我们可以认为解码器提供了一个条件分布$p_{model}(\boldsymbol{x}|\boldsymbol{h})$。接着根据最小化$-\log p_{decoder}(\boldsymbol{x}|\boldsymbol{h})$来训练自编码器。损失函数的具体形式视$p_{decoder}$的形式而定：

1. $\boldsymbol{x}$为连续实值，通常使用ReLU线性输出单元参数化高斯分布的均值，此时负对数似然对应均方误差；
2. $\boldsymbol{x}$为二值变量，对应伯努利分布，通常由Sigmoid输出单元确定；
3. $\boldsymbol{x}$为离散有限值，则对应Softmax分布，以此类推。

与此同时，我们可以将**编码函数**$f(\boldsymbol{x})$的概念推广为**编码分布**$p_{encoder}(\boldsymbol{h}|\boldsymbol{x})$。

任何潜变量模型$p_{model}(\boldsymbol{h}, \boldsymbol{x})$定义一个随机编码器

$$
p_{encoder}(\boldsymbol{h}|\boldsymbol{x}) = p_{model}(\boldsymbol{h}|\boldsymbol{x})
$$

以及一个随机解码器

$$
p_{decoder}(\boldsymbol{x}|\boldsymbol{h}) = p_{model}(\boldsymbol{x}|\boldsymbol{h})
$$

{% asset_img 1.png %}

通常情况下，编码器和解码器的分布没有必要是与唯一一个联合分布$p_{model}(\boldsymbol{x}; \boldsymbol{h})$相容的条件分布。

## 5. 去噪自编码器

**去噪自编码器**DAE是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的自编码器。

{% asset_img 2.png %}

DAE的训练过程如上图所示，引入了一个损坏过程$C(\tilde{\boldsymbol{x}}|\boldsymbol{x})$，这个条件分布代表给定数据样本$\boldsymbol{x}$产生损坏样本$\tilde{\boldsymbol{x}}$的概率。自编码器则根据一下过程，从训练数据对$(\boldsymbol{x}, \tilde{\boldsymbol{x}})$中学习重构分布$P_{reconstruct}(\boldsymbol{x}|\tilde{\boldsymbol{x}})$：

1. 从训练数据中采一个训练样本$\boldsymbol{x}$；
2. 从$C(\tilde{\boldsymbol{x}}|\boldsymbol{x})$采一个损坏样本$\tilde{\boldsymbol{x}}$；
3. 将$(\boldsymbol{x}, \tilde{\boldsymbol{x}})$作为训练样本来估计自编码器的重构分布$P_{reconstruct}(\boldsymbol{x}|\tilde{\boldsymbol{x}}) = p_{decoder}(\boldsymbol{x}|\boldsymbol{h})$。

通常我们可以简单地对负对数似然$\log p_{decoder}(\boldsymbol{x} | \boldsymbol{h})$进行基于梯度法（如小批量梯度下降）的近似最小化。只要编码器是确定性的，去噪自编码器就是一个前馈网络，并且可以使用与其他前馈网络完全相同的方式进行训练。

因此我们可以认为DAE是在以下期望下进行随机梯度下降：

$$
-\mathbb{E}_{\boldsymbol{x} \sim \hat{p}_{data}(\boldsymbol{x})}\mathbb{E}_{\tilde{\boldsymbol{x}} \sim C(\tilde{\boldsymbol{x}}|\boldsymbol{x})} \log p_{decoder}(\boldsymbol{x}|\boldsymbol{h} = f(\tilde{\boldsymbol{x}}))
$$

其中$\hat{p}_{data}(\boldsymbol{x})$是训练数据的分布。

### 5.1 得分估计

得分匹配是最大似然的代替。它提供了概率分布的一致估计，促使模型在各个数据点$\boldsymbol{x}$上获得与数据分布相同的**得分**。在这种情况下，得分是一个特定的梯度场：

$$
\bigtriangledown_{\boldsymbol{x}} \log p(\boldsymbol{x})
$$

对于现在讨论的自编码器，理解学习$\log p_{data}$的梯度场是学习$p_{data}$结构的一种方式就足够了。

DAE的训练准则（条件高斯$p(\boldsymbol{x}|\boldsymbol{h})$）能让自编码器学到能估计数据分布得分的向量场$g(f(\boldsymbol{x})) -\boldsymbol{x}$，这是DAE的一个重要特性。如下图所示

{% asset_img 3.png %}

对一类采用高斯噪声和均方误差作为重构误差的特定去噪自编码器（具有sigmoid 隐藏单元和线性重构单元）的去噪训练过程，与训练一类特定的被称为RBM的无向概率模型是等价的。

对于现在的讨论，我们只需要知道这个模型能显示的给出$p_{model}(\boldsymbol{x};\boldsymbol{\theta})$。当RBM使用**去噪得分匹配**算法训练时，它的学习算法与训练对应的去噪自编码器是等价的。在一个确定的噪声水平下，正则化的得分匹配不是一致估计量；相反它会恢复分布的一个模糊版本。然而，当噪声水平趋向于0且训练样本数趋向于无穷时，一致性就会恢复。

自编码器和RBM还存在其他联系。在RBM上应用得分匹配后，其代价函数将等价于重构误差结合类似CAE惩罚的正则项。自编码器的梯度是对RBM对比散度训练的近似。

对于连续的$\boldsymbol{x}$，高斯损坏和重构分布的去噪准则得到的得分估计适用于一般编码器和解码器的参数化。这意味着一个使用平方误差准则

$$
||g(f(\tilde{\boldsymbol{x}})) - \boldsymbol{x}||^2
$$

和噪声方差为$\sigma^2$的损坏

$$
C(\tilde{\boldsymbol{x}} | \boldsymbol{x}) = N(\tilde{\boldsymbol{x}};\mu=\boldsymbol{x},\sum=\sigma^2 I)
$$

的通用编码器-解码器架构可以用来训练估计得分。下图展示其中的工作原理

{% asset_img 4.png %}

一般情况下，不能保证重构函数$g(f(\boldsymbol{x}))$减去输入$\boldsymbol{x}$后对应于某个函数的梯度，更不用说得分。



## 6. 使用自编码器学习流形




## 7. 收缩自编码器




## 8. 预测稀疏分解





## 9. 自编码器的应用

















