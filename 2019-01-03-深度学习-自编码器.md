---
title: 深度学习-自编码器
date: 2019-01-03 21:25:43
categories:
- Deep Learning
tags:
- Theory
- DAE
- CAE
mathjax: true
---

参考：

> [《深度学习》第14章 自编码器](https://github.com/exacity/deeplearningbook-chinese)

自编码器autoencoder是神经网络的一种，经过训练后能尝试将输入复制到输出。

自编码器可以看作两个部分：一个由函数$\boldsymbol{h} = f(\boldsymbol{x})$表示的编码器和一个生成重构的解码器$\boldsymbol{r}=g(\boldsymbol{h})$。但是如果一个自编码器只是简单地学会处处设置为$g(f(\boldsymbol{x})) = \boldsymbol{x}$，那么这个自编码器毫无意义。相反我们不应该将自编码器设计成输入到输出完全相等。通常需要加入一些约束，使得自编码器的输出与训练数据的输入相似而不相同。

{% asset_img 0.png %}

<!-- more -->

## 1. 欠完备自编码器

通过限制$\boldsymbol{h}$的维度比$\boldsymbol{x}$小，这种编码维度小于输入维度的自编码器称为**欠完备**自编码器。显然，欠完备自编码器可以实现两个功能：降维和学习数据分布中最显著的特征。

损失函数为

$$
L(\boldsymbol{x}, g(f(\boldsymbol{x})))
$$

当解码器是线性的且$L$是均方误差，欠完备的自编码器会学习出与PCA相同的生成子空间。但是若编码器和解码器被赋予过大的容量，自编码器会执行简单的复制任务而捕捉不到任何有关数据分布的有用信息。

## 2. 正则自编码器

若隐藏编码的维输允许与输入相等，或隐藏编码维输大于输入的过完备情况下，编码器和解码器很有可能仅仅是复制数据而学习不到数据分布的有用信息。

正则自编码器通过使用带约束的损失函数避免上述问题的产生。

### 2.1 稀疏自编码器

稀疏自编码器在训练时增加编码层的稀疏惩罚$\Omega(\boldsymbol{h})$：

$$
L(\boldsymbol{x}, g(f(\boldsymbol{x}))) + \Omega(\boldsymbol{h})
$$

上式如何理解，首先我们知道贝叶斯近似推断认为，正则化的惩罚对应于模型参数的先验概率分布，也就说最大化对数似然对应最大化$p(\boldsymbol{\theta}|\boldsymbol{x})$，相当于最大化$\log p(\boldsymbol{x}|\boldsymbol{\theta}) + \log p(\boldsymbol{\theta})$。$\log p(\boldsymbol{x}|\boldsymbol{\theta})$即通常的数据似然项，参数的对数先验项$\log p(\boldsymbol{\theta})$则包含了对$\boldsymbol{\theta}$特定值的偏好。但是正则自编码器不适用这样的解释是因为正则项取决于数据（损失$L$与惩罚$\Omega$面向对象不同）。虽然我们任可以认为正则项隐式地表达了对函数的偏好。

从$\boldsymbol{h}$的角度来看，我们可以将$\boldsymbol{h}$作为潜变量，而整个稀疏自编码器框架是对潜变量$\boldsymbol{h}$的生成模型的近似最大似然训练。对于具有明确联合分布$p_{model}(\boldsymbol{x}, \boldsymbol{h}) = p_{model}(\boldsymbol{h})p_{model}(\boldsymbol{x}|\boldsymbol{h})$的模型，可以将$p_{model}(\boldsymbol{h})$视为模型关于潜变量的先验分布，表示模型看到$\boldsymbol{x}$的信念先验。对数似然可分解为

$$
\log p_{model}(\boldsymbol{x}) = \log \sum_{\boldsymbol{h}}p_{model}(\boldsymbol{h}, \boldsymbol{x})
$$

我们可以认为自编码器使用一个高似然值$\boldsymbol{h}$的点估计近似这个总和。但是$\boldsymbol{h}$是参数编码器的输出，从这个角度，我们根据这个选择的$\boldsymbol{h}$，最大化如下

$$
\log p_{model}(\boldsymbol{h},\boldsymbol{x}) = \log p_{model}(\boldsymbol{h}) + \log p_{model}(\boldsymbol{x}|\boldsymbol{h})
$$

$\log p_{model}(\boldsymbol{h})$项能被稀疏诱导。如Laplace先验

$$
p_{model}(h_i) = \frac{\lambda}{2} e^{-\lambda |h_t|}
$$

对应于绝对值稀疏惩罚。将对数先验表示为绝对值惩罚，得到

$$
\Omega(\boldsymbol{h}) = \lambda \sum_i |h_i|
\\
-\log p_{model}(\boldsymbol{h}) = \sum_i(\lambda|h_i| - \log \frac{\lambda}{2}) = \Omega(\boldsymbol{h}) + const
$$

这里的常数项只跟$\lambda$有关，通常将$\lambda$视为超参数，因此可以丢弃不影响参数学习的常数项。

从稀疏性导致$p_{model}(\boldsymbol{h})$学习成近似最大似然的结果看，稀疏惩罚完全不是一个正则项。这仅仅影响模型关于潜变量的分布。这个观点提供了训练自编码器的另一个动机：这是近似训练生成模型的一种途径。这也给出了为什么自编码器学到的特征是有用的另一个解释：它们描述的潜变量可以解释输入。

### 2.2 去噪自编码器

去噪自编码器DAE最小化

$$
L(\boldsymbol{x}, g(f(\tilde{\boldsymbol{x}})))
$$

其中$\tilde{\boldsymbol{x}}$是被某种噪声损坏的$\boldsymbol{x}$的副本。因此去噪自编码器必须撤销这些损坏，而不是简单地复制输入。

### 2.3 惩罚导数作为正则

类似稀疏自编码器中的惩罚项$\Omega$

$$
L(\boldsymbol{x}, g(f(\boldsymbol{x}))) + \Omega(\boldsymbol{h}, \boldsymbol{x})
$$

但是$\Omega$的形式不同

$$
\Omega(\boldsymbol{h}, \boldsymbol{x}) = \lambda\sum_i ||\bigtriangledown_{\boldsymbol{x}}h_i||^2
$$

这迫使模型学习一个在$\boldsymbol{x}$变化小时目标也没有太大变化的函数。因为这个惩罚只对训练数据适用，它迫使自编码器学习可以反映训练数据分布信息的特征。

这样正则化的自编码器被称为收缩自编码器（contractive autoencoder, CAE）。这种方法与去噪自编码器、流形学习和概率模型存在一定理论联系。

## 3. 表示能力、层的大小和深度

自编码器通常只有单层的编码器和解码器，但这不是必然的。实际上深度编码器和解码器能提供更多优势。

万能近似定理保证至少有一层隐藏层且隐藏单元足够多的前馈神经网络能以任意精度近似任意函数（在很大范围里），这是非平凡深度（至少有一层隐藏层）的一个主要优点。这意味着具有单隐藏层的自编码器在数据域内能表示任意近似数据的恒等函数。但是，从输入到编码的映射是浅层的。这意味这我们不能任意添加约束，比如约束编码稀疏。深度自编码器（编码器至少包含一层额外隐藏层）在给定足够多的隐藏单元的情况下，能以任意精度近似任何从输入到编码的映射。

深度可以指数地降低表示某些函数的计算成本。深度也能指数地减少学习一些函数所需的训练数据量。

实验中，深度自编码器能比相应的浅层或线性自编码器产生更好的压缩效率。

训练深度自编码器的普遍策略是训练一堆浅层的自编码器来贪心地预训练相应的深度架构。所以即使最终目标是训练深度自编码器，我们也经常会遇到浅层自编码器。

## 4. 随机编码器和解码器




## 5. 去噪自编码器



## 6. 使用自编码器学习流行




## 7. 收缩自编码器




## 8. 预测稀疏分解





## 9. 自编码器的应用

















