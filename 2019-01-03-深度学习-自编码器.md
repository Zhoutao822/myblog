---
title: 深度学习-自编码器
date: 2019-01-03 21:25:43
categories:
- Deep Learning
tags:
- Theory
mathjax: true
---

参考：

> [《深度学习》第14章 自编码器](https://github.com/exacity/deeplearningbook-chinese)

自编码器autoencoder是神经网络的一种，经过训练后能尝试将输入复制到输出。

自编码器可以看作两个部分：一个由函数$\boldsymbol{h} = f(\boldsymbol{x})$表示的编码器和一个生成重构的解码器$\boldsymbol{r}=g(\boldsymbol{h})$。但是如果一个自编码器只是简单地学会处处设置为$g(f(\boldsymbol{x})) = \boldsymbol{x}$，那么这个自编码器毫无意义。相反我们不应该将自编码器设计成输入到输出完全相等。通常需要加入一些约束，使得自编码器的输出与训练数据的输入相似而不相同。

{% asset_img 0.png %}

<!-- more -->

## 1. 欠完备自编码器

通过限制$\boldsymbol{h}$的维度比$\boldsymbol{x}$小，这种编码维度小于输入维度的自编码器称为**欠完备**自编码器。显然，欠完备自编码器可以实现两个功能：降维和学习数据中最显著的特征。

损失函数为

$$
L(\boldsymbol{x}, g(f(\boldsymbol{x})))
$$

当解码器是线性的且$L$是均方误差，欠完备的自编码器会学习出与PCA相同的生成子空间。但是若编码器和解码器被赋予过大的容量，自编码器会执行简单的复制任务而捕捉不到任何有关数据分布的有用信息。

## 2. 正则自编码器



## 3. 表示能力、层的大小和深度



## 4. 随机编码器和解码器




## 5. 去噪自编码器



## 6. 使用自编码器学习流行




## 7. 收缩自编码器




## 8. 预测稀疏分解





## 9. 自编码器的应用

















