---
title: 深度学习-自编码器
date: 2019-01-03 21:25:43
categories:
- Deep Learning
tags:
- Theory
- DAE
- CAE
- PSD
mathjax: true
---

参考：

> [《深度学习》第14章 自编码器](https://github.com/exacity/deeplearningbook-chinese)

自编码器autoencoder是神经网络的一种，经过训练后能尝试将输入复制到输出。

自编码器可以看作两个部分：一个由函数$\boldsymbol{h} = f(\boldsymbol{x})$表示的编码器和一个生成重构的解码器$\boldsymbol{r}=g(\boldsymbol{h})$。但是如果一个自编码器只是简单地学会处处设置为$g(f(\boldsymbol{x})) = \boldsymbol{x}$，那么这个自编码器毫无意义。相反我们不应该将自编码器设计成输入到输出完全相等。通常需要加入一些约束，使得自编码器的输出与训练数据的输入相似而不相同。

{% asset_img 0.png %}

<!-- more -->

## 1. 欠完备自编码器

通过限制$\boldsymbol{h}$的维度比$\boldsymbol{x}$小，这种编码维度小于输入维度的自编码器称为**欠完备**自编码器。显然，欠完备自编码器可以实现两个功能：降维和学习数据分布中最显著的特征。

损失函数为

$$
L(\boldsymbol{x}, g(f(\boldsymbol{x})))
$$

当解码器是线性的且$L$是均方误差，欠完备的自编码器会学习出与PCA相同的生成子空间。但是若编码器和解码器被赋予过大的容量，自编码器会执行简单的复制任务而捕捉不到任何有关数据分布的有用信息。

## 2. 正则自编码器

若隐藏编码的维输允许与输入相等，或隐藏编码维输大于输入的过完备情况下，编码器和解码器很有可能仅仅是复制数据而学习不到数据分布的有用信息。

正则自编码器通过使用带约束的损失函数避免上述问题的产生。

### 2.1 稀疏自编码器

稀疏自编码器在训练时增加编码层的稀疏惩罚$\Omega(\boldsymbol{h})$：

$$
L(\boldsymbol{x}, g(f(\boldsymbol{x}))) + \Omega(\boldsymbol{h})
$$

上式如何理解，首先我们知道贝叶斯近似推断认为，正则化的惩罚对应于模型参数的先验概率分布，也就说最大化对数似然对应最大化$p(\boldsymbol{\theta}|\boldsymbol{x})$，相当于最大化$\log p(\boldsymbol{x}|\boldsymbol{\theta}) + \log p(\boldsymbol{\theta})$。$\log p(\boldsymbol{x}|\boldsymbol{\theta})$即通常的数据似然项，参数的对数先验项$\log p(\boldsymbol{\theta})$则包含了对$\boldsymbol{\theta}$特定值的偏好。但是正则自编码器不适用这样的解释是因为正则项取决于数据（损失$L$与惩罚$\Omega$面向对象不同）。虽然我们任可以认为正则项隐式地表达了对函数的偏好。

从$\boldsymbol{h}$的角度来看，我们可以将$\boldsymbol{h}$作为潜变量，而整个稀疏自编码器框架是对潜变量$\boldsymbol{h}$的生成模型的近似最大似然训练。对于具有明确联合分布$p_{model}(\boldsymbol{x}, \boldsymbol{h}) = p_{model}(\boldsymbol{h})p_{model}(\boldsymbol{x}|\boldsymbol{h})$的模型，可以将$p_{model}(\boldsymbol{h})$视为模型关于潜变量的先验分布，表示模型看到$\boldsymbol{x}$的信念先验。对数似然可分解为

$$
\log p_{model}(\boldsymbol{x}) = \log \sum_{\boldsymbol{h}}p_{model}(\boldsymbol{h}, \boldsymbol{x})
$$

我们可以认为自编码器使用一个高似然值$\boldsymbol{h}$的点估计近似这个总和。但是$\boldsymbol{h}$是参数编码器的输出，从这个角度，我们根据这个选择的$\boldsymbol{h}$，最大化如下

$$
\log p_{model}(\boldsymbol{h},\boldsymbol{x}) = \log p_{model}(\boldsymbol{h}) + \log p_{model}(\boldsymbol{x}|\boldsymbol{h})
$$

$\log p_{model}(\boldsymbol{h})$项能被稀疏诱导。如Laplace先验

$$
p_{model}(h_i) = \frac{\lambda}{2} e^{-\lambda |h_t|}
$$

对应于绝对值稀疏惩罚。将对数先验表示为绝对值惩罚，得到

$$
\Omega(\boldsymbol{h}) = \lambda \sum_i |h_i|
\\
-\log p_{model}(\boldsymbol{h}) = \sum_i(\lambda|h_i| - \log \frac{\lambda}{2}) = \Omega(\boldsymbol{h}) + const
$$

这里的常数项只跟$\lambda$有关，通常将$\lambda$视为超参数，因此可以丢弃不影响参数学习的常数项。

从稀疏性导致$p_{model}(\boldsymbol{h})$学习成近似最大似然的结果看，稀疏惩罚完全不是一个正则项。这仅仅影响模型关于潜变量的分布。这个观点提供了训练自编码器的另一个动机：这是近似训练生成模型的一种途径。这也给出了为什么自编码器学到的特征是有用的另一个解释：它们描述的潜变量可以解释输入。

### 2.2 去噪自编码器

去噪自编码器DAE最小化

$$
L(\boldsymbol{x}, g(f(\tilde{\boldsymbol{x}})))
$$

其中$\tilde{\boldsymbol{x}}$是被某种噪声损坏的$\boldsymbol{x}$的副本。因此去噪自编码器必须撤销这些损坏，而不是简单地复制输入。

### 2.3 惩罚导数作为正则

类似稀疏自编码器中的惩罚项$\Omega$

$$
L(\boldsymbol{x}, g(f(\boldsymbol{x}))) + \Omega(\boldsymbol{h}, \boldsymbol{x})
$$

但是$\Omega$的形式不同

$$
\Omega(\boldsymbol{h}, \boldsymbol{x}) = \lambda\sum_i ||\bigtriangledown_{\boldsymbol{x}}h_i||^2
$$

这迫使模型学习一个在$\boldsymbol{x}$变化小时目标也没有太大变化的函数。因为这个惩罚只对训练数据适用，它迫使自编码器学习可以反映训练数据分布信息的特征。

这样正则化的自编码器被称为收缩自编码器（contractive autoencoder, CAE）。这种方法与去噪自编码器、流形学习和概率模型存在一定理论联系。

## 3. 表示能力、层的大小和深度

自编码器通常只有单层的编码器和解码器，但这不是必然的。实际上深度编码器和解码器能提供更多优势。

万能近似定理保证至少有一层隐藏层且隐藏单元足够多的前馈神经网络能以任意精度近似任意函数（在很大范围里），这是非平凡深度（至少有一层隐藏层）的一个主要优点。这意味着具有单隐藏层的自编码器在数据域内能表示任意近似数据的恒等函数。但是，从输入到编码的映射是浅层的。这意味这我们不能任意添加约束，比如约束编码稀疏。深度自编码器（编码器至少包含一层额外隐藏层）在给定足够多的隐藏单元的情况下，能以任意精度近似任何从输入到编码的映射。

深度可以指数地降低表示某些函数的计算成本。深度也能指数地减少学习一些函数所需的训练数据量。

实验中，深度自编码器能比相应的浅层或线性自编码器产生更好的压缩效率。

训练深度自编码器的普遍策略是训练一堆浅层的自编码器来贪心地预训练相应的深度架构。所以即使最终目标是训练深度自编码器，我们也经常会遇到浅层自编码器。

## 4. 随机编码器和解码器

在给定隐藏编码$\boldsymbol{h}$的条件下，我们可以认为解码器提供了一个条件分布$p_{model}(\boldsymbol{x}|\boldsymbol{h})$。接着根据最小化$-\log p_{decoder}(\boldsymbol{x}|\boldsymbol{h})$来训练自编码器。损失函数的具体形式视$p_{decoder}$的形式而定：

1. $\boldsymbol{x}$为连续实值，通常使用ReLU线性输出单元参数化高斯分布的均值，此时负对数似然对应均方误差；
2. $\boldsymbol{x}$为二值变量，对应伯努利分布，通常由Sigmoid输出单元确定；
3. $\boldsymbol{x}$为离散有限值，则对应Softmax分布，以此类推。

与此同时，我们可以将**编码函数**$f(\boldsymbol{x})$的概念推广为**编码分布**$p_{encoder}(\boldsymbol{h}|\boldsymbol{x})$。

任何潜变量模型$p_{model}(\boldsymbol{h}, \boldsymbol{x})$定义一个随机编码器

$$
p_{encoder}(\boldsymbol{h}|\boldsymbol{x}) = p_{model}(\boldsymbol{h}|\boldsymbol{x})
$$

以及一个随机解码器

$$
p_{decoder}(\boldsymbol{x}|\boldsymbol{h}) = p_{model}(\boldsymbol{x}|\boldsymbol{h})
$$

{% asset_img 1.png %}

通常情况下，编码器和解码器的分布没有必要是与唯一一个联合分布$p_{model}(\boldsymbol{x}; \boldsymbol{h})$相容的条件分布。

## 5. 去噪自编码器

**去噪自编码器**DAE是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的自编码器。

{% asset_img 2.png %}

DAE的训练过程如上图所示，引入了一个损坏过程$C(\tilde{\boldsymbol{x}}|\boldsymbol{x})$，这个条件分布代表给定数据样本$\boldsymbol{x}$产生损坏样本$\tilde{\boldsymbol{x}}$的概率。自编码器则根据一下过程，从训练数据对$(\boldsymbol{x}, \tilde{\boldsymbol{x}})$中学习重构分布$P_{reconstruct}(\boldsymbol{x}|\tilde{\boldsymbol{x}})$：

1. 从训练数据中采一个训练样本$\boldsymbol{x}$；
2. 从$C(\tilde{\boldsymbol{x}}|\boldsymbol{x})$采一个损坏样本$\tilde{\boldsymbol{x}}$；
3. 将$(\boldsymbol{x}, \tilde{\boldsymbol{x}})$作为训练样本来估计自编码器的重构分布$P_{reconstruct}(\boldsymbol{x}|\tilde{\boldsymbol{x}}) = p_{decoder}(\boldsymbol{x}|\boldsymbol{h})$。

通常我们可以简单地对负对数似然$\log p_{decoder}(\boldsymbol{x} | \boldsymbol{h})$进行基于梯度法（如小批量梯度下降）的近似最小化。只要编码器是确定性的，去噪自编码器就是一个前馈网络，并且可以使用与其他前馈网络完全相同的方式进行训练。

因此我们可以认为DAE是在以下期望下进行随机梯度下降：

$$
-\mathbb{E}_{\boldsymbol{x} \sim \hat{p}_{data}(\boldsymbol{x})}\mathbb{E}_{\tilde{\boldsymbol{x}} \sim C(\tilde{\boldsymbol{x}}|\boldsymbol{x})} \log p_{decoder}(\boldsymbol{x}|\boldsymbol{h} = f(\tilde{\boldsymbol{x}}))
$$

其中$\hat{p}_{data}(\boldsymbol{x})$是训练数据的分布。

### 5.1 得分估计

得分匹配是最大似然的代替。它提供了概率分布的一致估计，促使模型在各个数据点$\boldsymbol{x}$上获得与数据分布相同的**得分**。在这种情况下，得分是一个特定的梯度场：

$$
\bigtriangledown_{\boldsymbol{x}} \log p(\boldsymbol{x})
$$

对于现在讨论的自编码器，理解学习$\log p_{data}$的梯度场是学习$p_{data}$结构的一种方式就足够了。

DAE的训练准则（条件高斯$p(\boldsymbol{x}|\boldsymbol{h})$）能让自编码器学到能估计数据分布得分的向量场$g(f(\boldsymbol{x})) -\boldsymbol{x}$，这是DAE的一个重要特性。如下图所示

{% asset_img 3.png %}

对一类采用高斯噪声和均方误差作为重构误差的特定去噪自编码器（具有sigmoid 隐藏单元和线性重构单元）的去噪训练过程，与训练一类特定的被称为RBM的无向概率模型是等价的。

对于现在的讨论，我们只需要知道这个模型能显示的给出$p_{model}(\boldsymbol{x};\boldsymbol{\theta})$。当RBM使用**去噪得分匹配**算法训练时，它的学习算法与训练对应的去噪自编码器是等价的。在一个确定的噪声水平下，正则化的得分匹配不是一致估计量；相反它会恢复分布的一个模糊版本。然而，当噪声水平趋向于0且训练样本数趋向于无穷时，一致性就会恢复。

自编码器和RBM还存在其他联系。在RBM上应用得分匹配后，其代价函数将等价于重构误差结合类似CAE惩罚的正则项。自编码器的梯度是对RBM对比散度训练的近似。

对于连续的$\boldsymbol{x}$，高斯损坏和重构分布的去噪准则得到的得分估计适用于一般编码器和解码器的参数化。这意味着一个使用平方误差准则

$$
||g(f(\tilde{\boldsymbol{x}})) - \boldsymbol{x}||^2
$$

和噪声方差为$\sigma^2$的损坏

$$
C(\tilde{\boldsymbol{x}} | \boldsymbol{x}) = N(\tilde{\boldsymbol{x}};\mu=\boldsymbol{x},\sum=\sigma^2 I)
$$

的通用编码器-解码器架构可以用来训练估计得分。下图展示其中的工作原理

{% asset_img 4.png %}

一般情况下，不能保证重构函数$g(f(\boldsymbol{x}))$减去输入$\boldsymbol{x}$后对应于某个函数的梯度，更不用说得分。

目前为止我们所讨论的仅限于去噪自编码器如何学习表示一个概率分布。更一般的，我们可能希望使用自编码器作为生成模型，并从其分布中进行采样。

“去噪自编码器”的命名指的不仅仅是学习去噪，而且可以学到一个好的内部表示（作为学习去噪的副效用）。这个想法提出较晚。学习到的表示可以被用来预训练更深的无监督网络或监督网络。与稀疏自编码器、稀疏编码、收缩自编码器等正则化的自编码器类似，DAE的动机是允许学习容量很高的编码器，同时防止在编码器和解码器学习一个无用的恒等函数。

## 6. 使用自编码器学习流形

自编码器跟其他很多机器学习算法一样，也利用了数据集中在一个低维流形或者一小组这样的流形的思想。其中一些机器学习算法仅能学习到在流形上表现良好但给定不在流形上的输入会导致异常的函数。自编码器进一步借此想法，旨在学习流形的结构。

流形的一个重要特征是切平面（tangent plane）的集合。$d$维流形上的一点$\boldsymbol{x}$，切平面由能张成流形上允许变动的局部方向的$d$维基向量给出。如下图所示，这些局部方向决定了我们能如何微小地变动$\boldsymbol{x}$ 而保持于流形上。

{% asset_img 5.png %}

所有自编码器的训练过程涉及两种推动力的折衷：

1. 学习训练样本$\boldsymbol{x}$的表示$\boldsymbol{h}$使得$\boldsymbol{x}$能通过解码器近似地从$\boldsymbol{h}$中恢复。$\boldsymbol{x}$是从训练数据挑出的这一事实很关键，因为这意味着自编码器不需要成功重构不属于数据生成分布下的输入。
2. 满足约束或正则惩罚。这可以是限制自编码器容量的架构约束，也可以是加入到重构代价的一个正则项。这些技术一般倾向那些对输入较不敏感的解。

重要的原则是，自编码器必须有能力表示重构训练实例所需的变化。如果该数据生成分布集中靠近一个低维流形，自编码器能隐式产生捕捉这个流形局部坐标系的表示：仅在$\boldsymbol{x}$周围关于流形的相切变化需要对应于$\boldsymbol{h} = f(\boldsymbol{x})$中的变化。因此，编码器学习从输入空间$\boldsymbol{x}$到表示空间的映射，映射仅对沿着流形方向的变化敏感，并且对流形正交方向的变化不敏感。

下图的例子说明，我们可以通过构建对数据点周围的输入扰动不敏感的重构函数，使得自编码器恢复流形结构。

{% asset_img 6.png %}

为了理解自编码器可用于流形学习的原因，我们可以将自编码器和其他方法进行对比。学习表征流形最常见的是流形上（或附近）数据点的表示（representation）。对于特定的实例，这样的表示也被称为嵌入。它通常由一个低维向量给出，具有比这个流形的“外围”空间更少的维数。有些算法（下面讨论的非参数流形学习算法）直接学习每个训练样例的嵌入，而其他算法学习更一般的映射（有时被称为编码器或表示函数），将周围空间（输入空间）的任意点映射到它的嵌入。

流形学习大多专注于试图捕捉到这些流形的无监督学习过程。最初始的学习非线性流形的机器学习研究专注基于**最近邻图**（nearest neighbor graph）的**非参数**（non-parametric）方法。该图中每个训练样例对应一个节点，它的边连接近邻点对。这些方法将每个节点与张成实例和近邻之间的差向量变化方向的切平面相关联。

{% asset_img 7.png %}

全局坐标系则可以通过优化或求解线性系统获得。下图展示了如何通过大量局部线性的类高斯样平铺（或“薄煎饼”，因为高斯块在切平面方向是扁平的）得到一个流形。

{% asset_img 8.png %}

然而，Bengio and Monperrus (2005) 指出了这些局部非参数方法应用于流形学习的根本困难：如果流形不是很光滑（它们有许多波峰、波谷和曲折），为覆盖其中的每一个变化，我们可能需要非常多的训练样本，导致没有能力泛化到没见过的变化。实际上，这些方法只能通过内插，概括相邻实例之间流形的形状。不幸的是，AI 问题中涉及的流形可能具有非常复杂的结构，难以仅从局部插值捕获特征。考虑图14.6 转换所得的流形样例。如果我们只观察输入向量内的一个坐标$x_i$，当平移图像，我们可以观察到当这个坐标遇到波峰或波谷时，图像的亮度也会经历一个波峰或波谷。换句话说，底层图像模板亮度的模式复杂性决定执行简单的图像变换所产生的流形的复杂性。这是采用分布式表示和深度学习捕获流形结构的动机。

## 7. 收缩自编码器

收缩自编码器CAE再编码$\boldsymbol{h} = f(\boldsymbol{x})$的基础上添加了显示的正则项，鼓励$f$的导数尽可能小：

$$
\Omega(\boldsymbol{h}) = \lambda ||\frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{x}}||^2_F
$$

惩罚项$\Omega$为平方Frobenius 范数（元素平方之和），作用于与编码器的函数相关偏导数的Jacobian矩阵。

去噪自编码器和收缩自编码器之间存在一定联系：Alain and Bengio (2013) 指出在小高斯噪声的限制下，当重构函数将$\boldsymbol{x}$映射到$\boldsymbol{r} = g(f(\boldsymbol{x}))$时，去噪重构误差与收缩惩罚项是等价的。换句话说，去噪自编码器能抵抗小且有限的输入扰动，而收缩自编码器使特征提取函数能抵抗极小的输入扰动。

收缩（contractive）源于CAE弯曲空间的方式。具体来说，由于CAE训练为抵抗输入扰动，鼓励将输入点邻域映射到输出点处更小的邻域。我们能认为这是将输入的邻域收缩到更小的输出邻域。

说得更清楚一点，CAE只在局部收缩-一个训练样本$\boldsymbol{x}$的所有扰动都映射到$f(\boldsymbol{x})$的附近。全局来看，两个不同的点$\boldsymbol{x}$和$\boldsymbol{x}'$会分别被映射到远离原点的两个点$f(\boldsymbol{x})$和$f(\boldsymbol{x})$。$f$扩展到数据流形的中间或远处是合理的（见图14.7 中小例子的情况）。当$\Omega(\boldsymbol{h})$惩罚应用于sigmoid单元时，收缩Jacobian 的简单方式是令sigmoid趋向饱和的0或1。这鼓励CAE使用sigmoid的极值编码输入点，或许可以解释为二进制编码。它也保证了CAE可以穿过大部分sigmoid隐藏单元能张成的超立方体，进而扩散其编码值。

我们可以认为点$\boldsymbol{x}$处的Jacobian 矩阵$\boldsymbol{J}$能将非线性编码器近似为线性算子。这允许我们更形式地使用“收缩”这个词。在线性理论中，当$\boldsymbol{Jx}$的范数对于所有单位$\boldsymbol{x}$都小于等于1 时，J 被称为收缩的。换句话说，如果J 收缩了单位球，他就是收缩的。我们可以认为CAE 为鼓励每个局部线性算子具有收缩性，而在每个训练数据点处将Frobenius 范数作为$f(\boldsymbol{x})$的局部线性近似的惩罚。

正则自编码器基于两种相反的推动力学习流形。在CAE 的情况下，这两种推动力是重构误差和收缩惩罚$\Omega(\boldsymbol{h})$。单独的重构误差鼓励CAE 学习一个恒等函数。单独的收缩惩罚将鼓励CAE 学习关于$\boldsymbol{x}$是恒定的特征。这两种推动力的折衷产生导数$\frac{\partial f(\boldsymbol{x})}{\partial \boldsymbol{x}}$大多是微小的自编码器。只有少数隐藏单元，对应于一小部分输入数据的方向，可能有显著的导数。

CAE 的目标是学习数据的流形结构。使$\boldsymbol{Jx}$很大的方向$\boldsymbol{x}$，会快速改变$\boldsymbol{h}$，因此很可能是近似流形切平面的方向。Rifai et al. (2011a,b) 的实验显示训练CAE 会导致$\boldsymbol{J}$中大部分奇异值（幅值）比1 小，因此是收缩的。然而，有些奇异值仍然比1 大，因为重构误差的惩罚鼓励CAE 对最大局部变化的方向进行编码。对应于最大奇异值的方向被解释为收缩自编码器学到的切方向。理想情况下，这些切方向应对应于数据的真实变化。比如，一个应用于图像的CAE 应该能学到显示图像改变的切向量，如图14.6 图中物体渐渐改变状态。如下图所示，实验获得的奇异向量的可视化似乎真的对应于输入图象有意义的变换。

{% asset_img 9.png %}

收缩自编码器正则化准则的一个实际问题是，尽管它在单一隐藏层的自编码器情况下是容易计算的，但在更深的自编码器情况下会变的难以计算。根据Rifaiet al. (2011a) 的策略，分别训练一系列单层的自编码器，并且每个被训练为重构前一个自编码器的隐藏层。这些自编码器的组合就组成了一个深度自编码器。因为每个层分别训练成局部收缩，深度自编码器自然也是收缩的。这个结果与联合训练深度模型完整架构（带有关于Jacobian的惩罚项）获得的结果是不同的，但它抓住了许多理想的定性特征。

另一个实际问题是，如果我们不对解码器强加一些约束，收缩惩罚可能导致无用的结果。例如，编码器将输入乘一个小常数$\epsilon$，解码器将编码除以一个小常数$\epsilon$。随着$\epsilon$趋向于0，编码器会使收缩惩罚项$\Omega(\boldsymbol{h})$趋向于0 而学不到任何关于分布的信息。同时，解码器保持完美的重构。Rifai et al. (2011a) 通过绑定$f$ 和$g$ 的权重来防止这种情况。$f$ 和$g$ 都是由线性仿射变换后进行逐元素非线性变换的标准神经网络层组成，因此将$g$ 的权重矩阵设成$f$ 权重矩阵的转置是很直观的。

## 8. 预测稀疏分解

预测稀疏分解（predictive sparse decomposition, PSD）是稀疏编码和参数化自编码器(Kavukcuoglu et al., 2008) 的混合模型。参数化编码器被训练为能预测迭代推断的输出。PSD 被应用于图片和视频中对象识别的无监督特征学习(Kavukcuogluet al., 2009, 2010; Jarrett et al., 2009b; Farabet et al., 2011)，在音频中也有所应用(Henaff et al., 2011)。这个模型由一个编码器$f(\boldsymbol{x})$和一个解码器$g(\boldsymbol{h})$组成，并且都是参数化的。在训练过程中，$\boldsymbol{h}$由优化算法控制。优化过程是最小化

$$
||\boldsymbol{x} - g(\boldsymbol{h})||^2 + \lambda|\boldsymbol{h}|_1 + \gamma||\boldsymbol{h} - f(\boldsymbol{x})||^2
$$

就像稀疏编码，训练算法交替地相对$\boldsymbol{h}$和模型的参数最小化上述目标。相对$\boldsymbol{h}$最小化较快，因为$f(\boldsymbol{x})$提供$\boldsymbol{h}$的良好初始值以及损失函数将$\boldsymbol{h}$约束在$f(\boldsymbol{x})$附近。简单的梯度下降算法只需10 步左右就能获得理想的$\boldsymbol{h}$。

PSD 所使用的训练程序不是先训练稀疏编码模型，然后训练$f(\boldsymbol{x})$来预测稀疏编码的特征。PSD 训练过程正则化解码器，使用$f(\boldsymbol{x})$可以推断出良好编码的参数。

预测稀疏分解是学习**近似推断**（learned approximate inference）的一个例子。PSD 能够被解释为通过最大化模型的对数似然下界训练有向稀疏编码的概率模型。

在PSD 的实际应用中，迭代优化仅在训练过程中使用。模型被部署后，参数编码器$f$用于计算已经习得的特征。相比通过梯度下降推断$h$，计算$f$是很容易的。因为$f$是一个可微带参函数，PSD 模型可堆叠，并用于初始化其他训练准则的深度网络。

## 9. 自编码器的应用

自编码器已成功应用于降维和信息检索任务。降维是表示学习和深度学习的第一批应用之一。它是研究自编码器早期驱动力之一。例如，Hinton and Salakhutdinov(2006) 训练了一个栈式RBM，然后利用它们的权重初始化一个隐藏层逐渐减小的深度自编码器，终结于30 个单元的瓶颈。生成的编码比30 维的PCA 产生更少的重构误差，所学到的表示更容易定性解释，并能联系基础类别，这些类别表现为分离良好的集群。

低维表示可以提高许多任务的性能，例如分类。小空间的模型消耗更少的内存和运行时间。

相比普通任务，**信息检索**（information retrieval）从降维中获益更多，此任务需要找到数据库中类似查询的条目。此任务不仅和其他任务一样从降维中获得一般益处，还使某些低维空间中的搜索变得极为高效。特别的，如果我们训练降维算法生成一个低维且二值的编码，那么我们就可以将所有数据库条目在哈希表映射为二值编码向量。这个哈希表允许我们返回具有相同二值编码的数据库条目作为查询结果进行信息检索。我们也可以非常高效地搜索稍有不同条目，只需反转查询编码的各个位。这种通过降维和二值化的信息检索方法被称为**语义哈希**（semantic hashing）(Salakhutdinov and Hinton, 2007b, 2009b)，已经被用于文本输入(Salakhutdinov andHinton, 2007b, 2009b) 和图像(Torralba et al., 2008; Weiss et al., 2008; Krizhevsky and Hinton, 2011)。

通常在最终层上使用sigmoid 编码函数产生语义哈希的二值编码。sigmoid 单元必须被训练为到达饱和，对所有输入值都接近0 或接近1。能做到这一点的窍门就是训练时在sigmoid 非线性单元前简单地注入加性噪声。噪声的大小应该随时间增加。要对抗这种噪音并且保存尽可能多的信息，网络必须加大输入到sigmoid 函数的幅度，直到饱和。

学习哈希函数的思想已在其他多个方向进一步探讨，包括改变损失训练表示的想法，其中所需优化的损失与哈希表中查找附近样本的任务有更直接的联系。







