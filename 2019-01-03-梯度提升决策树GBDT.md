---
title: 梯度提升决策树GBDT
date: 2019-01-03 21:30:49
categories:
- Machine Learning
tags:
- Theory
- GBDT
- BDT
mathjax: true
---

参考：

> [Introduction to Gradient Boosted Decision Trees](http://videolectures.net/site/normal_dl/tag=81961/nipsworkshops2010_ye_gbd_01.pdf)
> [GBDT：梯度提升决策树](https://www.jianshu.com/p/005a4e6ac775)

## 1. Regression Decision Tree 回归树

参考`决策树`

## 2. Boosting Decision Tree 提升树

BDT由多棵回归树来集成，属于Boosting算法一种，因此它的算法思想为：首先训练一个初始的回归决策树，决策树的深度、叶结点数、最小分类数等等是预先设定好的，也就意味着单个决策树的性能不一定是非常好的（对比单一决策树），然后我们根据这棵决策树预测各个样本的预测值，然后与样本的真实值计算残差（真实值-预测值），然后再将所有样本的残差作为下一个决策树预测的目标，继续生成决策树。

{% asset_img 2.png %}

<!-- more -->

## 3. Gradient Boosting Decision Tree 梯度提升决策树

GBDT又叫MART（Multiple Additive Regression Tree），GBDT通常用来做回归预测，调整后也可以用于分类。

首先是损失函数对应的梯度计算结果

{% asset_img 1.png %}

但是上图中绝对值损失和Huber损失的优化方式比较复杂，因此GBDT采取的措施是：利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树。

{% asset_img 0.png %}

1. 首先初始化一个决策树，仅有一个根节点，估计使损失函数极小化的常数值$\gamma$；
2. 计算损失函数的负梯度在当前模型的值，将它作为残差的估计；
3. 估计回归树叶节点区域，以拟合残差的近似值；
4. 利用线性搜索估计叶节点区域的值，使损失函数极小化；
5. 更新回归树；
6. 得到输出的最终模型

**推荐GBDT树的深度：6**

