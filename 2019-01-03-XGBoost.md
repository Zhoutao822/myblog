---
title: XGBoost
date: 2019-01-03 21:32:09
categories:
- Machine Learning
tags:
- Theory
- XGBoost
mathjax: true
---

参考：

> [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754v3.pdf)
> [XGBoost基本原理](https://juejin.im/post/5a13c9a8f265da43333e0648)

## 1. 正则化学习目标

假定数据集有$n$个样本，每个样本有$m$个特征，$\mathcal{D} = \{ (\boldsymbol{x}_i, y_i) \} (|\mathcal{D}| = n, \boldsymbol{x}_i \in \mathbb{R}^m, y_i \in \mathbb{R})$，并且该数据集属于回归任务，特征值和预测目标都是任意常数。假定用$K$个树构建模型，并且预测结果为所有树的预测结果的和

$$
\hat{y}_i = \phi(\boldsymbol{x}_i) = \sum^K_{k=1}f_k(\boldsymbol{x}_i), \quad f_k \in \mathcal{F}
$$

其中$\mathcal{F} = \{ f(\boldsymbol{x}) = w_{q(\boldsymbol{x})} \}(q:\mathbb{R}^m \rightarrow T, w \in \mathbb{R}^T)$


{% asset_img 0.png %}


{% asset_img 1.png %}
