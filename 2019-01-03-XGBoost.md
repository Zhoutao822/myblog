---
title: XGBoost
date: 2019-01-03 21:32:09
categories:
- Machine Learning
tags:
- Theory
- XGBoost
mathjax: true
---

参考：

> [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754v3.pdf)
> [XGBoost基本原理](https://juejin.im/post/5a13c9a8f265da43333e0648)

## 1. 正则化学习目标

假定数据集有$n$个样本，每个样本有$m$个特征，$\mathcal{D} = \{ (\boldsymbol{x}_i, y_i) \} (|\mathcal{D}| = n, \boldsymbol{x}_i \in \mathbb{R}^m, y_i \in \mathbb{R})$，并且该数据集属于回归任务，特征值和预测目标都是任意常数。假定用$K$个树构建模型，并且预测结果为所有树的预测结果的和

$$
\hat{y}_i = \phi(\boldsymbol{x}_i) = \sum^K_{k=1}f_k(\boldsymbol{x}_i), \quad f_k \in \mathcal{F}
$$

其中$\mathcal{F} = \{ f(\boldsymbol{x}) = w_{q(\boldsymbol{x})} \}(q:\mathbb{R}^m \rightarrow T, w \in \mathbb{R}^T)$，$T$是当前树的叶结点数（我们将叶结点编号），$q$是从样本$\boldsymbol{x}$到叶结点index的映射，即样本$\boldsymbol{x}$落在树的index叶结点上，$w_{q(\boldsymbol{x})}$表示该样本属于的叶结点的权值，我们可以将$f$看作是一棵树的表示函数，其包括两个部分：树的结构函数$q$和树的叶结点权重$w$。

{% asset_img 0.png %}

XGBoost是由多个决策树相加形成的，考虑增加正则项$\Omega$，则整个模型的优化目标为

$$
\mathcal{L}(\phi) = \sum_i l(\hat{y}_i, y_i) + \sum_k \Omega(f_k)
\\
\Omega(f) = \gamma T + \frac{1}{2}\lambda ||w||^2
$$

前半部分是预测值与真实值的误差，后半部分是对树的结构的惩罚，$\Omega$前半部分是对结节点数的惩罚（避免过拟合严重），后半部分是对叶结点权重的惩罚（$L_2$范数），也就是说XGBoost在回归树的基础上增加了对结点的权重惩罚（回归树结点取样本均值）。

<!-- more -->

## 2. 决策树梯度上升

我们知道神经网络的反向传播是应用于所有参数的，也就是说神经网络的参数更新是对所有的参数；XGBoost的梯度上升或下降是通过加法实现的，也就是说通过构建下一棵树来实现对当前树的梯度下降，我们在第$t$轮（这里$t$与上面的$k$几乎等价）的优化目标为

$$
\mathcal{L}^{(t)} = \sum^n_{i=1}l(y_i, \hat{y}_i^{(i-1)} + f_t(\boldsymbol{x}_i)) + \Omega(f_t)
$$

我们仅考虑加入$f_t$以优化模型，这种做法是贪心的。通过二次近似（$f(x+\bigtriangleup x) = f(x) + f'(x)\bigtriangleup x + \frac{1}{2}f''(x) (\bigtriangleup x)^2$），可以将上式近似表示为

$$
\mathcal{L}^{(t)} \simeq \sum^n_{i=1}[l(y_i, \hat{y}_i^{(t-1)})+ g_if_t(\boldsymbol{x}_i) + \frac{1}{2}h_if_t^2(\boldsymbol{x}_i)] + \Omega(f_t)
$$

其中$g_i = \partial_{\hat{y}_i^{(t-1)}}l(y_i, \hat{y}_i^{(t-1)})$，$h_i = \partial^2_{\hat{y}_i^{(t-1)}}l(y_i, \hat{y}_i^{(t-1)})$，本质上就是损失的一阶梯度和二阶梯度，由于我们的目标是最小化$t$轮的误差，所以可以去掉与$f_t$无关的量

$$
\tilde{\mathcal{L}}^{(t)} = \sum^n_{i=1}[g_if_t(\boldsymbol{x}_i) + \frac{1}{2}h_if_t^2(\boldsymbol{x}_i)] + \Omega(f_t)
$$

定义$I_j = \{ i|q(\boldsymbol{x}_i) = j \}$为划分到index为$j$的叶结点的样本的index集合（简单来说就是统计落入$j$叶结点的所有样本），那么重写上式为

$$
\tilde{\mathcal{L}}^{(t)} = \sum^n_{i=1}[g_if_t(\boldsymbol{x}_i) + \frac{1}{2}h_if_t^2(\boldsymbol{x}_i)] + \gamma T + \frac{1}{2}\lambda \sum^T_{j=1}w_j^2
\\
= \sum^T_{j=1}[(\sum_{i \in I_j}g_i)w_j + \frac{1}{2}(\sum_{i \in I_j}h_i + \lambda)w_j^2] + \gamma T
$$

{% asset_img 1.png %}

上式的两种变形考虑的对象不同，第一个式子考虑每个样本，第二个式子考虑的是当前树的每个叶结点（由于每个样本必定落入某个叶结点，因此考虑叶结点简化了计算过程），通过对$w_j$求偏导数使损失最小

$$
w^*_j = - \frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j}h_i + \lambda}
$$

将最优$w_j^*$代入到损失函数中，我们得到与树结构$q$相关的损失函数

$$
\tilde{\mathcal{L}}^{(t)}(q) = -\frac{1}{2}\sum^T_{j=1}\frac{(\sum_{i \in I_j}g_i)^2}{\sum_{i \in I_j}h_i + \lambda} + \gamma T
$$

因此上式可以作为衡量当前决策树性能的分数函数，通常用$Obj$表示。

如何确定最优树结构，理论上遍历所有树结构可以得到最小损失，但是很明显实际不可行。通常从一个根结点出发，逐渐增加左右两个分支，令$I_L$和$I_R$分别表示落入到左子树和右子树的样本集，计算损失下降$\mathcal{L}_{split}$

$$
\mathcal{L}_{split} = \frac{1}{2}[\frac{(\sum_{i \in I_L}g_i)^2}{\sum_{i \in I_L}h_i + \lambda} + \frac{(\sum_{i \in I_R}g_i)^2}{\sum_{i \in I_R}h_i + \lambda} - \frac{(\sum_{i \in I}g_i)^2}{\sum_{i \in I}h_i + \lambda}] - \lambda
$$

这个式子很好理解，就是`未分支损失-分枝后损失`（$\lambda$系数为1是由于仅增加了一个叶结点），所以这个值越大说明分支效果越好。

## 3. 收缩与特征采样

原文是Shrinkage和Column Subsampling，是两种避免过拟合的策略

* Shrinkage通过参数$\eta$限制新加入的树的权重$w$，目的是为了避免新加入的树性能过强导致后面加入的树没有优化空间，从而避免过拟合；
* Column Subsampling在随机森林中也使用过，先从当前结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择最优属性用于划分。

## 4. 分支划分算法

















