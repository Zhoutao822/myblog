---
title: XGBoost
date: 2019-01-03 21:32:09
categories:
- Machine Learning
tags:
- Theory
- XGBoost
mathjax: true
---

参考：

> [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754v3.pdf)
> [XGBoost基本原理](https://juejin.im/post/5a13c9a8f265da43333e0648)

## 1. 正则化学习目标

假定数据集有$n$个样本，每个样本有$m$个特征，$\mathcal{D} = \{ (\boldsymbol{x}_i, y_i) \} (|\mathcal{D}| = n, \boldsymbol{x}_i \in \mathbb{R}^m, y_i \in \mathbb{R})$，并且该数据集属于回归任务，特征值和预测目标都是任意常数。假定用$K$个树构建模型，并且预测结果为所有树的预测结果的和

$$
\hat{y}_i = \phi(\boldsymbol{x}_i) = \sum^K_{k=1}f_k(\boldsymbol{x}_i), \quad f_k \in \mathcal{F}
$$

其中$\mathcal{F} = \{ f(\boldsymbol{x}) = w_{q(\boldsymbol{x})} \}(q:\mathbb{R}^m \rightarrow T, w \in \mathbb{R}^T)$，$T$是当前树的叶节点数（我们将叶节点编号），$q$是从样本$\boldsymbol{x}$到叶节点index的映射，即样本$\boldsymbol{x}$落在树的index叶节点上，$w_{q(\boldsymbol{x})}$表示该样本属于的叶节点的权值。

{% asset_img 0.png %}

XGBoost是由多个决策树相加形成的，考虑增加正则项$\Omega$，则整个模型的优化目标为

$$
\mathcal{L}(\phi) = \sum_i l(\hat{y}_i, y_i) + \sum_k \Omega(f_k)
\\
\Omega(f) = \gamma T + \frac{1}{2}\lambda ||w||^2
$$

前半部分是预测值与真实值的误差，后半部分是对树的结构的惩罚，$\Omega$前半部分是对叶节点数的惩罚（避免过拟合严重），后半部分是对叶节点权重的惩罚（$L_2$范数），也就是说XGBoost在回归树的基础上增加了对节点的权重惩罚（回归树节点取样本均值）。

<!-- more -->

## 2. 
{% asset_img 1.png %}
