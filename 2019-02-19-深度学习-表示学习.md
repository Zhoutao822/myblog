---
title: 深度学习-表示学习
date: 2019-02-19 20:28:19
categories:
- Deep Learning
tags:
- Theory
mathjax: true
---

参考：

> [《深度学习》第15章 表示学习](https://github.com/exacity/deeplearningbook-chinese)

表示学习类似于找到更好的数据表示形式，比如在算术运算过程中使用阿拉伯数字而不是使用罗马数字，虽然两者的含义相同，但是在手写运算的过程中，阿拉伯数字更具优势，我们可以把计算当作后续任务，把以哪种形式表示当作前馈任务，通过表示学习的方法，学习到最优的表示。因此，表示学习可以用于非监督学习或者半监督学习任务中，用于学习到数据潜在的表示形式，便于后续任务的进行。

<!-- more -->

## 1. 贪心逐层无监督预训练

贪心逐层无监督预训练依赖于单层表示学习算法，例如RBM、单层自编码器、稀疏编码模型或其他学习潜在表示的模型。每一层使用无监督学习预训练，将前一层的输出作为输入，输出数据的新的表示。这个新的表示的分布（或者是和其他变量比如要预测类别的关系）有可能是更简单的。

{% asset_img 1.png %}

贪心逐层无监督预训练的几个标志性特点：
* 贪心：基于贪心算法，每次仅优化当前的一层网络，对后续的网络情况不需要了解；
* 逐层：预训练过程通常包括一个多层的神经网络，保持前面的网络层的参数不变，每次训练一层，并将训练完的输出作为下一层的输入；
* 无监督：每一层都是使用无监督学习算法。

因此它被称为预训练，我们可以在很多论文中发现作者会使用这种逐层的预训练的方式进行网络的初始化。在监督学习任务中，它可以被看作是正则化项（在一些实验中，预训练不能降低训练误差，但能降低测试误差）和参数初始化的一种形式。

我们也可以进行贪心逐层监督预训练。这是建立在训练浅层模型比深度模型更容易的前提下，而该前提似乎在一些情况下已被证实。

### 1.1 何时以及为何无监督预训练有效？

在很多分类任务中，贪心逐层无监督预训练能够在测试误差上获得重大提升。

然而，在很多其他问题上，无监督预训练不能带来改善，甚至还会带来明显的负面影响。

无监督预训练结合了两种不同的想法：

1. 利用深度神经网络对初始参数的选择，可以对模型有着显著的正则化效果（在较小程度上，可以改进优化）的想法；
2. 学习输入分布有助于学习从输入到输出的映射。

第一个想法，我们知道神经网络的训练过程本质上是一个寻找最优解的过程，在空间中等价于找到那个最低点，而从不同的起点出发，随着梯度下降的作用，我们可能得到的是一个局部极小值而不是全局最小值，通过无监督预训练，我们可以获取此阶段提取的数据的某些信息，若这些信息对我们的训练目标有用，那么训练起点会处于一个相对较优的位置。但是也有可能陷入一个非常不好的处境，比如代价函数无法进一步下降或下降很少，因此现代方法通常**同时**使用无监督学习和监督学习，而不是依序使用两个学习阶段。

为了保持无监督学习阶段提取的信息，可以固定特征提取器的参数，仅仅将监督学习作为顶层学成特征的分类器。

第二个想法，即学习算法可以使用无监督阶段学习的信息，在监督学习的阶段表现得更好。其基本想法是对于无监督任务有用的一些特征对于监督学习任务也可能是有用的。例如，如果我们训练汽车和摩托车图像的生成模型，它需要知道轮子的概念，以及一张图中应该有多少个轮子。如果我们幸运的话，无监督阶段学习的轮子表示会适合于监督学习。同时这也印证了为什么**同时**使用无监督学习和监督学习-输出层施加的约束很自然地从一开始就包括在内。

无监督学习表示的一个重要例子就是词嵌入（word embeding）。使用one-hot向量表示的词并不具有很多信息，这是因为两个one-hot向量之间的距离都是相同的。

学成的词嵌入自然会用它们彼此之间的距离来编码词之间的相似性。因此，无监督预训练在处理单词时特别有用。然而在处理图像时是不太有用的，可能是因为图像已经在一个很丰富的向量空间中，其中的距离只能提供低质量的相似性度量。

何时预训练效果最好——预训练的网络越深，测试误差的均值和方差下降得越多。值得注意的是，这些实验是在训练非常深层网络的现代方法发明和流行（整流线性单元，Dropout 和批标准化）之前进行的，因此对于无监督预训练与当前方法的结合，我们所知甚少。

与无监督学习的其他形式相比，无监督预训练的缺点是其使用了两个单独的训练阶段。很多正则化技术都具有一个优点，允许用户通过调整单一超参数的值来控制正则化的强度。无监督预训练没有一种明确的方法来调整无监督阶段正则化的强度。相反，无监督预训练有许多超参数，但其效果只能之后度量，通常难以提前预测。当我们同时执行无监督和监督学习而不使用预训练策略时，会有单个超参数（通常是附加到无监督代价的系数）控制无监督目标正则化监督模型的强度。减少该系数，总是能够可预测地获得较少正则化强度。在无监督预训练的情况下，没有一种灵活调整正则化强度的方式——要么监督模型初始化为预训练的参数，要么不是。

## 2. 迁移学习和领域自适应

迁移学习和领域自适应指的是利用一个情景（例如，分布P1）中已经学到的内容去改善另一个情景（比如分布P2）中的泛化情况。这点概括了上一节提出的想法，即在无监督学习任务和监督学习任务之间转移表示。

**迁移学习**，举个简单的例子，TensorFlow.keras中包括很多训练好的模型，比如VGG16、ResNet等等，这些模型都是在ImageNet数据集上训练得到的，如果我们把这些模型拿来对猫狗进行分类任务，这就是迁移学习。甚至是ImageNet中并不包括的种类，我们也可以利用训练好的模型进行迁移学习，这是因为许多视觉类别共享一些低级概念，比如边缘、视觉形状、几何变化、光照变化的影响等等。

一般而言，当存在对不同情景或任务有用特征时，并且这些特征对应多个情景出现的潜在因素，迁移学习、多任务学习和领域自适应可以使用表示学习来实现。

然而，有时不同任务之间共享的不是输入的语义，而是输出的语义。例如，语音识别系统需要在输出层产生有效的句子，但是输入附近的较低层可能需要识别相同音素或子音素发音的非常不同的版本（这取决于说话人）。在这样的情况下，共享神经网络的上层（输出附近）和进行任务特定的预处理是有意义的。

**领域自适应**，即目标任务是相同的，但是输入分布不同。例如，考虑情感分析的任务，如判断一条评论是表达积极的还是消极的情绪。网上的评论有许多类别。在书、视频和音乐等媒体内容上训练的顾客评论情感预测器，被用于分析诸如电视机或智能电话的消费电子产品的评论时，领域自适应情景可能会出现。

对于监督训练来说，由于语句的词汇和风格不同，且并没有出现在训练集中，那么很有可能在领域自适应的情况下得不到正确的结果，但是对于无监督学习来说，它能够将正面、中性、负面评价的潜在特征学习到，从而能够用于领域自适应。

**概念漂移**，指的是数据分布随时间为逐渐变化。一般可以将其视为多任务学习的特定形式，在所有这些情况下，我们的目标是利用第一个情景下的数据，提取那些在第二种情景中学习时或直接进行预测时可能有用的信息。表示学习的核心思想是相同的表示可能在两种情景中都是有用的。两个情景使用相同的表示，使得表示可以受益于两个任务的训练数据。

迁移学习有两种极端的形式：

1. **一次学习**，只有一个标注样本的迁移任务被称为一次学习，看起来很不可思议，但是如果考虑表示空间已经学习的非常好了，那么在表示空间中，聚集在标注样本周围的点应该具有相同的标签；
2. **零次学习**，没有标注样本的迁移任务被称为零次学习，举个例子，学习器已经读取了大量文本，然后要解决对象识别的问题。如果文本足够好地描述了对象，那么即使没有看到某对象的图像，也能识别出该对象的类别。例如，已知猫有四条腿和尖尖的耳朵，那么学习器可以在没有见过猫的情况下猜测该图像中是猫。

当然上述学习并不是没有约束的，只有在训练时使用了额外的信息，零数据学习和零次学习才是有可能的。在我们的例子中，没有提前看到猫的图像而去识别猫，所以拥有一些未标注文本数据包含句子诸如“猫有四条腿”或“猫有尖耳朵”，对于学习非常有帮助。

零次学习是迁移学习的一种特殊形式。同样的原理可以解释如何能执行多模态学习，学习两种模态的表示，和一种模态中的观察结果$\boldsymbol{x}$与另一种模态中的观察结果$\boldsymbol{y}$组成的对$(\boldsymbol{x, y})$之间的关系（通常是一个联合分布）。通过学习所有的三组参数（从$\boldsymbol{x}$到它的表示、从$\boldsymbol{y}$到它的表示，以及两个表示之间的关系），一个表示中的概念被锚定在另一个表示中，反之亦然，从而可以有效地推广到新的对组。这个过程可以用下图展示

{% asset_img 2.png %}

## 3. 半监督解释因果关系

*什么原因能使一个表示比另一个表示更好？*

一种假设是，理想表示中的特征对应到观测数据的潜在成因，特征空间中不同的特征或方向对应着不同的原因，从而表示能够区分这些原因。

可以参考受限玻尔兹曼机RBM和混合高斯模型GMM，对于输入$\boldsymbol{x}$来说，若它属于某种混合分布，而对应的目标$\boldsymbol{y}$属于其中的混合分量，那么通过无监督学习，我们可以找到一个非常好的表示空间用于描述输入$\boldsymbol{x}$。

{% asset_img 3.png %}

关于上述成立的条件是，大多数观察是由极其大量的潜在成因形成的。半监督学习的一个重要研究前沿是确定每种情况下要编码什么。目前，处理大量潜在原因的两个主要策略是，同时使用无监督学习和监督学习信号，从而使得模型捕获最相关的变动因素，或是使用纯无监督学习学习更大规模的表示。

无监督学习的另一个思路是选择一个更好的确定哪些潜在因素最为关键的定义。这个问题就很突出了，潜在因素有很多，甚至说我们并不能预先知道潜在因素有哪些或者有多少，但是我们需要确定哪些潜在因素最为关键，而放弃掉那些不重要的因素。

例如，图像像素的均方误差隐式地指定，一个潜在因素只有在其显著地改变大量像素的亮度时，才是重要影响因素。如果我们希望解决的问题涉及到小对象之间的相互作用，那么这将有可能遇到问题。在机器人任务中，自编码器未能学习到编码小乒乓球。同样是这个机器人，它可以成功地与更大的对象进行交互（例如棒球，均方误差在这种情况下很显著）。

{% asset_img 4.png %}

这就与生成式对抗网络有很大的联系了，具体参考生成式对抗网络。生成式对抗网络只是确定应该表示哪些因素的一小步。我们期望未来的研究能够发现更好的方式来确定表示哪些因素，并且根据任务来开发表示不同因素的机制。

{% asset_img 5.png %}

## 4. 分布式表示






## 5. 得益于深度的指数增益



## 6. 提供发现潜在原因的线索






