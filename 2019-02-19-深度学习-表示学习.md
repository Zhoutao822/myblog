---
title: 深度学习-表示学习
date: 2019-02-19 20:28:19
categories:
- Deep Learning
tags:
- Theory
mathjax: true
---

参考：

> [《深度学习》第15章 表示学习](https://github.com/exacity/deeplearningbook-chinese)

表示学习类似于找到更好的数据表示形式，比如在算术运算过程中使用阿拉伯数字而不是使用罗马数字，虽然两者的含义相同，但是在手写运算的过程中，阿拉伯数字更具优势，我们可以把计算当作后续任务，把以哪种形式表示当作前馈任务，通过表示学习的方法，学习到最优的表示。因此，表示学习可以用于非监督学习或者半监督学习任务中，用于学习到数据潜在的表示形式，便于后续任务的进行。

<!-- more -->

## 1. 贪心逐层无监督预训练

贪心逐层无监督预训练依赖于单层表示学习算法，例如RBM、单层自编码器、稀疏编码模型或其他学习潜在表示的模型。每一层使用无监督学习预训练，将前一层的输出作为输入，输出数据的新的表示。这个新的表示的分布（或者是和其他变量比如要预测类别的关系）有可能是更简单的。

{% asset_img 1.png %}

贪心逐层无监督预训练的几个标志性特点：
* 贪心：基于贪心算法，每次仅优化当前的一层网络，对后续的网络情况不需要了解；
* 逐层：预训练过程通常包括一个多层的神经网络，保持前面的网络层的参数不变，每次训练一层，并将训练完的输出作为下一层的输入；
* 无监督：每一层都是使用无监督学习算法。

因此它被称为预训练，我们可以在很多论文中发现作者会使用这种逐层的预训练的方式进行网络的初始化。在监督学习任务中，它可以被看作是正则化项（在一些实验中，预训练不能降低训练误差，但能降低测试误差）和参数初始化的一种形式。

我们也可以进行贪心逐层监督预训练。这是建立在训练浅层模型比深度模型更容易的前提下，而该前提似乎在一些情况下已被证实。

### 1.1 何时以及为何无监督预训练有效？

在很多分类任务中，贪心逐层无监督预训练能够在测试误差上获得重大提升。

然而，在很多其他问题上，无监督预训练不能带来改善，甚至还会带来明显的负面影响。

无监督预训练结合了两种不同的想法：

1. 利用深度神经网络对初始参数的选择，可以对模型有着显著的正则化效果（在较小程度上，可以改进优化）的想法；
2. 学习输入分布有助于学习从输入到输出的映射。

第一个想法，我们知道神经网络的训练过程本质上是一个寻找最优解的过程，在空间中等价于找到那个最低点，而从不同的起点出发，随着梯度下降的作用，我们可能得到的是一个局部极小值而不是全局最小值，通过无监督预训练，我们可以获取此阶段提取的数据的某些信息，若这些信息对我们的训练目标有用，那么训练起点会处于一个相对较优的位置。但是也有可能陷入一个非常不好的处境，比如代价函数无法进一步下降或下降很少，因此现代方法通常**同时**使用无监督学习和监督学习，而不是依序使用两个学习阶段。

为了保持无监督学习阶段提取的信息，可以固定特征提取器的参数，仅仅将监督学习作为顶层学成特征的分类器。

第二个想法，即学习算法可以使用无监督阶段学习的信息，在监督学习的阶段表现得更好。其基本想法是对于无监督任务有用的一些特征对于监督学习任务也可能是有用的。例如，如果我们训练汽车和摩托车图像的生成模型，它需要知道轮子的概念，以及一张图中应该有多少个轮子。如果我们幸运的话，无监督阶段学习的轮子表示会适合于监督学习。同时这也印证了为什么**同时**使用无监督学习和监督学习-输出层施加的约束很自然地从一开始就包括在内。

无监督学习表示的一个重要例子就是词嵌入（word embeding）。使用one-hot向量表示的词并不具有很多信息，这是因为两个one-hot向量之间的距离都是相同的。

学成的词嵌入自然会用它们彼此之间的距离来编码词之间的相似性。因此，无监督预训练在处理单词时特别有用。然而在处理图像时是不太有用的，可能是因为图像已经在一个很丰富的向量空间中，其中的距离只能提供低质量的相似性度量。

何时预训练效果最好——预训练的网络越深，测试误差的均值和方差下降得越多。值得注意的是，这些实验是在训练非常深层网络的现代方法发明和流行（整流线性单元，Dropout 和批标准化）之前进行的，因此对于无监督预训练与当前方法的结合，我们所知甚少。

与无监督学习的其他形式相比，无监督预训练的缺点是其使用了两个单独的训练阶段。很多正则化技术都具有一个优点，允许用户通过调整单一超参数的值来控制正则化的强度。无监督预训练没有一种明确的方法来调整无监督阶段正则化的强度。相反，无监督预训练有许多超参数，但其效果只能之后度量，通常难以提前预测。当我们同时执行无监督和监督学习而不使用预训练策略时，会有单个超参数（通常是附加到无监督代价的系数）控制无监督目标正则化监督模型的强度。减少该系数，总是能够可预测地获得较少正则化强度。在无监督预训练的情况下，没有一种灵活调整正则化强度的方式——要么监督模型初始化为预训练的参数，要么不是。

## 2. 迁移学习和领域自适应

迁移学习和领域自适应指的是利用一个情景（例如，分布P1）中已经学到的内容去改善另一个情景（比如分布P2）中的泛化情况。这点概括了上一节提出的想法，即在无监督学习任务和监督学习任务之间转移表示。

**迁移学习**，举个简单的例子，TensorFlow.keras中包括很多训练好的模型，比如VGG16、ResNet等等，这些模型都是在ImageNet数据集上训练得到的，如果我们把这些模型拿来对猫狗进行分类任务，这就是迁移学习。甚至是ImageNet中并不包括的种类，我们也可以利用训练好的模型进行迁移学习，这是因为许多视觉类别共享一些低级概念，比如边缘、视觉形状、几何变化、光照变化的影响等等。

一般而言，当存在对不同情景或任务有用特征时，并且这些特征对应多个情景出现的潜在因素，迁移学习、多任务学习和领域自适应可以使用表示学习来实现。

然而，有时不同任务之间共享的不是输入的语义，而是输出的语义。例如，语音识别系统需要在输出层产生有效的句子，但是输入附近的较低层可能需要识别相同音素或子音素发音的非常不同的版本（这取决于说话人）。在这样的情况下，共享神经网络的上层（输出附近）和进行任务特定的预处理是有意义的。

**领域自适应**，即目标任务是相同的，但是输入分布不同。例如，考虑情感分析的任务，如判断一条评论是表达积极的还是消极的情绪。网上的评论有许多类别。在书、视频和音乐等媒体内容上训练的顾客评论情感预测器，被用于分析诸如电视机或智能电话的消费电子产品的评论时，领域自适应情景可能会出现。

对于监督训练来说，由于语句的词汇和风格不同，且并没有出现在训练集中，那么很有可能在领域自适应的情况下得不到正确的结果，但是对于无监督学习来说，它能够将正面、中性、负面评价的潜在特征学习到，从而能够用于领域自适应。

**概念漂移**，指的是数据分布随时间为逐渐变化。一般可以将其视为多任务学习的特定形式，在所有这些情况下，我们的目标是利用第一个情景下的数据，提取那些在第二种情景中学习时或直接进行预测时可能有用的信息。表示学习的核心思想是相同的表示可能在两种情景中都是有用的。两个情景使用相同的表示，使得表示可以受益于两个任务的训练数据。

迁移学习有两种极端的形式：

1. **一次学习**，只有一个标注样本的迁移任务被称为一次学习，看起来很不可思议，但是如果考虑表示空间已经学习的非常好了，那么在表示空间中，聚集在标注样本周围的点应该具有相同的标签；
2. **零次学习**，没有标注样本的迁移任务被称为零次学习，举个例子，学习器已经读取了大量文本，然后要解决对象识别的问题。如果文本足够好地描述了对象，那么即使没有看到某对象的图像，也能识别出该对象的类别。例如，已知猫有四条腿和尖尖的耳朵，那么学习器可以在没有见过猫的情况下猜测该图像中是猫。

当然上述学习并不是没有约束的，只有在训练时使用了额外的信息，零数据学习和零次学习才是有可能的。在我们的例子中，没有提前看到猫的图像而去识别猫，所以拥有一些未标注文本数据包含句子诸如“猫有四条腿”或“猫有尖耳朵”，对于学习非常有帮助。

零次学习是迁移学习的一种特殊形式。同样的原理可以解释如何能执行多模态学习，学习两种模态的表示，和一种模态中的观察结果$\boldsymbol{x}$与另一种模态中的观察结果$\boldsymbol{y}$组成的对$(\boldsymbol{x, y})$之间的关系（通常是一个联合分布）。通过学习所有的三组参数（从$\boldsymbol{x}$到它的表示、从$\boldsymbol{y}$到它的表示，以及两个表示之间的关系），一个表示中的概念被锚定在另一个表示中，反之亦然，从而可以有效地推广到新的对组。这个过程可以用下图展示

{% asset_img 2.png %}

## 3. 半监督解释因果关系

*什么原因能使一个表示比另一个表示更好？*

一种假设是，理想表示中的特征对应到观测数据的潜在成因，特征空间中不同的特征或方向对应着不同的原因，从而表示能够区分这些原因。

可以参考受限玻尔兹曼机RBM和混合高斯模型GMM，对于输入$\boldsymbol{x}$来说，若它属于某种混合分布，而对应的目标$\boldsymbol{y}$属于其中的混合分量，那么通过无监督学习，我们可以找到一个非常好的表示空间用于描述输入$\boldsymbol{x}$。

{% asset_img 3.png %}

关于上述成立的条件是，大多数观察是由极其大量的潜在成因形成的。半监督学习的一个重要研究前沿是确定每种情况下要编码什么。目前，处理大量潜在原因的两个主要策略是，同时使用无监督学习和监督学习信号，从而使得模型捕获最相关的变动因素，或是使用纯无监督学习学习更大规模的表示。

无监督学习的另一个思路是选择一个更好的确定哪些潜在因素最为关键的定义。这个问题就很突出了，潜在因素有很多，甚至说我们并不能预先知道潜在因素有哪些或者有多少，但是我们需要确定哪些潜在因素最为关键，而放弃掉那些不重要的因素。

例如，图像像素的均方误差隐式地指定，一个潜在因素只有在其显著地改变大量像素的亮度时，才是重要影响因素。如果我们希望解决的问题涉及到小对象之间的相互作用，那么这将有可能遇到问题。在机器人任务中，自编码器未能学习到编码小乒乓球。同样是这个机器人，它可以成功地与更大的对象进行交互（例如棒球，均方误差在这种情况下很显著）。

{% asset_img 4.png %}

这就与生成式对抗网络有很大的联系了，具体参考生成式对抗网络。生成式对抗网络只是确定应该表示哪些因素的一小步。我们期望未来的研究能够发现更好的方式来确定表示哪些因素，并且根据任务来开发表示不同因素的机制。

{% asset_img 5.png %}

## 4. 分布式表示

**分布式表示**，它的含义是由很多元素组合的表示，这些元素之间可以设置成可分离的，简而言之就是对于我们需要考察的任务，我们都以具有$k$个值的$n$个特征去描述，总共我们就可以描述$k^n$个不同的概念。

分布式表示是表示学习最重要的工具之一。就神经网络而言，隐藏神经元即利用了分布式表示的策略；就概率模型而言，潜变量也是利用了分布式表示的策略。我们之所以使用了这些隐藏神经元或潜变量，是因为如果表示空间足够大，我们必定能够以某种方式找到任务在表示空间中的方向。

以$n$维二元向量为例，有$2^n$种配置，每一种都对应输入空间中的一个不同区域。这与符号表示类似但是不同，比如说字典中有$n$个符号，以one-hot向量表示，那么每一个字典里的符号都是一个唯一的$n$维二元向量。但是仅需要$n$个不同的区域而不是$2^n$，因此符号表示是非分布式表示类中的一个具体示例，它可以包含很多条目，但是每个条目没有显著意义的单独控制作用。

与此类似的基于非分布式表示的学习算法有：

* 聚类算法，包含k-means算法：每个输入点恰好分配一个类别。
* k-最近邻算法：给定一个输入，一个或几个模板或原型样本与之关联。
* 决策树：给定输入时，只有一个叶节点是被激活的。
* 高斯混合体和专家混合体：模板（聚类中心）或专家关联一个激活的程度。
* 具有高斯核（或其他类似的局部核）的核机器：尽管每个“支持向量”或模板样本的激活程度是连续值，但仍然会出现和高斯混合体相同的问题。
* 基于n-gram的语言或翻译模型：根据后缀的树结构划分上下文集合（符号序列）。

基于分布式表示的示例：

{% asset_img 7.png %}

基于非分布式表示的示例：

{% asset_img 8.png %}

将分布式表示和符号表示区分开来的一个重要概念是，**由不同概念之间的共享属性而产生的泛化**。比如对于“猫”和“狗”，如果作为符号表示，那么“猫”、“狗”之间的距离与其他任何符号的距离都是相同的，可以理解为one-hot向量，但是若作为分布式表示，我们将以“具有皮毛”、“腿的数目”等特征作为表示空间，那么“猫”、“狗”之间的距离与其他对象的距离就是不同的，比如“房屋”，因为在某些特征上“猫”、“狗”更加接近，从而导致以分布式表示的效果比符号表示更好。这里就不得不说**词嵌入**，在语言模型上，词嵌入比one-hot表示更加优秀。分布式表示具有丰富的相似性空间，语义上相近的概念（或输入）在距离上接近，这是纯粹的符号表示所缺少的特点。

在学习算法中使用分布式表示何时以及为什么具有统计优势？当一个明显复杂的结构可以用较少参数紧致地表示时，分布式表示具有统计上的优点。

对于上图分布式表示的示例，我们可以通过对输入的线性函数进行阈值处理来提取二元特征。那么可以将空间划分的数量为：

$$
\sum^d_{j=0} \binom{n}{j} = O(n^d)
$$

显然，关于输入大小呈指数级增长，关于隐藏单元的数量呈多项式级增长。

这提供了分布式表示泛化能力的一种几何解释：$O(nd)$个参数能够明确表示输入空间中$O(n^d)$个不同区域。这也就意味着使用更少的参数来表示模型，只需要更少的训练样本就可以获得比非分布式表示更好的泛化。

另一种解释基于分布式表示的模型泛化能力更好的说法是，尽管能够，明确地编码这么多不同的区域，但它们的容量仍然是有限的。

在实践中，隐藏单元并不能总是学习出具有简单语言学名称的事物，但有趣的是，这些事物会在那些最好的计算机视觉深度网络的顶层附近出现。这些特征的共同之处在于，我们可以设想学习其中的每个特征不需要知道所有其他特征的所有配置。

{% asset_img 6.png %}

## 5. 得益于深度的指数增益

多层感知机是万能近似器相比于浅层网络，一些函数能够用指数级小的深度网络表示。缩小模型规模能够提高统计效率。

在许多不同情景中已经证明，非线性和重用特征层次结构的组合来组织计算，可以使分布式表示获得指数级加速之外，还可以获得统计效率的指数级提升。许多种类的只有一个隐藏层的网络（例如，具有饱和非线性，布尔门，和/积，或RBF 单元的网络）都可以被视为万能近似器。在给定足够多隐藏单元的情况下，这个模型族是一个万能近似器，可以在任意非零允错级别近似一大类函数（包括所有连续函数）。然而，隐藏单元所需的数量可能会非常大。

## 6. 提供发现潜在原因的线索

**什么原因能够使一个表示比另一个表示更好？**

一个理想的表示能够区分生成数据变化的潜在因果因子，特别是那些与我们的应用相关的因素。表示学习的大多数策略都会引入一些有助于学习潜在变差因素的线索。这些线索可以帮助学习器将这些观察到的因素与其他因素分开。

诸如没有免费午餐定理的这些结果表明，正则化策略对于获得良好泛化是很有必要的。当不可能找到一个普遍良好的正则化策略时，深度学习的一个目标是找到一套相当通用的正则化策略，使其能够适用于各种各样的AI 任务（类似于人和动物能够解决的任务）。

在此，我们提供了一些通用正则化策略的列表。该列表显然是不详尽的，但是给出了一些学习算法是如何发现对应潜在因素的特征的具体示例。

* 平滑：假设对于单位$\boldsymbol{d}$和小量$\epsilon$有$f(\boldsymbol{x} + \epsilon \boldsymbol{d}) \approx f(\boldsymbol{x})$。这个假设允许学习器从训练样本泛化到输入空间中附近的点。许多机器学习算法都利用了这个想法，但它不能克服维数灾难难题。
* 线性：很多学习算法假定一些变量之间的关系是线性的。这使得算法能够预测远离观测数据的点，但有时可能会导致一些极端的预测。大多数简单的学习算法不会做平滑假设，而会做线性假设。这些假设实际上是不同的，具有很大权重的线性函数在高维空间中可能不是非常平滑的。
* 多个解释因子：许多表示学习算法受以下假设的启发，数据是由多个潜在解释因子生成的，并且给定每一个因子的状态，大多数任务都能轻易解决。
* 因果因子：该模型认为学成表示所描述的变差因素是观察数据x 的成因，而并非反过来。
* 深度，或者解释因子的层次组织：高级抽象概念能够通过将简单概念层次化来定义。从另一个角度来看，深度架构表达了我们认为任务应该由多个程序步骤完成的观念，其中每一个步骤回溯到先前步骤处理之后的输出。
* 任务间共享因素：当多个对应到不同变量$y_i$的任务共享相同的输入$\boldsymbol{x}$时，或者当每个任务关联到全局输入$\boldsymbol{x}$的子集或者函数$f^{(i)}(\boldsymbol{x})$时，我们会假设每个变量$y_i$关联到来自相关因素$\boldsymbol{h}$公共池的不同子集。
* 流形：概率质量集中，并且集中区域是局部连通的，且占据很小的体积。在连续情况下，这些区域可以用比数据所在原始空间低很多维的低维流形来近似。
* 自然聚类：很多机器学习算法假设输入空间中每个连通流形可以被分配一个单独的类。数据分布在许多个不连通的流形上，但相同流形上数据的类别是相同的。这个假设激励了各种学习算法，包括正切传播、双反向传播、流形正切分类器和对抗训练。
* 时间和空间相干性：慢特征分析和相关的算法假设，最重要的解释因子随时间变化很缓慢，或者至少假设预测真实的潜在解释因子比预测诸如像素值这类原始观察会更容易些。
* 稀疏性：假设大部分特征和大部分输入不相关，如在表示猫的图像时，没有必要使用象鼻的特征。因此，我们可以强加一个先验，任何可以解释为“存在”或“不存在”的特征在大多数时间都是不存在的。
* 简化因子依赖：在良好的高级表示中，因子会通过简单的依赖相互关联。比如边缘独立。

