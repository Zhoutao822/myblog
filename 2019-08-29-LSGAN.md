---
title: LSGAN
date: 2019-08-29 20:51:48
categories:
- Deep Learning
tags:
- Theory
- LSGAN
mathjax: true
---

参考：

> [Least Squares Generative Adversarial Networks](https://arxiv.org/abs/1611.04076)

## 1. LSGAN作者说

LSGAN(Least Squares Generative Adversarial Networks)，提出的目的也是为了加强训练过程的稳定性，同时经过对比实验发现LSGAN能生成质量更好的
图片，它的做法是修改损失函数，通过对判别器使用最小均方损失来实现。

对于原始GAN来说，其目标函数为

$$
\min_G \max_D V_{GAN}(D, G) = \mathbb{E}_{\boldsymbol{x} \sim p_{data}(\boldsymbol{x})}[\log D(\boldsymbol{x})] + \mathbb{E}_{\boldsymbol{z} \sim p_{z}(\boldsymbol{z})}[\log (1 - D(G(\boldsymbol{z})))]
$$

原始GAN的判别器使用了Sigmoid激活函数输出结果，我们知道Sigmoid有一个很显著的特点就是对于输出结果非常容易将其划分到1或者0，这一点可以从Sigmoid函数图像中看出，这在CNN分类任务中是很有效的，因为分类任务的目的是将目标的种类进行划分，而目标的种类不是0就是1（二分类），但是对于GAN来说使用Sigmoid会产生问题，GAN的目的是生成对抗样本，如果判别器将生成样本判别为1，则生成器梯度下降为0，那么此样本将不会对模型产生任何训练影响，但是此样本是否属于接近真实样本我们不得而知，而仅仅依靠判别器进行判断，所以说Sigmoid作为最后一层的激活函数应该不适合用于GAN的判别器。

{% asset_img sigmoid.png %}

因此LSGAN的作者提出了使用最小二乘法作为损失函数

$$
\min_D V_{LSGAN}(D) = \frac{1}{2} \mathbb{E}_{\boldsymbol{x} \sim p_{data}(\boldsymbol{x})}[(D(\boldsymbol{x}) - b)^2] + \frac{1}{2} \mathbb{E}_{\boldsymbol{z} \sim p_{z}(\boldsymbol{z})}[(D(G(\boldsymbol{z}))-a)^2]
\\
\min_G V_{LSGAN}(G) = \frac{1}{2}\mathbb{E}_{\boldsymbol{z} \sim p_{z}(\boldsymbol{z})}[(D(G(\boldsymbol{z}))-c)^2]
$$

其中$a, b$分别是生成样本和真实样本的标签，$c$表示G希望D相信这是生成样本的程度，如果$c = b$，则表示我们希望G能够生成更加接近真实样本的数据，如果$a < c < b$，则表示我们希望G能够生成多样性更强的数据。

当G固定时，D的最优解为

$$
D^*(\boldsymbol{x}) = \frac{bp_{data}(\boldsymbol{x}) + ap_g(\boldsymbol{x})}{p_{data}(\boldsymbol{x}) + p_g(\boldsymbol{x})}
$$


