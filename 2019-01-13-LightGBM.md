---
title: LightGBM
date: 2019-01-13 21:39:40
categories:
- Machine Learning
tags:
- Theory
- LightGBM
mathjax: true
---

参考：

> [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)
> [LightGBM整理](https://dataxujing.github.io/LightGBM-learn/#1)
> [从结构到性能，一文概述XGBoost、Light GBM和CatBoost的同与不同](https://juejin.im/post/5ab1d713f265da238f126b87)
> [LightGBM核心解析与调参](https://juejin.im/post/5b76437ae51d45666b5d9b05)

LightGBM属于Boosting一族，由微软开源，被评价为“速度惊人”，“非常有启发”，“支持分布式”，“内存占用小”等。

<!-- more -->

## 1. 回顾

### 1.1 Boosting

参考`决策树`

{% asset_img 0.png %}

### 1.2 Gradient Boosting

参考`GBDT`

## 2. Leaf-wise

{% asset_img 1.png %}

### 2.1 Level-wise算法

传统决策树算法采用的就是Level-wise算法，每次划分（连续值）都在减少当前结点的损失，在不考虑预剪枝的情况下，这种做法缺点是贪心，优点是可以利用多线程。

### 2.2 Leaf-wise算法

Leaf-wise算法主要用于LightGBM，它的核心思想是：在将结点一分为二后，考虑在损失最大的那个子结点上继续进行划分（这里比较的是全部数据集的损失，也就是说如果某次迭代中子树的损失小于其某个祖先的兄弟时，可以从祖先的兄弟继续划分子树）。这样可以很容易实现整体的最小损失，但是容易过拟合，我们可以通过控制树的深度避免过拟合。

上图中的$p,f,v$应该代表划分的样本子集，最佳特征，最佳特征对应的最佳阈值。

## 3. 对比XGBoost

参考`XGBoost`，首先了解一下XGBoost的优缺点

* 精确贪心算法
    * 优点：
        * 可以找到精确的划分条件
    * 缺点：
        * 计算量巨大
        * 内存占用巨大
        * 易产生过拟合
* Level-wise迭代方式
    * 优点：
        * 可以使用多线程
        * 可以加速精确贪心算法
    * 缺点： 
        * 效率低下，可能产生不必要的叶结点

LightGBM官方宣称的优点，很明显是针对XGBoost：

* 快速高效
* 内存占用低
* 准确率高
* 支持分布式
* 支持large-scale data
* 支持直接使用category特征

{% asset_img 2.png %}

表格中提到的Histogram algorithm稍后介绍，XGBoost与LightGBM的对比，看表格应该是LightGBM全面碾压XGBoost

{% asset_img 3.png %}

## 4. Histogram algorithm

{% asset_img 4.png %}

Histogram algorithm应该翻译为直方图算法，直方图算法的思想也很简单，首先将连续的浮点数据转换为bin数据，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。（看起来很高大上，其实就是直方图统计，最后我们将大规模的数据放在了直方图中）

直方图算法有几个需要注意的地方：

* 使用bin替代原始数据相当于增加了正则化；
* 使用bin意味着很多数据的细节特征被放弃了，相似的数据可能被划分到相同的桶中，这样的数据之间的差异就消失了；
* bin数量选择决定了正则化的程度，bin越少惩罚越严重，欠拟合风险越高。

直方图算法：

{% asset_img 5.png %}

直方图算法需要注意的地方：

* 构建直方图时不需要对数据进行排序（比XGBoost快），因为预先设定了bin的范围；
* 直方图除了保存划分阈值和当前bin内样本数以外还保存了当前bin内所有样本的一阶梯度和（一阶梯度和的平方的均值等价于均方损失）；
* 阈值的选取是按照直方图从小到大遍历，使用了上面的一阶梯度和，目的是得到划分之后$\bigtriangleup loss$最大的特征及阈值。




{% asset_img 3.png %}
