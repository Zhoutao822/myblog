---
title: 神经网络-coding
date: 2018-11-18 18:35:34
categories:
- Coding
tags:
- Code
- Neural Network
- DNNClassifier
- DNNRegressor
- DNNLinearCombinedRegressor
- Estimator
- Keras
mathjax: true
---

## 1. 数据集说明

### 1.1 二分类数据集-乳腺癌

* 数据来源：`sklearn.datasets.load_breast_cancer`；
* 数据集形状：总计569个样本，良性357个，恶性212个，每个样本由30个属性表示，target表示肿瘤良性1还是恶性0，所有属性值均为number，详情可调用`load_breast_cancer()['DESCR']`了解每个属性的具体含义；
* 数据集划分：随机选出20%数据作为测试集，不做验证集要求；
* 性能度量：accuracy或ROC。

### 1.2 回归数据集-California房价

* 数据来源：`sklearn.datasets.fetch_california_housing`；
* 数据集形状：总计20640个样本，每个样本8个属性表示，以及房价作为target，所有属性值均为number，详情可调用`fetch_california_housing()['DESCR']`了解每个属性的具体含义；
* 数据集划分：随机选出20%数据作为测试集，不做验证集要求；
* 性能度量：MSE或者RMSE均可以。

### 1.3 多分类数据集-森林植被类型

* 数据来源：`sklearn.datasets.fetch_covtype`；
* 数据集形状：总计581012个样本，每个样本由54个维度表示（12个属性，其中2个分别是onehot4维和onehot40维），以及target表示植被类型1-7，所有属性值均为number，详情可调用`fetch_covtype()['DESCR']`了解每个属性的具体含义；
* 数据集划分：随机选出20%数据作为测试集，不做验证集要求；
* 性能度量：accuracy或ROC。

<!-- more -->

## 2. 神经网络

神经网络一般都是由输入层、隐藏层和输出层组成。输入层即控制数据输入，仅有一层；隐藏层一般包括多层，每层指定一个神经元数，激活函数一般为ReLU；输出层决定你的神经网络的输出，即最终预测的结果的类型，比如回归型神经网络输出层是对上一层的输出进行求和，二分类型神经网络输出层是是对上一层的输出做Sigmoid运算，多分类神经网络输出层是对上一层进行Softmax运算，根据输出层的激活函数不同，具体的反向传播公式会有细微的差别（在对损失函数求导方面）。

### 2.1 梯度下降算法实现二分类

假设一个浅层神经网络的流程如下

$$
(x, W^1, b^1) \rightarrow z^1 = W^1x + b^1 \rightarrow a^1 = \sigma(z^1)
\\
(a^1, W^2, b^2) \rightarrow	z^2 = W^2a^1 + b^2 \rightarrow a^2 = \sigma(z^2) \rightarrow L(a^2, y)
$$

$L(a^2, y)$为损失函数交叉熵，对二分类来说，其计算方式为

$$
L(a, y) = -y\log a - (1-y)\log (1-a)
$$

就二分类来说，隐藏层激活函数为ReLU，输出层Sigmoid，其正向传播和反向传播简化公式（将输入$x$组成矩阵$\boldsymbol{X}, \boldsymbol{X} \in \mathbb{R}^{n_0 \times m}$）为

**正向传播**

$$
\boldsymbol{Z}^1 = \boldsymbol{W}^1\boldsymbol{X} + \boldsymbol{b}^1
\\
\boldsymbol{A}^1 = \sigma(\boldsymbol{Z}^1)
\\
\boldsymbol{Z}^2 = \boldsymbol{W}^2\boldsymbol{A}^1 + \boldsymbol{b}^2
\\
\boldsymbol{A}^2 = \sigma(\boldsymbol{Z}^2)
$$

**反向传播**
$$
d\boldsymbol{Z}^2 = \boldsymbol{A}^2 - \boldsymbol{Y}
\\
d\boldsymbol{W}^2 = \frac{1}{m}d\boldsymbol{Z}^2\boldsymbol{A}^{1T}
\\
d\boldsymbol{b}^2 = \frac{1}{m} np.sum(d\boldsymbol{Z}^2, axis=1, keepdims=True)
\\
d\boldsymbol{Z}^1 = \boldsymbol{W}^{2T}d\boldsymbol{Z}^2 \cdot g'^1(\boldsymbol{Z}^1)
\\
d\boldsymbol{W}^1 = \frac{1}{m} d\boldsymbol{Z}^1\boldsymbol{X}^T
\\
d\boldsymbol{b}^1 = \frac{1}{m} np.sum(d\boldsymbol{Z}^1, axis=1, keepdims=True)
$$

首先导入需要的库

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import  load_breast_cancer
from sklearn.model_selection import train_test_split
```

然后我们需要根据数据形状设计参数形状，输入X的形状为`n0 * m`，n0为特征数，m为样本数，则第一层W1为`n0 * n1`，b1为`n1 * 1`，计算形式为`WX + b`

```python
def init_variables(layers):
    """
        desc:
            根据layers初始化神经网络参数
        parameters: 
            layers: 神经网络的结构，每一层的神经元个数，list
        return:
            variables: 初始化的神经网络参数，w和b，dict
    """
    L = len(layers)
    variables = {}
    for i in range(1, L):
        variables["W" + str(i)] = np.random.randn(layers[i], layers[i - 1]) * 0.01
        variables["b" + str(i)] = np.zeros((layers[i], 1))
    return variables

def relu(Z):
    """
        激活函数ReLU
    """
    return np.maximum(0, Z)

def sigmoid(Z):
    """
        激活函数Sigmoid
    """
    return 1 / (1 + np.exp(-Z))
```

再定义前向传播函数和损失函数，这里计算的是交叉熵

```python
def fp(X, variables):
    """
        desc: 
            前向传播，计算预测值
        parameters:
            X: 输入数据集，形式n0*m，m为样本数，n0为特征数
            variables: 神经网络参数，w和b
        return:
            AL: 预测结果
            caches: 计算过程中缓存的每一层神经元的输入输出以及参数
    """
    A = X
    L = len(variables) // 2
    caches = [(None, None, None, X)]
    for l in range(1, L):
        A_pre = A
        W = variables['W' + str(l)]
        b = variables['b' + str(l)]
        z = np.dot(W, A_pre) + b
        A = relu(z)
        caches.append((W, b, z, A))
    WL = variables['W' + str(L)]
    bL = variables['b' + str(L)]
    zL = np.dot(WL, A) + bL
    AL = sigmoid(zL)
    caches.append((WL, bL, zL, AL))
    return AL, caches

def compute_cost(AL, Y):
    cost = np.mean(np.multiply(-np.log(AL), Y) + np.multiply(-np.log(1 - AL), 1 - Y))
    cost = np.squeeze(cost)
    return cost
```

定义反向传播函数，其中有反函数

```python
def relu_back(A):
    return np.int64(A > 0)

def bp(AL, Y, caches):
    """
        desc:
            反向传播，计算导数
        parameters:
            AL: 前向传播得到的结果
            Y: 真实值
            caches: 前向传播过程中缓存的数据
        return:
            gradients: 反向传播的导数
    """
    m = Y.shape[1]
    L = len(caches) - 1
    prev_AL = caches[L - 1][3]
    dzL = 1. / m * (AL - Y)
    dWL = np.dot(dzL, prev_AL.T)
    dbL = np.sum(dzL, axis = 1, keepdims = True)
    gradients = {'dW' + str(L) : dWL, 'db' + str(L) : dbL}
    for i in reversed(range(1, L)):
        post_W = caches[i + 1][0]
        dz = dzL
        dal = np.dot(post_W.T, dz)
        #Al = caches[i][3]
        #dzl = np.multiply(dal, relu_back(Al))
        #使用Al和zl效果相同

        zl = caches[i][2]
        dzl = np.multiply(dal, relu_back(zl))

        prev_A = caches[i -1][3]
        dwl = np.dot(dzl, prev_A.T)
        dbl = np.sum(dzl, axis = 1, keepdims = True)

        gradients['dW' + str(i)] = dwl
        gradients['db' + str(i)] = dbl
        dzL = dzl
    return gradients
```
定义更新参数的函数，以及其他辅助的函数，最后指明main函数

```python
def update_param(variables, gradients, learning_rate):
    L = len(variables) // 2
    for i in range(L):
        variables['W' + str(i + 1)] -= learning_rate * gradients['dW' + str(i + 1)]
        variables['b' + str(i + 1)] -= learning_rate * gradients['db' + str(i + 1)]
    return variables

def L_layer_model(X, Y, layers, learning_rate, maxCycles):
    costs = []
    variables = init_variables(layers)
    for i in range(maxCycles):
        AL, caches = fp(X, variables)
        cost = compute_cost(AL, Y)
        if i % 1000 == 0:
            print('Cost after iteration {} : {}'.format(i, cost))
            costs.append(cost)
        gradients = bp(AL, Y, caches)
        variables = update_param(variables, gradients, learning_rate)
    plt.clf()
    plt.plot(costs)
    plt.xlabel('iterations')
    plt.ylabel('cost')
    plt.show()
    return variables

def predict(X_test,y_test,variables):
    m = y_test.shape[1]
    Y_prediction = np.zeros((1, m))
    prob, caches = fp(X_test, variables)
    for i in range(prob.shape[1]):
        # 将概率转换为标签
        if prob[0, i] > 0.5:
            Y_prediction[0, i] = 1
        else:
            Y_prediction[0, i] = 0
    accuracy = 1- np.mean(np.abs(Y_prediction - y_test))
    return accuracy

def DNN(X_train, y_train, X_test, y_test, layers, learning_rate= 0.01, num_iterations=40000):
    variables = L_layer_model(X_train, y_train, layers, learning_rate, num_iterations)
    accuracy = predict(X_test,y_test,variables)
    return accuracy

if __name__ == "__main__":
    X_data, y_data = load_breast_cancer(return_X_y=True)
    X_train, X_test,y_train,y_test = train_test_split(X_data, y_data, train_size=0.8)
    X_train = X_train.T
    y_train = y_train.reshape(y_train.shape[0], -1).T
    X_test = X_test.T
    y_test = y_test.reshape(y_test.shape[0], -1).T
    accuracy = DNN(X_train,y_train,X_test,y_test,[X_train.shape[0],20, 20, 10, 5, 1])
    print('accuracy reaches %.4f' % accuracy)
```

> accuracy reaches 0.9035

若我们使用同样的数据集划分，对比线性回归中准确率，我们应该会发现，神经网络的准确率相当高。

训练过程中损失变化情况如下
{% asset_img nn.png 损失变化 %}

### 2.2 Estimator实现房价预测

导入库，不一定会用到，但是是可以使用的

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import seaborn as sns # 数据图形化，后面会用到
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd # pandas输出处理
import tensorflow as tf
```

获取数据，并进行处理，划分测试集

```python
(data, target) = fetch_california_housing(return_X_y=True)

x_train, x_test, y_train, y_test = train_test_split(data, target, shuffle=True, test_size=0.2)
```

添加特征列，这里可以分析数据进行不一样的特征列处理；
定义输入函数，这里采用了`pandas_input_fn`，直接将`DataFrame`作为输入，就可以不需要定义字典。
此数据集中有经纬度数据，即地域对房价的影响，不同地域内的房价往往有着不同的均值和方差，因此可以修改特征列。

```python
feature_names = fetch_california_housing()['feature_names']

feature_columns = []
for name in feature_names:
    feature_columns.append(tf.feature_column.numeric_column(key=name))

def input_fn(x, y, training=True):
    dataframe = pd.DataFrame(data=x, columns=feature_names)
    dataframe['HousePrice'] = y
    if training: # 训练集可以重复数据，测试集就没必要
        return tf.estimator.inputs.pandas_input_fn(
            x=dataframe, 
            y=dataframe['HousePrice'],
            batch_size=64, 
            shuffle=True, 
            num_epochs=20) # 这里决定了后面模型的训练迭代轮数
    else:
        return tf.estimator.inputs.pandas_input_fn(
            x=dataframe,
            y=dataframe['HousePrice'],
            shuffle=False)
```

这里定义训练模型，我们定义了神经网络模型dnn和线性模型linear，进行比较，优化器都选择`ProximalAdagradOptimizer`

```python
dnn = tf.estimator.DNNRegressor(
    feature_columns=feature_columns, 
    hidden_units=[32, 32, 16, 8], 
    model_dir="C://Users//Admin//Desktop//model//DNNRegressor",
    optimizer=tf.train.ProximalAdagradOptimizer(
        learning_rate=0.01, 
        l1_regularization_strength=0.001))

linear = tf.estimator.LinearRegressor(
    feature_columns=feature_columns,
    model_dir="C://Users//Admin//Desktop//model//LinearRegressor",
    optimizer=tf.train.ProximalAdagradOptimizer(
        learning_rate=0.01,
        l1_regularization_strength=0.001))

# 这里循环多次是为了了解训练过程是否过拟合，对应前面输入函数num_epochs=20设置一个较小的值
for i in range(7):
    dnn.train(input_fn=input_fn(x_train, y_train), max_steps=40000)
    dnn.evaluate(input_fn=input_fn(x_test, y_test, training=False))
    linear.train(input_fn=input_fn(x_train, y_train), max_steps=40000)
    linear.evaluate(inpun=input_fn(x_test, y_test, training=False))
```

{% asset_img dnn-linear.png 对比%}

从图中可以知道，线性模型的收敛速度比神经网络快，但是最终神经网络模型的效果略好一点（并不意味着神经网路一定优于线性模型），由于神经网络的结构对结果影响很大，我得到的最优的平均损失在0.50附近。
若分析数据进行特征工程，我想应该能得到更好的结果。

---

**1. 首先将数据转换成Pandas形式，观察数据的一般统计量**

```python
df = pd.DataFrame(data=data, columns=feature_names)
df['HousePrice'] = target
df.describe()
```

{% asset_img house0.png 房价数据 %}

从图中可以发现几个问题：

* AveRooms的最大值141，而均值和75%的值都在7以内，说明有的街区是豪宅或者统计错误，需要过滤掉这些异常值，同理AveBedrms、Population和AveOccup都存在异常值；
* Latitude和Longitude是经纬度，理论上是一种地理位置属性，两者应该放在一起考虑；

**2. 然后我们计算各个特征之间的协方差**

```python
corr = df.corr()
sns.heatmap(corr)
```

{% asset_img corr.png 协方差矩阵 %}

从协方差热力图中可以发现
* MedInc与房价之间有正相关的线性关系，且是这几个特征中最强的，所以我们可能会考虑使用[`DNNLinearCombinedRegressor`](https://arxiv.org/pdf/1606.07792.pdf)并将MedInc作为线性模型特征列，其他作为神经网络特征列；
* HouseAge与其他所有的特征的相关性并不强,尤其是与房价的相关性，对比其他的特征，是最低的，所以最后可能放弃使用这个特征；
* Latitude和Longitude呈负相关，说明在地理上面，样本点的布局呈现负导数斜线的形状。

**3. 尝试构造额外的特征**

(1) 根据Population和AveOccup，可以得到属于该街区的房子总数，$TotalHouse = \frac{Population}{AveOccup}$；

(2) 根据MedInc和Population，可以得到该街区的收入总数（不准确，使用了中位数而不是均值），$TotalInc = MedInc \times Population$；

(3) 根据AveRooms和AveBedrms，可以得到平均其他房间数，$AveOtherrms = AveRooms - AveBedrms$。

```python
df['TotalHouse'] = df['Population'] / df['AveOccup']
df['TotalInc'] = df['MedInc'] * df['Population']
df['AveOtherrms'] = df['AveRooms'] - df['AveBedrms']
```

*构造的特征不一定有效，但是可以增加对数据的了解，在后面，我们要考虑使用哪些特征。*

**4. 分析房价与其他所有特征的二维坐标图**

```python
for name in df.columns.values.tolist():
    sns.relplot(x=name, y='HousePrice',data=df)
```

{% asset_img house1.png 异常值 %}

首先，我们可以发现图中确实有很多的异常点，与上面我们分析的是对应的，我们可以筛选出处于正常的样本，这几个阈值仅作参考，最终我们的数据集剩下20057个样本，少了600左右的数据

```python
df = df[(df['MedInc'] < 10) & (df['AveRooms'] < 15) & (df['AveBedrms'] < 4) & (df['Population'] < 8000) & (df['AveOccup'] < 6)]
```

{% asset_img house2.png 无用特征 %}

房价与房龄可以确定没有相关性，与上面分析对应，确定放弃此特征。

{% asset_img house3.png 线性相关 %}

AveOtherrms、MedInc和HousePrice可以看作线性相关，放在线性层，其他数据没有可以明确描述的相关性，但是数据呈现一定分布规律。

**5. 根据经纬度与房价的信息构造位置特征**

```python
sns.relplot(x='Latitude', y='Longitude',hue='HousePrice', data=df)
```

{% asset_img house4.png 经纬度与房价 %}

可以分析出某些特定位置的房价是很高的，所以对于落在此区域的房子，在其他条件相同的个情况下，其房价应当会高于其他位置的房子，我的处理方式是，根据经纬度把地区划分为一个一个正方形，增加新的特征$AreaPrice = mean(HousePrice \in area)$，用当前区域内的房价的均值作为地价特征。

```python
df['AreaPrice'] = -1
for i in range(11):
    for j in range(12):
        rows = (df['Latitude'] >= 32. + i) & \
        (df['Latitude'] < 33. + i) & \
        (df['Longitude'] >= -125. + j) & \
        (df['Longitude'] < -124. + j) 
        df.loc[rows, ['AreaPrice']] = df[rows]['HousePrice'].mean()
```

具体实现是先找到经纬度的最小值和最大值，划分为$1 \times 1$大小的矩形，统计当前区域内的房价均值。最后我们需要在看一下该特征是否有效，流程再走一遍。

**6. 放弃部分特征**

* 经纬度放弃，已经生成了新的特征，所以没有用了，除非经纬度还与其他特征相关；
* HouseAge放弃，相关性很弱；
* ...（暂时没有，可以再训练完成后分析进而再选择）

```python
df = df.drop(columns=['HouseAge', 'Latitude', 'Longitude'])
```

**7. 使用DNNLinearCombinedRegressor**

```python
# 线性模型特征列
linear_feature_names = ['MedInc', 'AveOtherrms', 'AreaPrice']
linear_feature_columns = []
for name in linear_feature_names:
    linear_feature_columns.append(tf.feature_column.numeric_column(key=name))
# 深度模型特征列
dnn_feature_names = df.columns.values.tolist()
dnn_feature_names.remove('HousePrice')
dnn_feature_columns = []
for name in dnn_feature_names:
    dnn_feature_columns.append(tf.feature_column.numeric_column(key=name))

# 划分数据集
trainset = df.sample(frac=0.8)
testset = df.drop(trainset.index.tolist(), axis=0)

# 定义训练模型
model = tf.estimator.DNNLinearCombinedRegressor(
    linear_feature_columns=linear_feature_columns,
    linear_optimizer=tf.train.FtrlOptimizer(learning_rate=0.01),
    dnn_feature_columns=dnn_feature_columns,
    dnn_optimizer=tf.train.ProximalAdagradOptimizer(
        learning_rate=0.01,
        l1_regularization_strength=0.0001
    ),
    dnn_hidden_units=[16, 16, 8, 4],
    model_dir="C://Users//Admin//Desktop//model//DNNLinearCombinedRegressor",
)

model.train(input_fn=tf.estimator.inputs.pandas_input_fn(
    x=trainset,
    y=trainset['HousePrice'],
    batch_size=32, 
    shuffle=True, 
    num_epochs=200
), max_steps=80000)

model.evaluate(input_fn=tf.estimator.inputs.pandas_input_fn(
    x=testset,
    y=testset['HousePrice'],
    shuffle=False
))
```
> 'average_loss': 0.42808318,
> 'label/mean': 2.0479238,
> 'loss': 53.65755,
> 'prediction/mean': 1.9944419,
> 'global_step': 80000

预测结果一般，应该需要再进一步的分析数据，调整特征列和神经网络结构。

### 2.3 Keras实现植被分类

首先给出使用Estimator预测的代码和结果作为参照，基本流程为：获取数据->定义特征列->定义输入函数->构造Estimator->执行train和evaluate。

```python
from sklearn.datasets import fetch_covtype
from sklearn.model_selection import train_test_split
import tensorflow as tf
import numpy as np

(data, target) = fetch_covtype(return_X_y=True)

x_train, x_test, y_train, y_test = train_test_split(data, target, shuffle=True, test_size=0.2)

feature_names = [
    'Elevation', 'Aspect', 'Slope', 
    'Horizontal_Distance_To_Hydrology', 
    'Vertical_Distance_To_Hydrology', 
    'Horizontal_Distance_To_Roadways', 
    'Hillshade_9am', 'Hillshade_Noon', 
    'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 
    'Wilderness_Area', 'Soil_Type']

feature_columns = []
for i in range(10):
    feature_columns.append(tf.feature_column.numeric_column(key=feature_names[i]))
feature_columns.append(tf.feature_column.numeric_column(key=feature_names[10], shape=(4,)))
feature_columns.append(tf.feature_column.numeric_column(key=feature_names[11], shape=(40,)))


def input_fn(x, y, training=True):
    y = y - 1
    inputs = {}
    for i in range(10):
        inputs[feature_names[i]] = np.array(x[:, i])
    inputs[feature_names[10]] = np.array(x[:, 10:14])
    inputs[feature_names[11]] = np.array(x[:, 14:])
    if training:
        return tf.estimator.inputs.numpy_input_fn(
            x=inputs,
            y=y,
            batch_size=64,
            shuffle=True,
            num_epochs=20
        )
    else:
        return tf.estimator.inputs.numpy_input_fn(
            x=inputs,
            y=y,
            shuffle=False
        )

model = tf.estimator.DNNClassifier(
    hidden_units=[64, 64, 32, 16],
    feature_columns=feature_columns,
    n_classes=7,
    model_dir="C://Users//Admin//Desktop//model//DNNClassifier",
    optimizer=tf.train.ProximalAdagradOptimizer(
        learning_rate=0.01,
        l1_regularization_strength=0.0001
    )
)

for i in range(5):
    model.train(input_fn=input_fn(x_train, y_train), max_steps=800000)
    print(model.evaluate(input_fn=input_fn(x_test, y_test, training=False)))
```
> 'accuracy': 0.7873463, 
> 'average_loss': 0.5104109, 
> 'loss': 65.32079,
> 'global_step': 800000

结果仅供参考，增加训练轮数应该还能提高，因为我还没有遇到过拟合就停止训练了，仅用CPU时间耗费太长。

使用keras实现

```python
import tensorflow as tf
import numpy as np
from sklearn.datasets import fetch_covtype
from sklearn import preprocessing
from tensorflow import keras
from tensorflow.python.keras import layers

# 获取数据，并对数据做归一化处理，同时这里需要将target转换成one_hot变量，因为后面使用的是softmax激活函数进行多分类
(data, target) = fetch_covtype(return_X_y=True)
scaler = preprocessing.StandardScaler().fit(X=data)
data = scaler.transform(data)
target_onehot = tf.one_hot(target - 1, depth=7)

# 注释掉的部分是通过Dataset控制输入，

# x_train, x_test, y_train, y_test = train_test_split(data, target, shuffle=True, test_size=0.2)

# y_train_onehot = tf.one_hot(y_train - 1, depth=7)
# y_test_onehot = tf.one_hot(y_test - 1, depth=7)

# trainset = tf.data.Dataset.from_tensor_slices((x_train, y_train_onehot))
# trainset = trainset.batch(BATCH_SIZE).repeat()

# testset = tf.data.Dataset.from_tensor_slices((x_test, y_test_onehot))
# testset = testset.batch(BATCH_SIZE).repeat()

# 定义batch大小，训练轮数，每轮的steps
BATCH_SIZE = 64
EPOCHS = 30
STEPS_PER_EPOCH = 6000

# 定义神经网络，第一层需要定义输入形状input_shape，最后一层使用softmax，我这里没有使用全部特征，只用了前10个
model = keras.Sequential([
    layers.Dense(units=64, activation='relu', input_shape=(54,)),
    layers.Dense(units=64, activation='relu'),
    layers.Dense(units=32, activation='relu'),
    layers.Dense(units=16, activation='relu'),
    layers.Dense(units=7, activation='softmax')
])

# 损失函数的形式对应多分类是categorical_crossentropy，二分类是binary_crossentropy
model.compile(loss='categorical_crossentropy',
    optimizer=tf.train.AdamOptimizer(0.01),
    metrics=['accuracy'])

# 定义一些callback，比如early_stop可以使训练提前终止，ModelCheckpoint保存checkpoint，
# TensorBoard保存tensorboard参数，PrintLoss打印训练过程输出信息
early_stop = tf.keras.callbacks.EarlyStopping(
    patience=5, 
    monitor='val_loss', 
    mode='auto'
)

ckpt_callback = tf.keras.callbacks.ModelCheckpoint(
    "./checkpoint/cp-{epoch:04d}.ckpt",
    verbose=1,
    save_weights_only=True,
    period=10
)

tb_callback = tf.keras.callbacks.TensorBoard(
    log_dir='./log',
    batch_size=BATCH_SIZE
)

class PrintLoss(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        print('Epoch: {:03d} - loss: {:.5f} - acc: {:.5f} - \
        val_loss: {:.5f} - val_acc: {:.5f}'.format(epoch + 1, logs['loss'], logs['acc'], logs['val_loss'], logs['val_acc']))

# 开始训练，指定x，对应的也是前10个特征，validation_split设置验证集比例
model.fit(
    x=data,
    y=target_onehot,
    epochs=EPOCHS,
    shuffle=True,
    validation_split=0.2,
    steps_per_epoch=STEPS_PER_EPOCH,
    validation_steps=STEPS_PER_EPOCH // 4,
    callbacks=[tb_callback, ckpt_callback]
)

#注释掉的部分对应使用Dataset作为输入的fit的方式，注意细节上有点区别

# history = model.fit(
#     trainset,
#     epochs=EPOCHS,
#     steps_per_epoch=STEPS_PER_EPOCH,
#     validation_data=testset,
#     validation_steps=STEPS_PER_EPOCH // 4,
#     verbose=0,
#     callbacks=[PrintLoss()]
# )
```
直接使用numpy数据训练部分结果（时间过长，未完成全部训练）：

> Epoch 1/30
> 4000/4000 [==============================] - 2761s 690ms/step - loss: 0.2626 - acc: 0.8936 - val_loss: 1.8917 - val_acc: 0.6318
> Epoch 2/30
> 4000/4000 [==============================] - 3236s 809ms/step - loss: 0.1862 - acc: 0.9270 - val_loss: 2.0801 - val_acc: 0.6481

使用Dataset作为输入训练的结果，可以明确地表示，使用Dataset作为输入可以提升整体训练的速度，上面的时间在1小时左右才训练了2轮，使用Dataset10分钟以内就完成了30轮的训练，而且验证集达到了85.65%的准确率：

> Epoch: 030 - loss: 0.38923 - acc: 0.85355 -         val_loss: 0.40994 - val_acc: 0.85650

* 使用keras的好处在于，它简化了构造神经网络的方式，并且提供了训练过程中显示训练状态的功能，比如显示实时loss和accuracy，显示剩余时间等等；
* 使用keras时，务必先检查数据集的数据特征，比如各个特征的数量级，如果相差很大必须做归一化处理，否则训练过程中不收敛或收敛到局部极小处（推荐无论怎样都进行归一化处理）；
* 不同数据特征的归一化处理应当不同，比如对onehot变量，可以使用MaxAbsScaler，从而避免破坏其稀疏性，对其他类型可以使用StandardScaler；
* 使用不同的优化器Optimizer会影响收敛速度，就我的实际使用而言，AdamOptimizer收敛速度比其他的优化器要更快一些，使用AdamOptimizer在第一轮的1000steps时accuracy就达到了80%+，而AdagradOptimizer在第一轮2000steps时accuracy只有70%+；
* 我没有分别归一化，但是实际上需要这么做。