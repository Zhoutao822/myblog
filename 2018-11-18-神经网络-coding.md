---
title: 神经网络-coding
date: 2018-11-18 18:35:34
categories:
- deep learning
- coding
tags:
- code
- neural network
- DNNClassifier
- DNNRegressor
- DNNLinearCombinedClassifier
- DNNLinearCombinedRegressor
- Estimator
- keras
mathjax: true
---

## 1. 数据集说明

### 1.1 二分类数据集-乳腺癌

* 数据来源：`sklearn.datasets.load_breast_cancer`；
* 数据集形状：总计569个样本，良性357个，恶性212个，每个样本由30个属性表示，target表示肿瘤良性1还是恶性0，所有属性值均为number，详情可调用`load_breast_cancer()['DESCR']`了解每个属性的具体含义；
* 数据集划分：随机选出20%数据作为测试集，不做验证集要求；
* 性能度量：accuracy或ROC。

### 1.2 回归数据集-California房价

* 数据来源：`sklearn.datasets.fetch_california_housing`；
* 数据集形状：总计20640个样本，每个样本8个属性表示，以及房价作为target，所有属性值均为number，详情可调用`fetch_california_housing()['DESCR']`了解每个属性的具体含义；
* 数据集划分：随机选出20%数据作为测试集，不做验证集要求；
* 性能度量：MSE或者RMSE均可以。

### 1.3 多分类数据集-森林植被类型

* 数据来源：`sklearn.datasets.fetch_covtype`；
* 数据集形状：总计581012个样本，每个样本由54个属性表示，target表示植被类型1-7，所有属性值均为number，详情可调用`fetch_covtype()['DESCR']`了解每个属性的具体含义；
* 数据集划分：随机选出20%数据作为测试集，不做验证集要求；
* 性能度量：accuracy或ROC。

<!-- more -->

## 2. 神经网络

### 2.1 梯度下降算法实现

首先导入需要的库

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import  load_breast_cancer
from sklearn.model_selection import train_test_split
```

然后我们需要根据数据形状设计参数形状，输入X的形状为n0 * m，n0为特征数，m为样本数，则第一层W1为n0 * n1，b1为n1 * 1，计算形式为WX + b

```python
def init_variables(layers):
	"""
        desc:
            根据layers初始化神经网络参数
        parameters: 
            layers: 神经网络的结构，每一层的神经元个数，list
        return:
            variables: 初始化的神经网络参数，w和b，dict
    """
	L = len(layers)
	variables = {}
	for i in range(1, L):
		variables["W" + str(i)] = np.random.randn(layers[i], layers[i - 1]) * 0.01
		variables["b" + str(i)] = np.zeros((layers[i], 1))
	return variables

def relu(Z):
    """
        激活函数ReLU
    """
	return np.maximum(0, Z)

def sigmoid(Z):
    """
        激活函数Sigmoid
    """
	return 1 / (1 + np.exp(-Z))
```

再定义前向传播函数和损失函数，这里计算的是交叉熵

```python
def fp(X, variables):
	"""
        desc: 
            前向传播，计算预测值
        parameters:
            X: 输入数据集，形式n0*m，m为样本数，n0为特征数
            variables: 神经网络参数，w和b
        return:
            AL: 预测结果
            caches: 计算过程中缓存的每一层神经元的输入输出以及参数
    """
	A = X
	L = len(variables) // 2
	caches = [(None, None, None, X)]
	for l in range(1, L):
		A_pre = A
		W = variables['W' + str(l)]
		b = variables['b' + str(l)]
		z = np.dot(W, A_pre) + b
		A = relu(z)
		caches.append((W, b, z, A))
	WL = variables['W' + str(L)]
	bL = variables['b' + str(L)]
	zL = np.dot(WL, A) + bL
	AL = sigmoid(zL)
	caches.append((WL, bL, zL, AL))
	return AL, caches

def compute_cost(AL, Y):
	cost = np.mean(np.multiply(-np.log(AL), Y) + np.multiply(-np.log(1 - AL), 1 - Y))
	cost = np.squeeze(cost)
	return cost
```

定义反向传播函数，其中有反函数

```python
def relu_back(A):
	return np.int64(A > 0)

def bp(AL, Y, caches):
	"""
        desc:
            反向传播，计算导数
        parameters:
            AL: 前向传播得到的结果
            Y: 真实值
            caches: 前向传播过程中缓存的数据
        return:
            gradients: 反向传播的导数
    """
	m = Y.shape[1]
	L = len(caches) - 1
	prev_AL = caches[L - 1][3]
	dzL = 1. / m * (AL - Y)
	dWL = np.dot(dzL, prev_AL.T)
	dbL = np.sum(dzL, axis = 1, keepdims = True)
	gradients = {'dW' + str(L) : dWL, 'db' + str(L) : dbL}
	for i in reversed(range(1, L)):
		post_W = caches[i + 1][0]
		dz = dzL
		dal = np.dot(post_W.T, dz)
		#Al = caches[i][3]
		#dzl = np.multiply(dal, relu_back(Al))
		#使用Al和zl效果相同

		zl = caches[i][2]
		dzl = np.multiply(dal, relu_back(zl))

		prev_A = caches[i -1][3]
		dwl = np.dot(dzl, prev_A.T)
		dbl = np.sum(dzl, axis = 1, keepdims = True)

		gradients['dW' + str(i)] = dwl
		gradients['db' + str(i)] = dbl
		dzL = dzl
	return gradients
```
定义更新参数的函数，以及其他辅助的函数，最后指明main函数

```python
def update_param(variables, gradients, learning_rate):
	L = len(variables) // 2
	for i in range(L):
		variables['W' + str(i + 1)] -= learning_rate * gradients['dW' + str(i + 1)]
		variables['b' + str(i + 1)] -= learning_rate * gradients['db' + str(i + 1)]
	return variables

def L_layer_model(X, Y, layers, learning_rate, maxCycles):
	costs = []
	variables = init_variables(layers)
	for i in range(maxCycles):
		AL, caches = fp(X, variables)
		cost = compute_cost(AL, Y)
		if i % 1000 == 0:
			print('Cost after iteration {} : {}'.format(i, cost))
			costs.append(cost)
		gradients = bp(AL, Y, caches)
		variables = update_param(variables, gradients, learning_rate)
	plt.clf()
	plt.plot(costs)
	plt.xlabel('iterations')
	plt.ylabel('cost')
	plt.show()
	return variables

def predict(X_test,y_test,variables):
	m = y_test.shape[1]
	Y_prediction = np.zeros((1, m))
	prob, caches = fp(X_test, variables)
	for i in range(prob.shape[1]):
		# 将概率转换为标签
		if prob[0, i] > 0.5:
			Y_prediction[0, i] = 1
		else:
			Y_prediction[0, i] = 0
	accuracy = 1- np.mean(np.abs(Y_prediction - y_test))
	return accuracy

def DNN(X_train, y_train, X_test, y_test, layers, learning_rate= 0.01, num_iterations=40000):
	variables = L_layer_model(X_train, y_train, layers, learning_rate, num_iterations)
	accuracy = predict(X_test,y_test,variables)
	return accuracy

if __name__ == "__main__":
	X_data, y_data = load_breast_cancer(return_X_y=True)
	X_train, X_test,y_train,y_test = train_test_split(X_data, y_data, train_size=0.8)
	X_train = X_train.T
	y_train = y_train.reshape(y_train.shape[0], -1).T
	X_test = X_test.T
	y_test = y_test.reshape(y_test.shape[0], -1).T
	accuracy = DNN(X_train,y_train,X_test,y_test,[X_train.shape[0],20, 20, 10, 5, 1])
	print('accuracy reaches %.4f' % accuracy)
```

> accuracy reaches 0.9035

若我们使用同样的数据集划分，对比线性回归中准确率，我们应该会发现，神经网络的准确率相当高。

{% asset_img nn.png 损失变化 %}

### 2.2 Estimator

导入库，不一定会用到，但是是可以使用的

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import seaborn as sns # 数据图形化
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd # pandas输出处理
import tensorflow as tf
```

获取数据，并进行处理，划分测试集

```python
(data, target) = fetch_california_housing(return_X_y=True)

x_train, x_test, y_train, y_test = train_test_split(data, target, shuffle=True, test_size=0.2)
```

添加特征列，这里可以分析数据进行不一样的特征列处理；
定义输入函数，这里采用了`pandas_input_fn`，直接将`DataFrame`作为输入，就可以不需要定义字典。
此数据集中有经纬度数据，即地域对房价的影响，不同地域内的房价往往有着不同的均值和方差，因此可以修改特征列。

```python
feature_names = fetch_california_housing()['feature_names']

feature_columns = []
for name in feature_names:
    feature_columns.append(tf.feature_column.numeric_column(key=name))

def input_fn(x, y, training=True):
    dataframe = pd.DataFrame(data=x, columns=feature_names)
    dataframe['HousePrice'] = y
    if training: # 训练集可以重复数据，测试集就没必要
        return tf.estimator.inputs.pandas_input_fn(
            x=dataframe, 
            y=dataframe['HousePrice'],
            batch_size=64, 
            shuffle=True, 
            num_epochs=20) # 这里决定了后面模型的训练迭代轮数
    else:
        return tf.estimator.inputs.pandas_input_fn(
            x=dataframe,
            y=dataframe['HousePrice'],
            shuffle=False)
```

这里定义训练模型，我们定义了神经网络模型dnn和线性模型linear，进行比较，优化器都选择`ProximalAdagradOptimizer`

```python
dnn = tf.estimator.DNNRegressor(
    feature_columns=feature_columns, 
    hidden_units=[32, 32, 16, 8], 
    model_dir="C://Users//Admin//Desktop//model//DNNRegressor",
    optimizer=tf.train.ProximalAdagradOptimizer(
        learning_rate=0.01, 
        l1_regularization_strength=0.001))

linear = tf.estimator.LinearRegressor(
    feature_columns=feature_columns,
    model_dir="C://Users//Admin//Desktop//model//LinearRegressor",
    optimizer=tf.train.ProximalAdagradOptimizer(
        learning_rate=0.01,
        l1_regularization_strength=0.001))

# 这里循环多次是为了了解训练过程是否过拟合，对应前面输入函数num_epochs=20设置一个较小的值
for i in range(7):
    dnn.train(input_fn=input_fn(x_train, y_train), max_steps=40000)
    dnn.evaluate(input_fn=input_fn(x_test, y_test, training=False))
    linear.train(input_fn=input_fn(x_train, y_train), max_steps=40000)
    linear.evaluate(inpun=input_fn(x_test, y_test, training=False))
```

{% asset_img dnn-linear.png 对比%}

从图中可以知道，线性模型的收敛速度比神经网络快，但是最终神经网络模型的效果略好一点，由于神经网络的结构对结果影响很大，我得到的最优的平均损失在0.50附近。
若分析数据进行特征工程，我想应该能得到更好的结果。

### 2.3 Keras
