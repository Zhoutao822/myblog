---
title: 语音识别-GMM
date: 2018-11-13 17:14:49
categories:
- speech recognition
tags:
- theory
- GMM
mathjax: true
---

参考：《解析深度学习-语音识别实践》第2章 混合高斯模型

## 1. 高斯分布

高斯分布又叫正态分布，在现实生活中充满了高斯分布，比如考试的分数、人的身高是一维高斯分布，图像处理领域的高斯模糊是二维高斯分布等等，通过高斯分布，我们可以很好的描述一类随机变量。

对于连续型标量随机变量$x$来说，若$x$服从正态分布，则其概率密度函数是

$$
p(x) = \frac{1}{(2\pi)^{1/2}\sigma} \exp[-\frac{1}{2}(\frac{x - \mu}{\sigma})^2] = N(x; \mu, \sigma^2)
\\
(-\infty < x < \infty; \sigma > 0)
$$

<!-- more -->
---
对于多元正态随机变量$\boldsymbol{x} = (x_1, x_2,...,x_D)^T$，其联合概率密度

$$
p(\boldsymbol{x}) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} \exp[-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu})^T\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})] = N(\boldsymbol{x};\boldsymbol{\mu}, \Sigma)
\\
其中 \boldsymbol{\mu} = E(\boldsymbol{x}) \in \mathbb{R}^D; \Sigma = E[(\boldsymbol{x} - \bar{\boldsymbol{x}})(\boldsymbol{x} - \bar{\boldsymbol{x}})^T] \in \mathbb{R}^{D \times D}
$$
---
对于一个标量连续随机变量x，若其服从混合高斯分布，其概率密度函数

$$
p(x) = \sum^{M}_{m=1}\frac{c_m}{(2\pi)^{1/2}\sigma_m}\exp[-\frac{1}{2}(\frac{x-\mu_m}{\sigma_m})^2]
\\
= \sum^{M}_{m=1}c_mN(x;\mu_m,\sigma^2_m)，其中\sum^M_{m=1}c_m = 1，E(x) = \sum^{M}_{m=1}c_m\mu_m
$$
---
推广到多变量的多元混合高斯分布，其联合概率密度函数为

$$
p(\boldsymbol{x}) = \sum^{M}_{m=1}\frac{c_m}{(2\pi)^{D/2}|\Sigma_m|^{1/2}}\exp[-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu}_m)^T\Sigma^{-1}_m(\boldsymbol{x} - \boldsymbol{\mu}_m)]
\\
= \sum^{M}_{m=1}c_mN(\boldsymbol{x};\boldsymbol{\mu}_m,\boldsymbol{\sigma}^2_m)
$$

在多元混合高斯分布中，如果变量x的维度D很大（比如40），那么使用全协方差矩阵（非对角）（$\Sigma_m$）将引入大量参数（大约为$M \times D^2$）。为了减少这个数量，可以使用对角协方差矩阵。当M很大时，也可以限制所有的协方差为相同矩阵，对所有的混合成分m，将参数$\Sigma_m$绑定在一起。

## 2. GMM示例

对于这样的二维数据分布，如何进行分类，若使用单个高斯成分，那么一个椭圆可以描述整体，但是根据观察，实际上这些数据应该属于两个不同的高斯成分，也就是说可以分为两类。

{% asset_img gmm01.png gmm%}

那么我们使用两个二维高斯分布来描述这个分布，当然最后的结果使用了$c_m$进行叠加

{% asset_img gmm02.png gmm%}

对于上例，我们定义$\boldsymbol{x} = (x_1, x_2)^T，M = 2$，则其GMM形式为

$$
p(\boldsymbol{x}) = c_1N(\boldsymbol{\mu}_1, \boldsymbol{\sigma}_1) + c_2N(\boldsymbol{\mu}_2, \boldsymbol{\sigma}_2)
$$

那么我们需要求解的参数为$(c_1, \boldsymbol{\mu}_1, \boldsymbol{\sigma}_1; c_2, \boldsymbol{\mu}_2, \boldsymbol{\sigma}_2)$

## 3. 参数估计

这里使用EM算法（EM算法必收敛）估计参数变量$\Theta = {c_m, \boldsymbol{\mu}_m, \boldsymbol{\sigma}_m}$，首先回顾一下，GMM表示形式

$$
p(\boldsymbol{x}) = \sum^{M}_{m=1}c_mN(\boldsymbol{x};\boldsymbol{\mu}_m,\boldsymbol{\sigma}^2_m)
$$

其中$c_m$可以看作第m个高斯成分被选中的概率。我们引入一个新的M维变量$\boldsymbol{z}$，$z_m$只能取0或1；$z_m = 1$表示第m类被选中的概率$c_m$，即$p(z_m = 1) = c_m，且\sum^M_{m=1} z_m = 1$，所以$\boldsymbol{z}$的联合概率分布：

$$
p(\boldsymbol{z}) = p(z_1)p(z_2)...p(z_M) = \prod_{m=1}^{M}c_m^{z_m}
$$

又因为对于属于某一个高斯成分的数据来说（$z_m = 1$），其分布必定是高斯分布，所以

$$
p(\boldsymbol{x}|z_m = 1) = N(\boldsymbol{x}|\boldsymbol{\mu}_m, \boldsymbol{\sigma}_m)
$$

进而上式等价于

$$
p(\boldsymbol{x}|\boldsymbol{z}) = \prod_{m=1}^{M}N(\boldsymbol{x}|\boldsymbol{\mu}_m, \boldsymbol{\sigma}_m)^{z_m}
$$

我们有了$p(\boldsymbol{z})和p(\boldsymbol{x}|\boldsymbol{z})$，那么可以计算$p(\boldsymbol{x})$

$$
p(\boldsymbol{x}) = \sum_Z p(\boldsymbol{x}|\boldsymbol{z})p(\boldsymbol{z})
\\
= \sum^{M}_{m=1}c_mN(\boldsymbol{x};\boldsymbol{\mu}_m,\boldsymbol{\sigma}^2_m)，z_m = 0的项在连乘中为1
$$

所以最终又回到了GMM的表示形式，但是我们增加了**隐含变量z**用于描述我们已经知道有多少个高斯成分。

---
在贝叶斯的思想下，$p(\boldsymbol{z})$是先验概率，$p(\boldsymbol{x}|\boldsymbol{z})$是似然概率，我们可以求解出后验概率$p(\boldsymbol{z}|\boldsymbol{x})$。

E步：基于当前迭代轮数（用j表示），针对某个高斯成分m，用给定观察值$\boldsymbol{x}^{(t)}$计算得到的后验概率$t = 1, ..., N，（N是采样率）$

$$
h^{(j)}_m(t) = p(z_m^{(j)}=1|\boldsymbol{x}^{(t)})
\\
= \frac{p(z_m^{(j)}=1)p(\boldsymbol{x}^{(t)}|z_m^{(j)}=1)}{p(\boldsymbol{x}^{(t)})}
\\
= \frac{p(z_m^{(j)}=1)p(\boldsymbol{x}^{(t)}|z_m^{(j)}=1)}{\sum^M_{i=1}p(z_i=1)p(\boldsymbol{x}^{(t)}|z_i=1)}
\\
= \frac{c_m^{(j)}N(\boldsymbol{x}^{(t)}; \boldsymbol{\mu}_m^{(j)}, \boldsymbol{\sigma}_m^{(j)})}{\sum^M_{i=1}c_i^{(j)}N(\boldsymbol{x}^{(t)}; \boldsymbol{\mu}_i^{(j)}, \boldsymbol{\sigma}_i^{(j)})}
$$

M步：基于后验概率估计参数$\Theta$

$$
c_m^{(j + 1)} = \frac{1}{N}\sum^N_{t=1}h_m^{(j)}(t)
\\
\boldsymbol{\mu}_m^{(j + 1)} = \frac{\sum^N_{t=1}h_m^{(j)}(t)\boldsymbol{x}^{(t)}}{\sum^N_{t=1}h_m^{(j)}(t)}
\\
\boldsymbol{\sigma}_m^{(j + 1)} = \frac{\sum^N_{t=1}h_m^{(j)}(t)[\boldsymbol{x}^{(t)} - \boldsymbol{\mu}_m^{(j)}][\boldsymbol{x}^{(t)} - \boldsymbol{\mu}_m^{(j)}]^T}{\sum^N_{t=1}h_m^{(j)}(t)}
$$

计算对数似然函数

$$
\ln p(\boldsymbol{x};c, \boldsymbol{\mu}, \boldsymbol{\sigma}) = \sum^N_{t=1}\ln{(\sum^M_{m=1}c_mN(\boldsymbol{x}^{(t)}|\boldsymbol{\mu}_m, \boldsymbol{\sigma}_m))}
$$

先对$\Theta$进行初始化，然后执行E步，得到后验概率h，然后执行M步，计算新的$\Theta$，并更新参数，计算对数似然，直到参数收敛或对数似然收敛。

## 4. 代码实现

### 4.1 sklearn实现

使用`sklearn.mixture.GaussianMixture`

```python
#coding=utf-8
import numpy as np
from sklearn.datasets import load_wine
from sklearn.mixture import GaussianMixture
# 这里使用酒类数据，数据集包括178个样本，每个样本由13个特征表示，以及一个标签，标签0、1、2分别表示3种酒
rawData = load_wine()

data = rawData['data']
target = rawData['target']

# 指定n_components分类数，我们已知有3种酒
gmm = GaussianMixture(n_components=3)

# 指定初始化的均值，同时根据先后顺序分配种类标签，指定means_init属性可以避免分类过程中分类标签与target不同
gmm.means_init = np.array([data[target == i].mean(axis=0) for i in range(3)])

# 这里就不划分测试集，直接对训练数据预测
prediction = gmm.fit_predict(data, y=target)
print(prediction)

# 计算训练集上的准确率
acc = np.mean(np.equal(prediction, target).astype(np.float))
print('GMM prediction accuracy: {:.4f}'.format(acc))
```
> GMM prediction accuracy: 0.8034
