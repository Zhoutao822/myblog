---
title: 深度学习-优化器
date: 2018-11-26 22:18:54
categories:
- deep learning
tags:
- theory
- tensorflow
- keras  
- optimizer
mathjax: true
---

参考：

> [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
> [Proximal Algorithm 入门](http://www.luolei.info/2016/09/27/proximalAlgo/)
> [Proximal Algorithms](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf)
> [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
> [在线最优化求解(Online Optimization)之四：RDA](http://www.cnblogs.com/luctw/p/4757943.html)
> [在线学习算法FTRL详解](https://www.cnblogs.com/EE-NovRain/p/3810737.html)

## 1. Tensorflow与Optimizer

Optimizer在Tensorflow框架中有两处：

1. [`tf.train.Optimizer`](https://tensorflow.google.cn/api_docs/python/tf/train/Optimizer)
2. [`tf.keras.optimizers.Optimizer`](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers)

前一个基本用在`Estimator`或者自定义模型中，后一个是`Keras`框架自带的优化器，两者几乎没有区别，在`Keras`中也可以使用`tf.train`下的优化器。

`Tensorflow`中的`Optimizer`:

1. `tf.train.AdadeltaOptimizer`
2. `tf.train.AdagradDAOptimizer`
3. `tf.train.AdagradOptimizer`
4. `tf.train.AdamOptimizer`
5. `tf.train.FtrlOptimizer`
6. `tf.train.GradientDescentOptimizer`
7. `tf.train.MomentumOptimizer`
8. `tf.train.ProximalAdagradOptimizer`
9. `tf.train.ProximalGradientDescentOptimizer`
10. `tf.train.RMSPropOptimizer`

`Keras`中的`Optimizer`:

1. `tf.keras.optimizers.Adadelta`
2. `tf.keras.optimizers.Adagrad`
3. `tf.keras.optimizers.Adam`
4. `tf.keras.optimizers.Adamax`
5. `tf.keras.optimizers.Nadam`
6. `tf.keras.optimizers.RMSprop`
7. `tf.keras.optimizers.SGD`

<!-- more -->

## 2. 梯度下降及其变种

目前对于大多数神经网络、线性模型来说，其参数更新方式为梯度下降，其核心思想是计算损失函数对各个参数的偏导数，各个参数再根据偏导数做调整。
根据计算梯度时使用的数据集大小（或者说更新参数的时机），梯度下降算法划分为3种。

### 2.1 Batch gradient descent

BSD（批量梯度下降），对应我在`线性模型-coding`中写的标准梯度下降，它是基于**整个数据集**的损失来进行参数更新，显然这样做的好处是我们考虑的是整个数据集，坏处在于参数更新步伐很大（在学习率一定的条件下），计算需要的内存大、时间长，到后期参数更新很缓慢。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta)
$$

参数更新伪代码，注意参数更新次数等于训练轮数`Epochs`

```python
for i in range(nb_epochs):
    params_grad = evaluate_gradient (loss_function, data, params)
    params = params - learning_rate * params_grad
```

### 2.2 Stochastic gradient descent

SGD（随机梯度下降），对应我在`线性模型-coding`中写的随机梯度下降，它是基于**单个样本**的损失来进行参数更新，而这个样本的选择是随机的，也就是说，在一个`Epoch`中，我们从中不放回地随机选取一个样本来进行参数更新，直到样本被抽完。显然，随机梯度下降参数更新的频率（样本数 $\times$ `Epochs`）远大于BSD，这样做会面临一个问题，其梯度下降很不稳定，假如一个样本的数值异常大，那么它会导致参数更新朝着一个异常的方向，这种异常具有两面性，一方面可能会导致前面的许多样本带来的下降前功尽弃，另一方面可能会帮助跳出局部极小区域；实验表明，若随着训练轮数增加将学习率减少会有助于使梯度下降平稳下来。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta; x^{(i)};y^{(i)})
$$

参数更新伪代码，注意参数更新次数（样本数 $\times$ `Epochs`）

```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for example in data:
        params_grad = evaluate_gradient(loss_function, example, params)
        params = params - learning_rate * params_grad
```

### 2.3 Mini-batch gradient descent

MBGD（小批量梯度下降），看名字就知道这是每次**不放回地随机选出一个batch大小**的样本进行参数更新，相当于BSD和SGD地合体，各取所长，即考虑到一个batch大小地数据的偏差，又减小了运算开销，适当增加参数更新频率，所以几乎最常见的优化器都是基于MBGD的。通常一个batch的大小在50到256之间，但是根据具体的情况可以做大幅度调整。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta; x^{(i:i+n)};y^{(i:i+n)})
$$

参数更新伪代码，注意参数更新次数（样本数 $\div$ batch大小$n$  $\times$ `Epochs`）

```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for batch in get_batches(data, batch_size =50):
        params_grad = evaluate_gradient(loss_function, batch, params)
        params = params - learning_rate * params_grad
```

## 3. 学习率选择

学习率$\eta$如何选择对梯度下降来说同样重要

* 学习率过小，梯度下降缓慢；学习率过大，最终由于步伐过大导致在最低点波动而无法到达；
* 按照约定调整学习率，问题在于在训练之前如何决定在哪一轮开始调整，调整的幅度是多大；
* 对于稀疏的数据集，对于不同的特征使用相同的学习率是否合适，稀疏特征可能不希望更新步伐和其他特征相同，如何解决这些问题；
* 非凸问题，鞍点问题，如何解决被困在局部极小的区域，梯度变成0如何跳出。

## 4. 常见梯度下降算法

###  4.1 Momentum

{% asset_img momentum.jpg Momentum %}

Momentum（动量），核心思想是使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。其参数更新公式为

$$
v_t = \gamma v_{t-1} + \eta \bigtriangledown_\theta J(\theta)
\\
\theta = \theta - v_t
$$

$\gamma$一般设置为0.9或差不多的数值，这里可以理解为通过使用$v_t$存储梯度下降的动量，这是一个累积量，每一次参数更新都对其进行累积，梯度下降方向始终一致的特征的动量存储越多，每一轮更新时变化也就越多；反之，梯度相互抵消的特征，其存储的动量较少，相较于前者，梯度下降的速度差异就体现出来。

### 4.2 Nesterov accelerated gradient

{% asset_img nag.jpg NAG %}

NAG，核心思想是在Momentum步伐过大时减少下一步更新步伐，类似于刹车。其参数更新公式为

$$
v_t = \gamma v_{t-1} + \eta \bigtriangledown_\theta J(\theta - \gamma v_{t-1})
\\
\theta = \theta - v_t
$$

$\gamma$一般设置为0.9或差不多的数值，这里可以理解为在Momentum的基础上，我们设置一个踩刹车的步骤，当前面累计的动量很多导致当前更新步伐很大时，下一步会由于$\theta - \gamma v_{t-1}$产生一个较小的梯度变化，从而实现平稳下降避免速度的太快。实验表明在RNNs上，NAG有着很好的表现。

### 4.3  Adagrad

Adagrad，核心思想是对低频的参数做较大的更新，对高频的做较小的更新，这个算法针对的是学习率，将学习率变成一种动态的自动调节的参数。其参数更新公式为

$$
\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii}+\epsilon}} \cdot g_{t,i} 
$$

$\eta$一般设置为0.01，$t$代表steps，$g_{t,i}$表示第$t$步第$i$个参数的$\bigtriangledown_{\theta_t} J(\theta_{t,i})$，$G_t$是对角线矩阵，每个对角线元素$G_{t, ii}$的值是在时间上累计的所有的$\theta_i$的梯度的平方和，即$G_{t, ii} = \sum^t_{k=0}g_{k, i}^2$，$\epsilon$是一个常数，避免分母为0。随着时间增加，分母越来越大，学习率逐渐下降，且低频的参数做较大的更新，对高频的做较小的更新。实验表明在稀疏输入的条件下，Adagrad有很好的表现。

### 4.4  Adadelta

Adadelta，是对Adagrad的进一步优化，由于Adagrad是对整个时间上的累加，必然会导致时间越长学习率逐渐趋向0，Adadelta将这种累计限制在一个区间$w$内，使得学习率下降速度减缓。在实际使用中并不会定义$w$，而是使用均值作为替换。其参数更新公式为

$$
E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g^2_t
$$

$\gamma$一般设置为0.9，使用均值替换累加值，得到

$$
\bigtriangleup \theta_t = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}g_t
$$

更进一步，我们可以不设置学习率，直接使用前一步的均方根作为当前的学习率

$$
\\
E[\bigtriangleup \theta^2]_t = \gamma E[\bigtriangleup \theta^2]_{t-1} + (1-\gamma)\bigtriangleup \theta^2_t
\\
RMS[\bigtriangleup \theta]_t = \sqrt{E[\bigtriangleup \theta^2]_t + \epsilon}
\\
\bigtriangleup \theta_t = -\frac{RMS[\bigtriangleup \theta]_{t-1}}{RMS[g]_t}g_t
\\
\theta_{t+1} = \theta_t + \bigtriangleup \theta_t
$$

### 4.5 RMSprop

RMSprop，与Adadelta的前一部分相同。其参数更新公式为

$$
E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1g^2_t
$$

$\gamma$建议设置为0.9，使用均值替换累加值，得到

$$
\theta_{t+1} =\theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}g_t
$$

$\eta$设置为0.001

### 4.6  Adam

Adaptive Moment Estimation（自适应矩估计），除了像Adadelta和RMSprop一样存储了过去梯度的平方$v_t$的指数衰减平均值 ，也像momentum一样保持了过去梯度$m_t$的指数衰减平均值。

$$
m_t = \beta_1m_{t-1} + (1-\beta_1)g_t
\\
v_t = \beta_2v_{t-1} + (1-\beta_2)g_t^2
$$

如果$m_t$和$v_t$被初始化为0向量，那它们就会向0偏置，所以做了偏差校正，通过计算偏差校正后的$m_t$和$v_t$来抵消这些偏差

$$
\hat{m}_t = \frac{m_t}{1-\beta_1^t}
\\
\hat{v}_t = \frac{v_t}{1-\beta_2^t}
$$

其参数更新公式为

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t
$$

$\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$为默认设置。

### 4.7  AdaMax

AdaMax，修改了Adam的$v_t$，因为$v_t$可以看作是对$g_t$的$l_2$范数，AdaMax从$l_\infty$范数中选择最大的范数作为替换，以加速梯度下降，这里用$u_t$表示

$$
u_t = \beta^\infty_2v_{t-1} + (1-\beta^\infty_2)|g_t|^\infty
\\
= \max(\beta_2 \cdot v_{t-1}, |g_t|)
$$

其参数更新公式为

$$
\theta_{t+1} = \theta_t - \frac{\eta}{u_t}\hat{m}_t
$$

$\beta_1 = 0.9, \beta_2 = 0.999, \eta = 0.002$为默认设置。

### 4.8  Nadam

Nesterov-accelerated Adaptive Moment Estimation 是Adam和NAG的组合。

首先，从NAG中知道

$$
m_t = \gamma m_{t-1} + \eta g_t
$$

从Adam中知道

$$
m_t = \beta_1m_{t-1} + (1-\beta_1)g_t
\\
\hat{m}_t = \frac{m_t}{1-\beta_1^t}
\\
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t
$$

那么扩展一下更新公式

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}(\frac{\beta_1m_{t-1}}{1-\beta^t_1} + \frac{(1-\beta_1)g_t}{1-\beta^t_1})
$$

其中$\frac{\beta_1m_{t-1}}{1-\beta^t_1}$可以使用$\hat{m}_{t-1}$替换，与此同时，也可以根据NAG使用$\hat{m}_t$替换$\hat{m}_{t-1}$。最终参数更新公式为

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}(\beta_1\hat{m}_t + \frac{(1-\beta_1)g_t}{1-\beta^t_1})
$$

### 4.9 Proximal Algorithm

对应`tf.train.ProximalAdagradOptimizer`和`tf.train.ProximalGradientDescentOptimizer`，主要是针对稀疏矩阵，目的是为了更好地得到稀疏解。稀疏解一方面可以增强泛化性能，另一方面减小运算时内存压力。

Proximal Algorithm是求解$l1$正则项的方法，为什么$l1$比$l2$更容易产生稀疏解？

为了求解$z$：

$$
\underset{z \in \mathbb{R}}{\min} L = \lambda|z| + \frac{\gamma}{2}(z - x)^2
$$

当$z>0$时有

$$
\frac{\partial L}{\partial z} = \lambda + \gamma(z-x) = 0
\\
z=x-\frac{\lambda}{\gamma}
$$

当$z<0$时有

$$
\frac{\partial L}{\partial z} = -\lambda + \gamma(z-x) = 0
\\
z=x+\frac{\lambda}{\gamma}
$$

综上，当$x>\frac{\lambda}{\gamma}$，$z=x-\frac{\lambda}{\gamma}$，当$x<-\frac{\lambda}{\gamma}$，$z=x+\frac{\lambda}{\gamma}$；当$-\frac{\lambda}{\gamma} \leqslant x \leqslant \frac{\lambda}{\gamma}$，$z=0$，所以$l1$容易产生稀疏解。

若是$l2$：

$$
\underset{z \in \mathbb{R}}{\min} L = \lambda z^2 + \frac{\gamma}{2}(z - x)^2
$$

求导得

$$
z= \frac{\gamma}{\gamma + \lambda}x
$$

其中即使$x$很接近0，$z$也只是更接近0而不会变成0。

---

对于目标函数不是处处连续可微的情况通常使用[次梯度](https://zh.wikipedia.org/wiki/%E6%AC%A1%E5%AF%BC%E6%95%B0)进行优化，次梯度会导致两个问题：

* 求解慢
* 通常不会产生稀疏解

Proximal Algorithm算法可以解决这两个问题。

算法的核心部分proximal operator：

$$
prox_{\lambda f}(v) = \underset{x}{\arg \min}(f(x) + \frac{1}{2\lambda}||x-v||^2)
$$

上式描述的是求解一个离$v$不太远的点，且使$f(x)$尽可能的小，显然$f(x) \leqslant f(v)$

{% asset_img prox.jpg Proximal Algorithms in Statistics and Machine Learning %}

其中加粗的黑线表示作用域，浅色的黑线表示函数f的等高线，蓝色的点对应上面式子的v点，红色点表示最终求得的x点。

设待优化目标函数为$F(x)=l(x)+\phi(x)$，其中$l(x)$是连续可微的，$\phi(x)$不是处处连续的，这类优化目标在机器学习中比较常见，如$l(x)$表示最小二乘的拟合误差，$\phi(x)$表示$L1$正则化因子用于产生稀疏解。

> Proximal Gradient Algorithm
> for t in range(n)
> > 1.Gradient Step，定义$v^t$是沿着$l(x)$梯度方向找到的一个点：
> > $$v^t = x^t - \gamma \bigtriangledown l(x^t)$$
> > 2.Proximal Operator Step，使用$prox$优化$\phi(x)$：
> > $$x^{t+1}=prox_{\lambda \phi}(v^t)$$
> 直到收敛或达到最大迭代次数

参数$\lambda$的选择必须使$\bigtriangledown l(x)$满足[利普希茨连续](https://zh.wikipedia.org/wiki/%E5%88%A9%E6%99%AE%E5%B8%8C%E8%8C%A8%E9%80%A3%E7%BA%8C)，若利普希茨常数为$L$，则$\lambda \in (0, \frac{1}{L})$，若$L$未知，可以使用line search的方法去找：

> repeat
> > 1.$z = prox_{\lambda \phi}(v^t)$
> > 2.break if $f(z) \leqslant f(v^t) + \bigtriangledown f^T(v^t)(v^t-z)+\frac{1}{2\lambda}||v^t - z||^2$
> > 3.$\lambda = \frac{\lambda}{2}$
> return $x^{t+1} = z$

## 4.10 FTRL

Follow-the-regularized-Leader，在处理诸如逻辑回归之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色

## 4.11 RDA

Regularized Dual Averaging Algorithm（正则对偶平均算法），稀疏矩阵

## 5. 梯度下降可视化及选择

{% asset_img optimizer1.gif 优化算法在三维空间中随时间推移而变化的可视化效果 %}

{% asset_img optimizer2.gif 优化器 %}

Adagrad, Adadelta, RMSprop 几乎很快就找到了正确的方向并前进，收敛速度也相当快，而其它方法要么很慢，要么走了很多弯路才找到。

* 如果数据是稀疏的，就用自适应方法，即Adagrad, Adadelta, RMSprop, Adam
* RMSprop, Adadelta, Adam在很多情况下的效果是相似的
* Adam 就是在RMSprop的基础上加了bias-correction和momentum
* 随着梯度变的稀疏，Adam比RMSprop效果会好

## 6. 其他用于优化SGD的策略

### 6.1 Shuffling and Curriculum Learning

Shuffling是打乱训练集的顺序，Curriculum Learning是按照训练样本难易程度依次提供给模型，两者看似矛盾，但是其实是需要根据具体情况进行选择。比如在预测房价这个案例中，我们不希望数据从小到大的输入到模型中，这样可能会导致收敛很慢或者收敛到鞍点的情况发生，此时需要使用Shuffling；而在图像识别案例中，比如识别猫狗，我们可以先输入一些猫狗的简单素描图，这样模型能学习到一些基本特征，然后再输入复杂一点的图片（带环境、颜色、不同品种），Curriculum Learning往往能更快收敛；当然有实验表明再LSTMs的训练过程中，往往需要结合这些策略。

### 6.2 Batch normalization

小批量标准化，即在一个batch上进行数据标准化，比起在整个数据集上标准化，我们能使用更高的学习率且不要那么在意初始化参数。此外，批量正则化还可以看作是一种正则化手段，能够减少（甚至去除）留出法的使用。

### 6.3 Early stopping

早停法，即在误差或损失已经达到预期或者误差没有明显改进的情况下停止训练，一方面减少了训练开销，因为后期收敛速度变慢，另一方面有助于提升泛化性能。

### 6.4 Gradient noise

梯度噪声，在梯度更新时加入高斯噪声$N(0,\sigma_t^2)$

$$
g_{t,i} = g_{t, i} + N(0,\sigma_t^2)
\\
\sigma_t^2 = \frac{\eta}{(1+t)^\gamma}
$$

这种方式能够提升神经网络在不良初始化前提下的鲁棒性，并能帮助训练特别是深层、复杂的神经网络。实验发现，加入噪声项之后，模型更有可能发现并跳出在深度网络中频繁出现的局部最小值。