---
title: 深度学习-优化器
date: 2018-11-26 22:18:54
categories:
- deep learning
tags:
- theory
- tensorflow
- keras  
- optimizer
mathjax: true
---

参考：

> [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)

## 1. Tensorflow与Optimizer

Optimizer在Tensorflow框架中有两处：

1. [`tf.train.Optimizer`](https://tensorflow.google.cn/api_docs/python/tf/train/Optimizer)
2. [`tf.keras.optimizers.Optimizer`](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers)

前一个基本用在`Estimator`或者自定义模型中，后一个是`Keras`框架自带的优化器，两者几乎没有区别，在`Keras`中也可以使用`tf.train`下的优化器。

`Tensorflow`中的`Optimizer`:

1. `tf.train.AdadeltaOptimizer`
2. `tf.train.AdagradDAOptimizer`
3. `tf.train.AdagradOptimizer`
4. `tf.train.AdamOptimizer`
5. `tf.train.GradientDescentOptimizer`
6. `tf.train.MomentumOptimizer`
7. `tf.train.ProximalAdagradOptimizer`
8. `tf.train.ProximalGradientDescentOptimizer`
9. `tf.train.RMSPropOptimizer`

`Keras`中的`Optimizer`:

1. `tf.keras.optimizers.Adadelta`
2. `tf.keras.optimizers.Adagrad`
3. `tf.keras.optimizers.Adam`
4. `tf.keras.optimizers.Adamax`
5. `tf.keras.optimizers.Nadam`
6. `tf.keras.optimizers.RMSprop`
7. `tf.keras.optimizers.SGD`

<!-- more -->

## 2. 梯度下降及其变种

目前对于大多数神经网络、线性模型来说，其参数更新方式为梯度下降，其核心思想是计算损失函数对各个参数的偏导数，各个参数再根据偏导数做调整。
根据计算梯度时使用的数据集大小（或者说更新参数的时机），梯度下降算法划分为3种。

### 2.1 Batch gradient descent

BSD（批量梯度下降），对应我在`线性模型-coding`中写的标准梯度下降，它是基于**整个数据集**的损失来进行参数更新，显然这样做的好处是我们考虑的是整个数据集，坏处在于参数更新步伐很大（在学习率一定的条件下），计算需要的内存大、时间长，到后期参数更新很缓慢。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta)
$$

参数更新伪代码，注意参数更新次数等于训练轮数`Epochs`

```python
for i in range(nb_epochs):
    params_grad = evaluate_gradient (loss_function, data, params)
    params = params - learning_rate * params_grad
```

### 2.2 Stochastic gradient descent

SGD（随机梯度下降），对应我在`线性模型-coding`中写的随机梯度下降，它是基于**单个样本**的损失来进行参数更新，而这个样本的选择是随机的，也就是说，在一个`Epoch`中，我们从中不放回地随机选取一个样本来进行参数更新，直到样本被抽完。显然，随机梯度下降参数更新的频率（样本数 $\times$ `Epochs`）远大于BSD，这样做会面临一个问题，其梯度下降很不稳定，假如一个样本的数值异常大，那么它会导致参数更新朝着一个异常的方向，这种异常具有两面性，一方面可能会导致前面的许多样本带来的下降前功尽弃，另一方面可能会帮助跳出局部极小区域；实验表明，若随着训练轮数增加将学习率减少会有助于使梯度下降平稳下来。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta; x^{(i)};y^{(i)})
$$

参数更新伪代码，注意参数更新次数（样本数 $\times$ `Epochs`）

```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for example in data:
        params_grad = evaluate_gradient(loss_function, example, params)
        params = params - learning_rate * params_grad
```

### 2.3 Mini-batch gradient descent

MBGD（小批量梯度下降），看名字就知道这是每次不放回地随机选出一个batch大小的样本进行参数更新，相当于BSD和SGD地合体，各取所长，即考虑到一个batch大小地数据的偏差，又减小了运算开销，适当增加参数更新频率，所以几乎最常见的优化器都是基于MBGD的。通常一个batch的大小在50到256之间，但是根据具体的情况可以做大幅度调整。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta; x^{(i:i+n)};y^{(i:i+n)})
$$

参数更新伪代码，注意参数更新次数（样本数 $\div$ batch大小$n$  $\times$ `Epochs`）

```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for batch in get_batches(data, batch_size =50):
        params_grad = evaluate_gradient(loss_function, batch, params)
        params = params - learning_rate * params_grad
```

## 3. 学习率选择

学习率$\eta$如何选择对梯度下降来说同样重要

* 学习率过小，梯度下降缓慢；学习率过大，最终由于步伐过大导致在最低点波动而无法到达；
* 按照约定调整学习率，问题在于在训练之前如何决定在哪一轮开始调整，调整的幅度是多大；
* 对于稀疏的数据集，对于不同的特征使用相同的学习率是否合适，稀疏特征可能不希望更新步伐和其他特征相同，如何解决这些问题；
* 非凸问题，鞍点问题，如何解决被困在局部极小的区域，梯度变成0如何跳出。

## 4. 常见梯度下降算法

###  4.1 Momentum

{% asset_img momentum.jpg Momentum %}

Momentum（动量），核心思想是使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。其参数更新公式为

$$
v_t = \gamma v_{t-1} + \eta \bigtriangledown_\theta J(\theta)
\\
\theta = \theta - v_t
$$

$\gamma$一般设置为0.9或差不多的数值，这里可以理解为通过使用$v_t$存储梯度下降的动量，这是一个累积量，每一次参数更新都对其进行累积，梯度下降方向始终一致的特征的动量存储越多，每一轮更新时变化也就越多；反之，梯度相互抵消的特征，其存储的动量较少，相较于前者，梯度下降的速度差异就体现出来。

### 4.2 Nesterov accelerated gradient

{% asset_img nag.jpg NAG %}

NAG，核心思想是在Momentum步伐过大时减少下一步更新步伐，类似于刹车。其参数更新公式为

$$
v_t = \gamma v_{t-1} + \eta \bigtriangledown_\theta J(\theta - \gamma v_{t-1})
\\
\theta = \theta - v_t
$$

$\gamma$一般设置为0.9或差不多的数值，这里可以理解为在Momentum的基础上，我们设置一个踩刹车的步骤，当前面累计的动量很多导致当前更新步伐很大时，下一步会由于$\theta - \gamma v_{t-1}$产生一个较小的梯度变化，从而实现平稳下降避免速度的太快。实验表明在RNNs上，NAG有着很好的表现。

### 4.3  Adagrad

Adagrad，核心思想是对低频的参数做较大的更新，对高频的做较小的更新，这个算法针对的是学习率，将学习率变成一种动态的自动调节的参数。其参数更新公式为

$$
\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii}+\epsilon}} \cdot g_{t,i} 
$$

$\eta$一般设置为0.01，$t$代表steps，$g_{t,i}$表示第$t$步第$i$个参数的$\bigtriangledown_{\theta_t} J(\theta_{t,i})$，$G_t$是对角线矩阵，每个对角线元素$G_{t, ii}$的值是在时间上累计的所有的$\theta_i$的梯度的平方和，即$G_{t, ii} = \sum^t_{k=0}g_{k, i}^2$，$\epsilon$是一个常数，避免分母为0。随着时间增加，分母越来越大，学习率逐渐下降，且低频的参数做较大的更新，对高频的做较小的更新。实验表明在稀疏输入的条件下，Adagrad有很好的表现。

### 4.4  Adadelta

Adadelta，是对Adagrad的进一步优化，由于Adagrad是对整个时间上的累加，必然会导致时间越长学习率逐渐趋向0，Adadelta将这种累计限制在一个区间$w$内，使得学习率下降速度减缓。在实际使用中并不会定义$w$，而是使用均值作为替换。其参数更新公式为

$$
E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g^2_t
$$

$\gamma$一般设置为0.9，使用均值替换累加值，得到

$$
\bigtriangleup \theta_t = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}g_t
$$

更进一步，我们可以不设置学习率，直接使用前一步的均方根作为当前的学习率

$$
\\
E[\bigtriangleup \theta^2]_t = \gamma E[\bigtriangleup \theta^2]_{t-1} + (1-\gamma)\bigtriangleup \theta^2_t
\\
RMS[\bigtriangleup \theta]_t = \sqrt{E[\bigtriangleup \theta^2]_t + \epsilon}
\\
\bigtriangleup \theta_t = -\frac{RMS[\bigtriangleup \theta]_{t-1}}{RMS[g]_t}g_t
\\
\theta_{t+1} = \theta_t + \bigtriangleup \theta_t
$$

### 4.5 RMSprop

RMSprop，与Adadelta的前一部分相同。其参数更新公式为

$$
E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1g^2_t
$$

$\gamma$建议设置为0.9，使用均值替换累加值，得到

$$
\theta_{t+1} =\theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}g_t
$$

$\eta$设置为0.001

### 4.6  Adam

Adaptive Moment Estimation（自适应矩估计），除了像Adadelta和RMSprop一样存储了过去梯度的平方$v_t$的指数衰减平均值 ，也像momentum一样保持了过去梯度$m_t$的指数衰减平均值。

$$
m_t = \beta_1m_{t-1} + (1-\beta_1)g_t
\\
v_t = \beta_2v_{t-1} + (1-\beta_2)g_t^2
$$

如果$m_t$和$v_t$被初始化为0向量，那它们就会向0偏置，所以做了偏差校正，通过计算偏差校正后的$m_t$和$v_t$来抵消这些偏差

$$
\hat{m}_t = \frac{m_t}{1-\beta_1^t}
\\
\hat{v}_t = \frac{v_t}{1-\beta_2^t}
$$

其参数更新公式为

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t
$$

$\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$为默认设置。

### 4.7  AdaMax

AdaMax，修改了Adam的$v_t$，因为$v_t$可以看作是对$g_t$的$l_2$范数，AdaMax从$l_\infty$范数中选择最大的范数作为替换，以加速梯度下降，这里用$u_t$表示

$$
u_t = \beta^\infty_2v_{t-1} + (1-\beta^\infty_2)|g_t|^\infty
\\
= \max(\beta_2 \cdot v_{t-1}, |g_t|)
$$

其参数更新公式为

$$
\theta_{t+1} = \theta_t - \frac{\eta}{u_t}\hat{m}_t
$$

$\beta_1 = 0.9, \beta_2 = 0.999, \eta = 0.002$为默认设置。

### 4.8  Nadam

Nesterov-accelerated Adaptive Moment Estimation 是Adam和NAG的组合。

首先，从NAG中知道

$$
m_t = \gamma m_{t-1} + \eta g_t
$$

从Adam中知道

$$
m_t = \beta_1m_{t-1} + (1-\beta_1)g_t
\\
\hat{m}_t = \frac{m_t}{1-\beta_1^t}
\\
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t
$$

那么扩展一下更新公式

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}(\frac{\beta_1m_{t-1}}{1-\beta^t_1} + \frac{(1-\beta_1)g_t}{1-\beta^t_1})
$$

其中$\frac{\beta_1m_{t-1}}{1-\beta^t_1}$可以使用$\hat{m}_{t-1}$替换，与此同时，也可以根据NAG使用$\hat{m}_t$替换$\hat{m}_{t-1}$。最终参数更新公式为

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}(\beta_1\hat{m}_t + \frac{(1-\beta_1)g_t}{1-\beta^t_1})
$$

## 5 梯度下降可视化及选择

{% asset_img optimizer1.gif 优化算法在三维空间中随时间推移而变化的可视化效果 %}

{% asset_img optimizer2.gif 优化器 %}

Adagrad, Adadelta, RMSprop 几乎很快就找到了正确的方向并前进，收敛速度也相当快，而其它方法要么很慢，要么走了很多弯路才找到。

* 如果数据是稀疏的，就用自适应方法，即Adagrad, Adadelta, RMSprop, Adam
* RMSprop, Adadelta, Adam在很多情况下的效果是相似的
* Adam 就是在RMSprop的基础上加了bias-correction和momentum
* 随着梯度变的稀疏，Adam比RMSprop效果会好

