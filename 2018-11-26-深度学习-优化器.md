---
title: 深度学习-优化器
date: 2018-11-26 22:18:54
categories:
- Deep Learning
tags:
- Theory
- Tensorflow
- Keras  
- Optimizer
mathjax: true
---

参考：

> [《深度学习》第8章 深度模型中的优化](https://github.com/exacity/deeplearningbook-chinese)
> [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
> [Proximal Algorithm 入门](http://www.luolei.info/2016/09/27/proximalAlgo/)
> [Proximal Algorithms](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf)
> [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
> [在线最优化求解(Online Optimization)之四：RDA](http://www.cnblogs.com/luctw/p/4757943.html)
> [在线学习算法FTRL详解](https://www.cnblogs.com/EE-NovRain/p/3810737.html)

## 1. Tensorflow与Optimizer

Optimizer在Tensorflow框架中有两处：

1. [`tf.train.Optimizer`](https://tensorflow.google.cn/api_docs/python/tf/train/Optimizer)
2. [`tf.keras.optimizers.Optimizer`](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers)

前一个基本用在`Estimator`或者自定义模型中，后一个是`Keras`框架自带的优化器，两者几乎没有区别，在`Keras`中也可以使用`tf.train`下的优化器。

`Tensorflow`中的`Optimizer`:

1. `tf.train.AdadeltaOptimizer`
2. `tf.train.AdagradDAOptimizer`
3. `tf.train.AdagradOptimizer`
4. `tf.train.AdamOptimizer`
5. `tf.train.FtrlOptimizer`
6. `tf.train.GradientDescentOptimizer`
7. `tf.train.MomentumOptimizer`
8. `tf.train.ProximalAdagradOptimizer`
9. `tf.train.ProximalGradientDescentOptimizer`
10. `tf.train.RMSPropOptimizer`

`Keras`中的`Optimizer`:

1. `tf.keras.optimizers.Adadelta`
2. `tf.keras.optimizers.Adagrad`
3. `tf.keras.optimizers.Adam`
4. `tf.keras.optimizers.Adamax`
5. `tf.keras.optimizers.Nadam`
6. `tf.keras.optimizers.RMSprop`
7. `tf.keras.optimizers.SGD`

<!-- more -->

## 2. 梯度下降及其变种

目前对于大多数神经网络、线性模型来说，其参数更新方式为梯度下降，其核心思想是计算损失函数对各个参数的偏导数，各个参数再根据偏导数做调整。
根据计算梯度时使用的数据集大小（或者说更新参数的时机），梯度下降算法划分为3种。

### 2.1 Batch gradient descent

BSD（批量梯度下降），对应我在`线性模型-coding`中写的标准梯度下降，它是基于**整个数据集**的损失来进行参数更新，显然这样做的好处是我们考虑的是整个数据集，坏处在于参数更新步伐很大（在学习率一定的条件下），计算需要的内存大、时间长，到后期参数更新很缓慢。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta)
$$

参数更新伪代码，注意参数更新次数等于训练轮数`Epochs`

```python
for i in range(nb_epochs):
    params_grad = evaluate_gradient (loss_function, data, params)
    params = params - learning_rate * params_grad
```

### 2.2 Stochastic gradient descent

SGD（随机梯度下降），对应我在`线性模型-coding`中写的随机梯度下降，它是基于**单个样本**的损失来进行参数更新，而这个样本的选择是随机的，也就是说，在一个`Epoch`中，我们从中不放回地随机选取一个样本来进行参数更新，直到样本被抽完。显然，随机梯度下降参数更新的频率（样本数 $\times$ `Epochs`）远大于BSD，这样做会面临一个问题，其梯度下降很不稳定，假如一个样本的数值异常大，那么它会导致参数更新朝着一个异常的方向，这种异常具有两面性，一方面可能会导致前面的许多样本带来的下降前功尽弃，另一方面可能会帮助跳出局部极小区域；实验表明，若随着训练轮数增加将学习率减少会有助于使梯度下降平稳下来。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta; x^{(i)};y^{(i)})
$$

参数更新伪代码，注意参数更新次数（样本数 $\times$ `Epochs`）

```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for example in data:
        params_grad = evaluate_gradient(loss_function, example, params)
        params = params - learning_rate * params_grad
```

### 2.3 Mini-batch gradient descent

MBGD（小批量梯度下降），看名字就知道这是每次**不放回地随机选出一个batch大小**的样本进行参数更新，相当于BSD和SGD地合体，各取所长，即考虑到一个batch大小地数据的偏差，又减小了运算开销，适当增加参数更新频率，所以几乎最常见的优化器都是基于MBGD的。通常一个batch的大小在50到256之间，但是根据具体的情况可以做大幅度调整。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta; x^{(i:i+n)};y^{(i:i+n)})
$$

参数更新伪代码，注意参数更新次数（样本数 $\div$ batch大小$n$  $\times$ `Epochs`）

```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for batch in get_batches(data, batch_size =50):
        params_grad = evaluate_gradient(loss_function, batch, params)
        params = params - learning_rate * params_grad
```

## 3. 学习率选择

学习率$\eta$如何选择对梯度下降来说同样重要

* 学习率过小，梯度下降缓慢；学习率过大，最终由于步伐过大导致在最低点波动而无法到达；
* 按照约定调整学习率，问题在于在训练之前如何决定在哪一轮开始调整，调整的幅度是多大；
* 对于稀疏的数据集，对于不同的特征使用相同的学习率是否合适，稀疏特征可能不希望更新步伐和其他特征相同，如何解决这些问题；
* 非凸问题，鞍点问题，如何解决被困在局部极小的区域，梯度变成0如何跳出。

## 4. 常见梯度下降算法

###  4.1 Momentum

{% asset_img momentum.jpg Momentum %}

Momentum（动量），核心思想是使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。其参数更新公式为

$$
v_t = \gamma v_{t-1} + \eta \bigtriangledown_\theta J(\theta)
\\
\theta = \theta - v_t
$$

$\gamma$一般设置为0.9或差不多的数值，这里可以理解为通过使用$v_t$存储梯度下降的动量，这是一个累积量，每一次参数更新都对其进行累积，梯度下降方向始终一致的特征的动量存储越多，每一轮更新时变化也就越多；反之，梯度相互抵消的特征，其存储的动量较少，相较于前者，梯度下降的速度差异就体现出来。

### 4.2 Nesterov accelerated gradient

{% asset_img nag.jpg NAG %}

NAG，核心思想是在Momentum步伐过大时减少下一步更新步伐，类似于刹车。其参数更新公式为

$$
v_t = \gamma v_{t-1} + \eta \bigtriangledown_\theta J(\theta - \gamma v_{t-1})
\\
\theta = \theta - v_t
$$

$\gamma$一般设置为0.9或差不多的数值，这里可以理解为在Momentum的基础上，我们设置一个踩刹车的步骤，当前面累计的动量很多导致当前更新步伐很大时，下一步会由于$\theta - \gamma v_{t-1}$产生一个较小的梯度变化，从而实现平稳下降避免速度的太快。实验表明在RNNs上，NAG有着很好的表现。

### 4.3  Adagrad

Adagrad，核心思想是对低频的参数做较大的更新，对高频的做较小的更新，这个算法针对的是学习率，将学习率变成一种动态的自动调节的参数。其参数更新公式为

$$
\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii}+\epsilon}} \cdot g_{t,i} 
$$

$\eta$一般设置为0.01，$t$代表steps，$g_{t,i}$表示第$t$步第$i$个参数的$\bigtriangledown_{\theta_t} J(\theta_{t,i})$，$G_t$是对角线矩阵，每个对角线元素$G_{t, ii}$的值是在时间上累计的所有的$\theta_i$的梯度的平方和，即$G_{t, ii} = \sum^t_{k=0}g_{k, i}^2$，$\epsilon$是一个常数，避免分母为0。随着时间增加，分母越来越大，学习率逐渐下降，且低频的参数做较大的更新，对高频的做较小的更新。实验表明在稀疏输入的条件下，Adagrad有很好的表现。

{% asset_img adagrad.png %}

### 4.4  Adadelta

Adadelta，是对Adagrad的进一步优化，由于Adagrad是对整个时间上的累加，必然会导致时间越长学习率逐渐趋向0，Adadelta将这种累计限制在一个区间$w$内，使得学习率下降速度减缓。在实际使用中并不会定义$w$，而是使用均值作为替换。其参数更新公式为

$$
E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma)g^2_t
$$

$\gamma$一般设置为0.9，使用均值替换累加值，得到

$$
\bigtriangleup \theta_t = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}g_t
$$

更进一步，我们可以不设置学习率，直接使用前一步的均方根作为当前的学习率

$$
\\
E[\bigtriangleup \theta^2]_t = \gamma E[\bigtriangleup \theta^2]_{t-1} + (1-\gamma)\bigtriangleup \theta^2_t
\\
RMS[\bigtriangleup \theta]_t = \sqrt{E[\bigtriangleup \theta^2]_t + \epsilon}
\\
\bigtriangleup \theta_t = -\frac{RMS[\bigtriangleup \theta]_{t-1}}{RMS[g]_t}g_t
\\
\theta_{t+1} = \theta_t + \bigtriangleup \theta_t
$$

### 4.5 RMSprop

RMSprop，与Adadelta的前一部分相同。其参数更新公式为

$$
E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1g^2_t
$$

$\gamma$建议设置为0.9，使用均值替换累加值，得到

$$
\theta_{t+1} =\theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}g_t
$$

$\eta$设置为0.001

{% asset_img rmsp.png %}

{% asset_img nrmsp.png %}

### 4.6  Adam

Adaptive Moment Estimation（自适应矩估计），除了像Adadelta和RMSprop一样存储了过去梯度的平方$v_t$的指数衰减平均值 ，也像momentum一样保持了过去梯度$m_t$的指数衰减平均值。

$$
m_t = \beta_1m_{t-1} + (1-\beta_1)g_t
\\
v_t = \beta_2v_{t-1} + (1-\beta_2)g_t^2
$$

如果$m_t$和$v_t$被初始化为0向量，那它们就会向0偏置，所以做了偏差校正，通过计算偏差校正后的$m_t$和$v_t$来抵消这些偏差

$$
\hat{m}_t = \frac{m_t}{1-\beta_1^t}
\\
\hat{v}_t = \frac{v_t}{1-\beta_2^t}
$$

其参数更新公式为

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t
$$

$\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$为默认设置。

{% asset_img adam.png %}

### 4.7  AdaMax

AdaMax，修改了Adam的$v_t$，因为$v_t$可以看作是对$g_t$的$l_2$范数，AdaMax从$l_\infty$范数中选择最大的范数作为替换，以加速梯度下降，这里用$u_t$表示

$$
u_t = \beta^\infty_2v_{t-1} + (1-\beta^\infty_2)|g_t|^\infty
\\
= \max(\beta_2 \cdot v_{t-1}, |g_t|)
$$

其参数更新公式为

$$
\theta_{t+1} = \theta_t - \frac{\eta}{u_t}\hat{m}_t
$$

$\beta_1 = 0.9, \beta_2 = 0.999, \eta = 0.002$为默认设置。

### 4.8  Nadam

Nesterov-accelerated Adaptive Moment Estimation 是Adam和NAG的组合。

首先，从NAG中知道

$$
m_t = \gamma m_{t-1} + \eta g_t
$$

从Adam中知道

$$
m_t = \beta_1m_{t-1} + (1-\beta_1)g_t
\\
\hat{m}_t = \frac{m_t}{1-\beta_1^t}
\\
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}\hat{m}_t
$$

那么扩展一下更新公式

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}(\frac{\beta_1m_{t-1}}{1-\beta^t_1} + \frac{(1-\beta_1)g_t}{1-\beta^t_1})
$$

其中$\frac{\beta_1m_{t-1}}{1-\beta^t_1}$可以使用$\hat{m}_{t-1}$替换，与此同时，也可以根据NAG使用$\hat{m}_t$替换$\hat{m}_{t-1}$。最终参数更新公式为

$$
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t}+\epsilon}(\beta_1\hat{m}_t + \frac{(1-\beta_1)g_t}{1-\beta^t_1})
$$

### 4.9 Proximal Algorithm

对应`tf.train.ProximalAdagradOptimizer`和`tf.train.ProximalGradientDescentOptimizer`，主要是针对稀疏矩阵，目的是为了更好地得到稀疏解。稀疏解一方面可以增强泛化性能，另一方面减小运算时内存压力。

Proximal Algorithm是求解$l1$正则项的方法，为什么$l1$比$l2$更容易产生稀疏解？

为了求解$z$：

$$
\underset{z \in \mathbb{R}}{\min} L = \lambda|z| + \frac{\gamma}{2}(z - x)^2
$$

当$z>0$时有

$$
\frac{\partial L}{\partial z} = \lambda + \gamma(z-x) = 0
\\
z=x-\frac{\lambda}{\gamma}
$$

当$z<0$时有

$$
\frac{\partial L}{\partial z} = -\lambda + \gamma(z-x) = 0
\\
z=x+\frac{\lambda}{\gamma}
$$

综上，当$x>\frac{\lambda}{\gamma}$，$z=x-\frac{\lambda}{\gamma}$，当$x<-\frac{\lambda}{\gamma}$，$z=x+\frac{\lambda}{\gamma}$；当$-\frac{\lambda}{\gamma} \leqslant x \leqslant \frac{\lambda}{\gamma}$，$z=0$，所以$l1$容易产生稀疏解。

若是$l2$：

$$
\underset{z \in \mathbb{R}}{\min} L = \lambda z^2 + \frac{\gamma}{2}(z - x)^2
$$

求导得

$$
z= \frac{\gamma}{\gamma + \lambda}x
$$

其中即使$x$很接近0，$z$也只是更接近0而不会变成0。

---

对于目标函数不是处处连续可微的情况通常使用[次梯度](https://zh.wikipedia.org/wiki/%E6%AC%A1%E5%AF%BC%E6%95%B0)进行优化，次梯度会导致两个问题：

* 求解慢
* 通常不会产生稀疏解

Proximal Algorithm算法可以解决这两个问题。

算法的核心部分proximal operator：

$$
prox_{\lambda f}(v) = \underset{x}{\arg \min}(f(x) + \frac{1}{2\lambda}||x-v||^2)
$$

上式描述的是求解一个离$v$不太远的点，且使$f(x)$尽可能的小，显然$f(x) \leqslant f(v)$

{% asset_img prox.jpg Proximal Algorithms in Statistics and Machine Learning %}

其中加粗的黑线表示作用域，浅色的黑线表示函数f的等高线，蓝色的点对应上面式子的v点，红色点表示最终求得的x点。

设待优化目标函数为$F(x)=l(x)+\phi(x)$，其中$l(x)$是连续可微的，$\phi(x)$不是处处连续的，这类优化目标在机器学习中比较常见，如$l(x)$表示最小二乘的拟合误差，$\phi(x)$表示$L1$正则化因子用于产生稀疏解。

> Proximal Gradient Algorithm
> 
> for t in range(n)
> 
> > 1.Gradient Step，定义$v^t$是沿着$l(x)$梯度方向找到的一个点：
> > $$v^t = x^t - \gamma \bigtriangledown l(x^t)$$
> > 2.Proximal Operator Step，使用$prox$优化$\phi(x)$：
> > $$x^{t+1}=prox_{\lambda \phi}(v^t)$$
> 
> 直到收敛或达到最大迭代次数

参数$\lambda$的选择必须使$\bigtriangledown l(x)$满足[利普希茨连续](https://zh.wikipedia.org/wiki/%E5%88%A9%E6%99%AE%E5%B8%8C%E8%8C%A8%E9%80%A3%E7%BA%8C)，若利普希茨常数为$L$，则$\lambda \in (0, \frac{1}{L})$，若$L$未知，可以使用line search的方法去找：

> repeat
> 
> > 1.$z = prox_{\lambda \phi}(v^t)$
> >
> > 2.break if $f(z) \leqslant f(v^t) + \bigtriangledown f^T(v^t)(v^t-z)+\frac{1}{2\lambda}||v^t - z||^2$
> >
> > 3.$\lambda = \frac{\lambda}{2}$
> 
> return $x^{t+1} = z$

### 4.10 FTRL

Follow-the-regularized-Leader，在处理诸如逻辑回归之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色

### 4.11 RDA

Regularized Dual Averaging Algorithm（正则对偶平均算法），稀疏矩阵

## 5. 梯度下降可视化及选择

{% asset_img optimizer1.gif 优化算法在三维空间中随时间推移而变化的可视化效果 %}

{% asset_img optimizer2.gif 优化器 %}

Adagrad, Adadelta, RMSprop 几乎很快就找到了正确的方向并前进，收敛速度也相当快，而其它方法要么很慢，要么走了很多弯路才找到。

* 如果数据是稀疏的，就用自适应方法，即Adagrad, Adadelta, RMSprop, Adam
* RMSprop, Adadelta, Adam在很多情况下的效果是相似的
* Adam 就是在RMSprop的基础上加了bias-correction和momentum
* 随着梯度变的稀疏，Adam比RMSprop效果会好

## 6. 其他用于优化SGD的策略

### 6.1 Shuffling and Curriculum Learning

Shuffling是打乱训练集的顺序，Curriculum Learning是按照训练样本难易程度依次提供给模型，两者看似矛盾，但是其实是需要根据具体情况进行选择。比如在预测房价这个案例中，我们不希望数据从小到大的输入到模型中，这样可能会导致收敛很慢或者收敛到鞍点的情况发生，此时需要使用Shuffling；而在图像识别案例中，比如识别猫狗，我们可以先输入一些猫狗的简单素描图，这样模型能学习到一些基本特征，然后再输入复杂一点的图片（带环境、颜色、不同品种），Curriculum Learning往往能更快收敛；当然有实验表明再LSTMs的训练过程中，往往需要结合这些策略。

### 6.2 Batch normalization

小批量标准化，即在一个batch上进行数据标准化，比起在整个数据集上标准化，我们能使用更高的学习率且不要那么在意初始化参数。此外，批量正则化还可以看作是一种正则化手段，能够减少（甚至去除）留出法的使用。

### 6.3 Early stopping

早停法，即在误差或损失已经达到预期或者误差没有明显改进的情况下停止训练，一方面减少了训练开销，因为后期收敛速度变慢，另一方面有助于提升泛化性能。

### 6.4 Gradient noise

梯度噪声，在梯度更新时加入高斯噪声$N(0,\sigma_t^2)$

$$
g_{t,i} = g_{t, i} + N(0,\sigma_t^2)
\\
\sigma_t^2 = \frac{\eta}{(1+t)^\gamma}
$$

这种方式能够提升神经网络在不良初始化前提下的鲁棒性，并能帮助训练特别是深层、复杂的神经网络。实验发现，加入噪声项之后，模型更有可能发现并跳出在深度网络中频繁出现的局部最小值。

## 7. 神经网络优化中的挑战

### 7.1 病态

Hessian矩阵$\boldsymbol{H}$的病态，体现在随机梯度下降会“卡”在某些情况，此时即使很小的更新步长也会增加代价函数。

代价函数的二阶泰勒级数展开预测梯度下降中的$-\epsilon\boldsymbol{g}$会增加

$$
\frac{1}{2}\epsilon^2\boldsymbol{g}^T\boldsymbol{Hg} - \epsilon \boldsymbol{g}^T\boldsymbol{g}
$$

到代价中。当$\frac{1}{2}\epsilon^2\boldsymbol{g}^T\boldsymbol{Hg}$超过$\epsilon \boldsymbol{g}^T\boldsymbol{g}$时，梯度的病态会成为问题。判断病态是否不利于神经网络训练任务，我们可以监测平方梯度范数$\boldsymbol{g}^T\boldsymbol{g},\boldsymbol{g}^T\boldsymbol{Hg}$。在很多情况中，梯度范数不会在训练过程中显著缩小，但是$\boldsymbol{g}^T\boldsymbol{Hg}$的增长会超过一个数量级。其结果是尽管梯度很强，学习会变得非常缓慢，因为学习率必须收缩以弥补更强的曲率。

牛顿法在解决带有病态条件的Hessian矩阵的凸优化问题时，是一个非常优秀的工具。

### 7.2 局部极小值

对于凸优化问题来说，局部极小等价于全局最小。对于非凸函数来说，可能会存在多个局部极小值。

由于模型可辨识性问题，神经网络和任意具有多个等效参数化潜变量的模型都会具有多个局部极小值。如果一个足够大的训练集可以唯一确定一组模型参数，那么该模型被称为可辨认的。带有潜变量的模型通常是不可辨认的，因为通过相互交换潜变量我们能得到等价的模型。

这些模型可辨识性问题意味着神经网络代价函数具有非常多、甚至不可数无限多的局部极小值。然而，所有这些由于不可辨识性问题而产生的局部极小值都有相同的代价函数值。因此，这些局部极小值并非是非凸所带来的问题。

一种能够排除局部极小值是主要问题的检测方法是画出梯度范数随时间的变化。如果梯度范数没有缩小到一个微小的值，那么该问题既不是局部极小值，也不是其他形式的临界点。在高维空间中，很难明确证明局部极小值是导致问题的原因。许多并非局部极小值的结构也具有很小的梯度。

### 7.3 高原、鞍点和其他平坦区域

对于很多高维非凸函数而言，局部极小值（以及极大值）事实上都远少于另一类梯度为零的点：鞍点。鞍点附近的某些点比鞍点有更大的代价，而其他点则有更小的代价。在鞍点处，Hessian矩阵同时具有正负特征值。位于正特征值对应的特征向量方向的点比鞍点有更大的代价，反之，位于负特征值对应的特征向量方向的点有更小的代价。我们可以将鞍点视为代价函数某个横截面上的局部极小点，同时也可以视为代价函数某个横截面上的局部极大点

鞍点激增对于训练算法来说有哪些影响呢？对于只使用梯度信息的一阶优化算法而言，目前情况还不清楚。鞍点附近的梯度通常会非常小。另一方面，实验中梯度下降似乎可以在许多情况下逃离鞍点。

也可能存在恒值的、宽且平坦的区域。在这些区域，梯度和 Hessian 矩阵都是零。这种退化的情形是所有数值优化算法的主要问题。在凸问题中，一个宽而平坦的区间肯定包含全局极小值，但是对于一般的优化问题而言，这样的区域可能会对应着目标函数中一个较高的值。

### 7.4 悬崖和梯度爆炸

{% asset_img grad.jpg 梯度爆炸与弥散 %}

多层神经网络通常存在像悬崖一样的斜率较大区域，这是由于几个较大的权重相乘导致的。遇到斜率极大的悬崖结构时，梯度更新会很大程度地改变参数值，通常会完全跳过这类悬崖结构。

**梯度截断**基本想法源自梯度并没有指明最佳步长，只说明了在无限小区域内的最佳方向。当
传统的梯度下降算法提议更新很大一步时，启发式梯度截断会干涉来减小步长，从而使其不太可能走出梯度近似为最陡下降方向的悬崖区域。悬崖结构在循环神经网络的代价函数中很常见，因为这类模型会涉及到多个因子的相乘，其中每个因子对应一个时间步。因此，长期时间序列会产生大量相乘。

### 7.5 长期依赖

当计算图变得极深时，神经网络优化算法会面临的另外一个难题就是长期依赖问题-由于变深的结构使模型丧失了学习到先前信息的能力，让优化变得极其困难。

例如，假设某个计算图中包含一条反复与矩阵$\boldsymbol{W}$相乘的路径（RNNs），那么$t$步后，相当于乘以$\boldsymbol{W}^t$，假设$\boldsymbol{W}$有特征分解$\boldsymbol{W} = \boldsymbol{V} diag(\boldsymbol{\lambda})\boldsymbol{V}^{-1}$，那么

$$
\boldsymbol{W}^t = \boldsymbol{V} diag(\boldsymbol{\lambda})^t\boldsymbol{V}^{-1}
$$

显然，当特征值$\lambda_i$不在1附近时，若在量级上大于1则会爆炸；若小于1时则会消失。**梯度消失与爆炸问题**是指该计算图上的梯度也会因为$diag(\boldsymbol{\lambda})^t$大幅度变化。梯度消失使得我们难以知道参数朝哪个方向移动能够改进代价函数，而梯度爆炸会使得学习不稳定。之前描述的促使我们使用梯度截断的悬崖结构便是梯度爆炸现象的一个例子。

此处描述的在各时间步重复与$\boldsymbol{W}$相乘非常类似于寻求矩阵$\boldsymbol{W}$的最大特征值及对应特征向量的**幂方法**。从这个观点来看，$\boldsymbol{x}^T\boldsymbol{W}^t$最终会丢弃$\boldsymbol{x}$中所有与$\boldsymbol{W}$的主特征向量正交的成分。

### 7.6 非精确梯度

大多数优化算法的先决条件都是我们知道精确的梯度或是Hessian矩阵。在实践中，通常这些量会有噪声，甚至是有偏的估计。几乎每一个深度学习算法都需要基于采样的估计，至少使用训练样本的小批量来计算梯度。

在其他情况，我们希望最小化的目标函数实际上是难以处理的。当目标函数不可解时，通常其梯度也是难以处理的。在这种情况下，我们只能近似梯度。这些问题主要出现在第三部分中更高级的模型中。例如，**对比散度**是用来近似玻尔兹曼机中难以处理的对数似然梯度的一种技术。

各种神经网络优化算法的设计都考虑到了梯度估计的缺陷。我们可以选择比真实损失函数更容易估计的代理损失函数来避免这个问题。

### 7.7 局部和全局结构间的弱对应

如果该方向在局部改进很大，但并没有指向代价低得多的遥远区域，那么我们有可能在单点处克服以上所有困难，但仍然表现不佳。

大多数优化研究的难点集中于训练是否找到了全局最小点、局部极小点或是鞍点，但在实践中神经网络不会到达任何一种临界点。

不管哪个问题最重要，如果存在一个区域，我们遵循局部下降便能合理地直接到达某个解，并且我们能够在该良好区域上初始化学习，那么这些问题都可以避免。最终的观点还是建议在传统优化算法上研究怎样选择更佳的初始化点，以此来实现目标更切实可行。

### 7.8 优化的理论限制

一些理论结果表明，我们为神经网络设计的任何优化算法都有性能限制。通常这些结果不影响神经网络在实践中的应用。

一些理论结果仅适用于神经网络的单元输出离散值的情况。然而，大多数神经网络单元输出光滑的连续值，使得局部搜索求解优化可行。一些理论结果表明，存在某类问题是不可解的，但很难判断一个特定问题是否属于该类。其他结果表明，寻找给定规模的网络的一个可行解是很困难的，但在实际情况中，我们通过设置更多参数，使用更大的网络，能轻松找到可接受的解。此外，在神经网络训练中，我们通常不关注某个函数的精确极小点，而只关注将其值下降到足够小以获得一个良好的泛化误差。对优化算法是否能完成此目标进行理论分析是非常困难的。因此，研究优化算法更现实的性能上界仍然是学术界的一个重要目标。