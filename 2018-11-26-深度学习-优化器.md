---
title: 深度学习-优化器
date: 2018-11-26 22:18:54
categories:
- deep learning
tags:
- theory
- tensorflow
- keras  
- optimizer
mathjax: true
---

参考：

> [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)

## 1. Tensorflow与Optimizer

Optimizer在Tensorflow框架中有两处：

1. [`tf.train.Optimizer`](https://tensorflow.google.cn/api_docs/python/tf/train/Optimizer)
2. [`tf.keras.optimizers.Optimizer`](https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers)

前一个基本用在`Estimator`或者自定义模型中，后一个是`Keras`框架自带的优化器，两者几乎没有区别，在`Keras`中也可以使用`tf.train`下的优化器。

`Tensorflow`中的`Optimizer`:

1. `tf.train.AdadeltaOptimizer`
2. `tf.train.AdagradDAOptimizer`
3. `tf.train.AdagradOptimizer`
4. `tf.train.AdamOptimizer`
5. `tf.train.GradientDescentOptimizer`
6. `tf.train.MomentumOptimizer`
7. `tf.train.ProximalAdagradOptimizer`
8. `tf.train.ProximalGradientDescentOptimizer`
9. `tf.train.RMSPropOptimizer`

`Keras`中的`Optimizer`:

1. `tf.keras.optimizers.Adadelta`
2. `tf.keras.optimizers.Adagrad`
3. `tf.keras.optimizers.Adam`
4. `tf.keras.optimizers.Adamax`
5. `tf.keras.optimizers.Nadam`
6. `tf.keras.optimizers.RMSprop`
7. `tf.keras.optimizers.SGD`

<!-- more -->

## 2. 梯度下降及其变种

目前对于大多数神经网络、线性模型来说，其参数更新方式为梯度下降，其核心思想是计算损失函数对各个参数的偏导数，各个参数再根据偏导数做调整。
根据计算梯度时使用的数据集大小（或者说更新参数的时机），梯度下降算法划分为3种。

### 2.1 Batch gradient descent

BSD（批量梯度下降），对应我在`线性模型-coding`中写的标准梯度下降，它是基于**整个数据集**的损失来进行参数更新，显然这样做的好处是我们考虑的是整个数据集，坏处在于参数更新步伐很大（在学习率一定的条件下），计算需要的内存大、时间长，到后期参数更新很缓慢。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta)
$$

参数更新伪代码，注意参数更新次数等于训练轮数`Epochs`

```python
for i in range(nb_epochs):
    params_grad = evaluate_gradient (loss_function, data, params)
    params = params - learning_rate * params_grad
```

### 2.2 Stochastic gradient descent

SGD（随机梯度下降），对应我在`线性模型-coding`中写的随机梯度下降，它是基于**单个样本**的损失来进行参数更新，而这个样本的选择是随机的，也就是说，在一个`Epoch`中，我们从中不放回地随机选取一个样本来进行参数更新，直到样本被抽完。显然，随机梯度下降参数更新的频率（样本数 $\times$ `Epochs`）远大于BSD，这样做会面临一个问题，其梯度下降很不稳定，假如一个样本的数值异常大，那么它会导致参数更新朝着一个异常的方向，这种异常具有两面性，一方面可能会导致前面的许多样本带来的下降前功尽弃，另一方面可能会帮助跳出局部极小区域；实验表明，若随着训练轮数增加将学习率减少会有助于使梯度下降平稳下来。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta; x^{(i)};y^{(i)})
$$

参数更新伪代码，注意参数更新次数（样本数 $\times$ `Epochs`）

```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for example in data:
        params_grad = evaluate_gradient(loss_function, example, params)
        params = params - learning_rate * params_grad
```

### 2.3 Mini-batch gradient descent

MBGD（小批量梯度下降），看名字就知道这是每次不放回地随机选出一个batch大小的样本进行参数更新，相当于BSD和SGD地合体，各取所长，即考虑到一个batch大小地数据的偏差，又减小了运算开销，适当增加参数更新频率，所以几乎最常见的优化器都是基于MBGD的。通常一个batch的大小在50到256之间，但是根据具体的情况可以做大幅度调整。其参数更新公式为

$$
\theta = \theta - \eta \cdot \bigtriangledown_\theta J(\theta; x^{(i:i+n)};y^{(i:i+n)})
$$

参数更新伪代码，注意参数更新次数（样本数 $\div$ batch大小$n$  $\times$ `Epochs`）

```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for batch in get_batches(data, batch_size =50):
        params_grad = evaluate_gradient(loss_function, batch, params)
        params = params - learning_rate * params_grad
```

## 3. 学习率选择

学习率$\eta$如何选择对梯度下降来说同样重要

* 学习率过小，梯度下降缓慢；学习率过大，最终由于步伐过大导致在最低点波动而无法到达；
* 按照约定调整学习率，问题在于在训练之前如何决定在哪一轮开始调整，调整的幅度是多大；
* 对于稀疏的数据集，对于不同的特征使用相同的学习率是否合适，稀疏特征可能不希望更新步伐和其他特征相同，如何解决这些问题；
* 非凸问题，鞍点问题，如何解决被困在局部极小的区域，梯度变成0如何跳出。

## 4. 常见梯度下降算法

###  4.1 Momentum

{% asset_img momentum.jpg Momentum %}

Momentum（动量），核心思想是使得梯度方向不变的维度上速度变快，梯度方向有所改变的维度上的更新速度变慢，这样就可以加快收敛并减小震荡。其参数更新公式为

$$
v_t = \gamma v_{t-1} + \eta \bigtriangledown_\theta J(\theta)
\\
\theta = \theta - v_t
$$

$\gamma$一般设置为0.9或差不多的数值，这里可以理解为通过使用$v_t$存储梯度下降的动量，这是一个累积量，每一次参数更新都对其进行累积，梯度下降方向始终一致的特征的动量存储越多，每一轮更新时变化也就越多；反之，梯度相互抵消的特征，其存储的动量较少，相较于前者，梯度下降的速度差异就体现出来。

### 4.2 Nesterov accelerated gradient

{% asset_img nag.jpg NAG %}

NAG，核心思想是在Momentum步伐过大时减少下一步更新步伐，类似于刹车。其参数更新公式为

$$
v_t = \gamma v_{t-1} + \eta \bigtriangledown_\theta J(\theta - \gamma v_{t-1})
\\
\theta = \theta - v_t
$$

$\gamma$一般设置为0.9或差不多的数值，这里可以理解为在Momentum的基础上，我们设置一个踩刹车的步骤，当前面累计的动量很多导致当前更新步伐很大时，下一步会由于$\theta - \gamma v_{t-1}$产生一个较小的梯度变化，从而实现平稳下降避免速度的太快。实验表明在RNNs上，NAG有着很好的表现。

### 4.3  Adagrad

Adagrad，核心思想是对低频的参数做较大的更新，对高频的做较小的更新，这个算法针对的是学习率，将学习率变成一种动态的自动调节的参数。其参数更新公式为

$$
\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, ii}+\epsilon}} \cdot g_{t,i} 
$$

$t$代表steps，$g_{t,i}$表示第$t$步第$i$个参数的$\bigtriangledown_{\theta_t} J(\theta_{t,i})$，$G_t$是对角线矩阵，每个对角线元素$G_{t, ii}$的值是在时间上累计的所有的$\theta_i$的梯度的平方和，即$G_{t, ii} = \sum^t_{k=0}g_{k, i}^2$，所以随着时间增加，分母越来越大，学习率逐渐下降，且低频的参数做较大的更新，对高频的做较小的更新。

### 4.4  Adadelta

Adadelta，是对Adagrad的进一步优化，由于Adagrad是对整个时间上的累加，必然会导致时间越长学习率逐渐趋向0，Adadelta将这种累计限制在一个区间内，使得学习率下降速度减缓。其参数更新公式为




{% asset_img optimizer1.gif 优化算法在三维空间中随时间推移而变化的可视化效果 %}


{% asset_img optimizer2.gif 优化器 %}