---
title: 深度学习-损失函数
date: 2018-12-10 16:31:02
categories:
- Deep Learning
tags:
- Theory
- Loss
- Tensorflow
mathjax: true
---

参考：

> [Tensorflow API](https://www.tensorflow.org/api_docs/python/tf/losses)
> [损失函数](https://blog.csdn.net/weixin_37136725/article/details/79291818)

## 1. absolute_difference

```
tf.losses.absolute_difference(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

绝对值损失，差距不会被平方缩放。

<!-- more -->

## 2. cosine_distance

```
tf.losses.cosine_distance(
    labels,
    predictions,
    axis=None,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS,
    dim=None
)
```

余弦距离，用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小的度量。

使用前提：labels和predictions已经标准化

$$
\cos <\boldsymbol{x, y}> = \frac{\sum x_iy_i}{|\boldsymbol{x}||\boldsymbol{y}|}
$$

## 3. hinge_loss

```
tf.losses.hinge_loss(
    labels,
    logits,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

铰链损失，常用于SVM中，参考`支持向量机`

## 4. huber_loss

```
tf.losses.huber_loss(
    labels,
    predictions,
    weights=1.0,
    delta=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

Huber loss是为了增强平方误差损失函数对噪声（或叫离群点）的鲁棒性提出的。通过设置`delta`阈值，对阈值以外的误差进行约束，以达到削弱离群值对损失的巨大影响。

$$
loss = 
\left\{\begin{matrix}
0.5 \times x^2, \quad  |x| \leqslant d\\ 
0.5 \times d^2 + d(|x| - d), \quad    |x| > d 
\end{matrix}\right.
\\
x = label - prediction,d = delta
$$

## 5. log_loss

```
tf.losses.log_loss(
    labels,
    predictions,
    weights=1.0,
    epsilon=1e-07,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

对数损失，对每一个样本$(\boldsymbol{x}, y)$来说

$$
loss = y\log(f(\boldsymbol{x}) + \epsilon) - (1- y)\log(1-f(\boldsymbol{x}) + \epsilon)
$$

$\epsilon$是为了防止出现$\log 0$

## 6. mean_pairwise_squared_error

```
tf.losses.mean_pairwise_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES
)
```

与mean_squared_error不同的是，这是predictions和labels的对应元素之间的差异的量度，mean_pairwise_squared_error是predictions和labels对应元素对之间的差异的量度。

如果$labels= [a,b,c]$和$predictions= [x,y,z]$，则将三对差值相加以计算$loss：loss = \frac{1}{3}[ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ]$

由于输入具有形状[batch_size, d0, ... dN]，因此在每个批处理示例中计算相应的对，但不在批次内的样本之间计算。例如，如果predictions表示一批16个维度为[batch_size,100,200]的灰度图像，则会从每个图像中提取一对配对集，而不是跨图像绘制。

## 7. mean_squared_error

```
tf.losses.mean_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

均方误差MSE，显然，误差平方和的均值，若在此基础上做开方运算则得到均方根误差RMSE，常用于回归任务。


## 8. sigmoid_cross_entropy

```
tf.losses.sigmoid_cross_entropy(
    multi_class_labels,
    logits,
    weights=1.0,
    label_smoothing=0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

sigmoid交叉熵，本质上还是调用`tf.nn.sigmoid_cross_entropy_with_logits`，由于有`label_smoothing`的存在，若其不为0，则需要对labels进行平滑处理（整体向$\frac{1}{2}$缩放）；注意这里用的是logits而不是predictions，因为使用sigmoid_cross_entropy的输出层不需要经过sigmoid激活函数，在计算损失sigmoid_cross_entropy的函数内部会先对logits进行sigmoid运算。常用于二分类。

`new_multiclass_labels = multiclass_labels * (1 - label_smoothing) + 0.5 * label_smoothing`



## 9. softmax_cross_entropy

```
tf.losses.softmax_cross_entropy(
    onehot_labels,
    logits,
    weights=1.0,
    label_smoothing=0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

softmax交叉熵，本质上还是调用`tf.nn.softmax_cross_entropy_with_logits_v2`，由于有`label_smoothing`的存在，若其不为0，则需要对labels进行平滑处理（整体向$\frac{1}{numclasses}$缩放），而且label是onehot类型，与logits的形状相同；注意这里用的是logits而不是predictions，logits是未归一化的对数概率，而且使用softmax_cross_entropy的输出层不需要经过softmax激活函数，在计算损失softmax_cross_entropy的函数内部会先对logits进行softmax运算。常用于多分类。

`new_onehot_labels = onehot_labels * (1 - label_smoothing) + label_smoothing / num_classes`


## 10. sparse_softmax_cross_entropy

```
tf.losses.sparse_softmax_cross_entropy(
    labels,
    logits,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

softmax交叉熵，本质上还是调用`tf.nn.sparse_softmax_cross_entropy_with_logits`，注意label是普通的数字类型，范围在$[0, numclasses-1]$；这里用的是logits而不是predictions，logits是未归一化的对数概率，而且使用sparse_softmax_cross_entropy的输出层不需要经过softmax激活函数，在计算损失sparse_softmax_cross_entropy的函数内部会先对logits进行softmax运算。常用于多分类。
