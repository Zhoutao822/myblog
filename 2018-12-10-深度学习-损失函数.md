---
title: 深度学习-损失函数
date: 2018-12-10 16:31:02
categories:
- Deep Learning
tags:
- Theory
- Loss
- Tensorflow
mathjax: true
---

参考：

> [Tensorflow API](https://www.tensorflow.org/api_docs/python/tf/losses)
> [损失函数](https://blog.csdn.net/weixin_37136725/article/details/79291818)
> [常见回归和分类损失函数比较](https://www.cnblogs.com/massquantity/p/8964029.html)

**所有的损失都是reduce_sum处理的，log运算以e为底**

## 1. absolute_difference

```
tf.losses.absolute_difference(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

绝对值损失，差距不会被平方缩放。

$$
loss = |label - prediction|
$$

<!-- more -->

## 2. cosine_distance

```
tf.losses.cosine_distance(
    labels,
    predictions,
    axis=None,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS,
    dim=None
)
```

余弦距离，用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小的度量。

使用前提：labels和predictions已经标准化

$$
\cos <\boldsymbol{x, y}> = \frac{\sum x_iy_i}{|\boldsymbol{x}||\boldsymbol{y}|}
$$

## 3. hinge_loss

```
tf.losses.hinge_loss(
    labels,
    logits,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

铰链损失，常用于SVM中，参考`支持向量机`

## 4. huber_loss

```
tf.losses.huber_loss(
    labels,
    predictions,
    weights=1.0,
    delta=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

Huber loss是为了增强平方误差损失函数对噪声（或叫离群点）的鲁棒性提出的。通过设置`delta`阈值，对阈值以外的误差进行约束，以达到削弱离群值对损失的巨大影响。

$$
loss = 
\left\{\begin{matrix}
0.5 \times x^2, \quad  |x| \leqslant d\\ 
0.5 \times d^2 + d(|x| - d), \quad    |x| > d 
\end{matrix}\right.
\\
x = label - prediction,d = delta
$$

## 5. log_loss

```
tf.losses.log_loss(
    labels,
    predictions,
    weights=1.0,
    epsilon=1e-07,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

对数损失，对每一个样本$(\boldsymbol{x}, y)$来说

$$
loss = y\log(f(\boldsymbol{x}) + \epsilon) - (1- y)\log(1-f(\boldsymbol{x}) + \epsilon)
$$

$\epsilon$是为了防止出现$\log 0$

## 6. mean_pairwise_squared_error

```
tf.losses.mean_pairwise_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES
)
```

与mean_squared_error不同的是，这是predictions和labels的对应元素之间的差异的量度，mean_pairwise_squared_error是predictions和labels对应元素对之间的差异的量度。

如果$labels= [a,b,c]$和$predictions= [x,y,z]$，则将三对差值相加以计算$loss：loss = \frac{1}{3}[ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ]$

由于输入具有形状[batch_size, d0, ... dN]，因此在每个批处理示例中计算相应的对，但不在批次内的样本之间计算。例如，如果predictions表示一批16个维度为[batch_size,100,200]的灰度图像，则会从每个图像中提取一对配对集，而不是跨图像绘制。

## 7. mean_squared_error

```
tf.losses.mean_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

均方误差MSE，显然，误差平方和的均值，若在此基础上做开方运算则得到均方根误差RMSE，常用于回归任务。

$$
loss = (label - prediction)^2
$$

## 8. sigmoid_cross_entropy

```
tf.losses.sigmoid_cross_entropy(
    multi_class_labels,
    logits,
    weights=1.0,
    label_smoothing=0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

sigmoid交叉熵，本质上还是调用`tf.nn.sigmoid_cross_entropy_with_logits`，由于有`label_smoothing`的存在，若其不为0，则需要对labels进行平滑处理（整体向$\frac{1}{2}$缩放）；注意这里用的是logits而不是predictions，因为使用sigmoid_cross_entropy的输出层不需要经过sigmoid激活函数，在计算损失sigmoid_cross_entropy的函数内部会先对logits进行sigmoid运算。常用于二分类。

`new_multiclass_labels = multiclass_labels * (1 - label_smoothing) + 0.5 * label_smoothing`

```
tf.nn.sigmoid_cross_entropy_with_logits(
    _sentinel=None,
    labels=None,
    logits=None,
    name=None
)
```

令`x = logits, z = labels`

$$
z * -\log(sigmoid(x)) + (1 - z) * -\log(1 - sigmoid(x))
\\
= z * -\log(1 / (1 + \exp(-x))) + (1 - z) * -\log(\exp(-x) / (1 + \exp(-x)))
\\
= z * \log(1 + \exp(-x)) + (1 - z) * (-\log(\exp(-x)) + \log(1 + \exp(-x)))
\\
= z * \log(1 + \exp(-x)) + (1 - z) * (x + \log(1 + \exp(-x))
\\
= (1 - z) * x + \log(1 + \exp(-x))
\\
= x - x * z + \log(1 + \exp(-x))
$$

若$x < 0$，为了避免$\exp(-x)$溢出，修正上式为

$$
x - x * z + \log(1 + \exp(-x))
\\
= \log(\exp(x)) - x * z + \log(1 + \exp(-x))
\\
= - x * z + \log(1 + \exp(x))
$$

于是最终使用的公式为

$$
loss = \max(x, 0) - x * z + \log(1 + \exp(-abs(x)))
$$

## 9. softmax_cross_entropy

```
tf.losses.softmax_cross_entropy(
    onehot_labels,
    logits,
    weights=1.0,
    label_smoothing=0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

softmax交叉熵，本质上还是调用`tf.nn.softmax_cross_entropy_with_logits_v2`，由于有`label_smoothing`的存在，若其不为0，则需要对labels进行平滑处理（整体向$\frac{1}{numclasses}$缩放），而且label是onehot类型，与logits的形状相同；注意这里用的是logits而不是predictions，logits是未归一化的对数概率，而且使用softmax_cross_entropy的输出层不需要经过softmax激活函数，在计算损失softmax_cross_entropy的函数内部会先对logits进行softmax运算。常用于多分类。

`new_onehot_labels = onehot_labels * (1 - label_smoothing) + label_smoothing / num_classes`


## 10. sparse_softmax_cross_entropy

```
tf.losses.sparse_softmax_cross_entropy(
    labels,
    logits,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
```

softmax交叉熵，本质上还是调用`tf.nn.sparse_softmax_cross_entropy_with_logits`，注意label是普通的数字类型，范围在$[0, numclasses-1]$；这里用的是logits而不是predictions，logits是未归一化的对数概率，而且使用sparse_softmax_cross_entropy的输出层不需要经过softmax激活函数，在计算损失sparse_softmax_cross_entropy的函数内部会先对logits进行softmax运算。常用于多分类。

## 11. 常见回归和分类损失函数

这一部分仅考虑损失函数形式$L(y, f(\boldsymbol{x}))$

### 11.1 回归问题

* 平方损失：$(y - f(\boldsymbol{x}))^2$
* 绝对值损失：$|y - f(\boldsymbol{x})|$
* Huber损失：参考`4. huber_loss`

其中最常用的是平方损失，然而其缺点是对于异常点会施以较大的惩罚，因而不够robust。如果有较多异常点，则绝对值损失表现较好，但绝对值损失的缺点是在$y−f(\boldsymbol{x})=0$处不连续可导，因而不容易优化。

Huber损失是对二者的综合，当$|y−f(\boldsymbol{x})|$小于一个事先指定的值$delta$时，变为平方损失，大于$delta$时，则变成类似于绝对值损失，因此也是比较robust的损失函数。

{% asset_img 0.png %}

### 11.2 分类问题

对于二分类问题，$y \in \{−1,+1\}$，损失函数常表示为关于$yf(\boldsymbol{x})$的单调递减形式。$yf(\boldsymbol{x})$ 被称为$margin$，其作用类似于回归问题中的残差$y−f(\boldsymbol{x})$。

* 0-1损失

$$
L(y,f(\boldsymbol{x})) = \left\{\begin{matrix} 0 \qquad if \;\; yf(\boldsymbol{x})\geq0 \\ 1 \qquad if \;\; yf(\boldsymbol{x}) < 0\end{matrix}\right.
$$

0-1损失对每个错分类点都施以相同的惩罚，这样那些“错的离谱”的点并不会收到大的关注，这在直觉上不是很合适。另外0-1损失不连续、非凸，优化困难，因而常使用其他的代理损失函数进行优化。

* Logistic损失

$$
L(y,f(\boldsymbol{x})) = \log(1+e^{-yf(\boldsymbol{x})})
$$

利用极大似然法可证明最小化Logistic损失等价于最大化准确率，在二分类问题中logistic loss和交叉熵损失是等价的，二者区别只是标签$y$的定义不同。

* Hinge损失

$$
L(y,f(\boldsymbol{x})) = \max(0,1-yf(\boldsymbol{x}))
$$

参考`支持向量机`

* 指数损失

$$
L(y,f(\boldsymbol{x})) = e^{-yf(\boldsymbol{x})}
$$

指数损失Exponential loss为AdaBoost中使用的损失函数，使用exponential loss能比较方便地利用加法模型推导出AdaBoost算法 (具体推导过程)。然而其和squared loss一样，对异常点敏感，不够robust。

* Modified Huber损失

$$
L(y,f(\boldsymbol{x})) = \left \{\begin{matrix} max(0,1-yf(\boldsymbol{x}))^2 \qquad if \;\;yf(\boldsymbol{x})\geq-1 \\ \qquad-4yf(\boldsymbol{x}) \qquad\qquad\;\; if\;\; yf(\boldsymbol{x})<-1\end{matrix}\right.\qquad
$$

modified huber loss结合了hinge loss和logistic loss的优点，既能在$yf(\boldsymbol{x})>1$时产生稀疏解提高训练效率，又能进行概率估计。另外其对于$(yf(\boldsymbol{x})<−1)$ 样本的惩罚以线性增加，这意味着受异常点的干扰较少，比较robust。

{% asset_img 1.png %}

从上图可以看出上面介绍的这些损失函数都可以看作是0-1损失的单调连续近似函数，而因为这些损失函数通常是凸的连续函数，因此常用来代替0-1损失进行优化。它们的相同点是都随着$margin \rightarrow - \infty$而加大惩罚；不同点在于，logistic loss和hinge loss都是线性增长，而exponential loss是以指数增长。

值得注意的是上图中modified huber loss的走向和exponential loss差不多，并不能看出其robust的属性。其实这和算法时间复杂度一样，成倍放大了之后才能体现出巨大差异：

{% asset_img 2.png %}
