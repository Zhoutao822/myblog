---
title: 深度学习-激活函数
date: 2018-12-10 16:30:02
categories:
- Deep Learning
tags:
- Theory
- Activation
- Tensorflow
mathjax: true
---

参考：

> [Tensorflow API](https://tensorflow.google.cn/api_docs/python/tf/keras/activations)
> [13种神经网络激活函数](http://m.elecfans.com/article/678118.html)
> [线性模型和非线性模型的区别，以及激活函数的作用](https://www.cnblogs.com/toone/p/8574294.html)

**为什么神经网络可以拟合非线性模型，什么是非线性模型，与线性模型的区别是什么？**

1. 线性模型不一定是一条直线（logistic），但是分类的决策边界一定是一条直线，参考`线性模型`；
2. 区分线性模型与非线性模型，主要还是看自变量受到几个参数的影响，如果自变量被两个或以上参数影响，那么此模型就是非线性模型；
3. 参考广义线性模型，一个简单的神经网络的输出与输入的关系$\boldsymbol{y} = g_1^{-1}(\boldsymbol{w_1}^Tg_2^{-1}(\boldsymbol{w_2}^T\boldsymbol{x} + b_2) + b_1)$，对于任意激活函数$\boldsymbol{g}^{-1}$来说，自变量或输入$\boldsymbol{x}$可能受到至少两个参数（来自$\boldsymbol{w_1,w_2}$）的影响，所以神经网络可以非线性的模型；
4. 激活函数的选择，能对输入进行约束，求导方便。

<!-- more -->

## 1. linear

```
tf.keras.activations.linear(x)
```

线性激活函数，输出和输入成比例。线性激活函数的问题在于，它的导数是常数，梯度也是常数，梯度下降无法工作。

$$
f(x) = x
$$

{% asset_img linear.png %}

## 2. elu

```
tf.keras.activations.elu(
    x,
    alpha=1.0
)
```

ELU（Exponential Linear Unit，指数线性单元）尝试加快学习速度。基于ELU，有可能得到比ReLU更高的分类精确度。这里$\alpha$是一个超参数（限制：$\alpha \geqslant 0$）。

$$
f(x) = 
\left\{\begin{matrix}
\alpha(e^x-1), \quad  x < 0\\ 
x, \quad    x \geqslant 0 
\end{matrix}\right.
$$

{% asset_img elu.png %}

## 3. hard_sigmoid

```
tf.keras.activations.hard_sigmoid(x)
```

比sigmoid函数计算更快

$$
f(x) = 
\left\{\begin{matrix}
0, \quad  x < -2.5\\ 
0.2 \times x + 0.5, \quad  -2.5 \leqslant x \leqslant 2.5\\
1, \quad    x > 2.5 
\end{matrix}\right.
$$

## 4. relu

```
tf.keras.activations.relu(
    x,
    alpha=0.0,
    max_value=None,
    threshold=0
)
```

整流线性函数，默认情况$\max(x, 0)$，训练速度比tanh快6倍。当输入值小于零时，输出值为零。当输入值大于等于零时，输出值等于输入值。当输入值为正数时，导数为1，因此不会出现sigmoid函数反向传播时的挤压效应。

$$
f(x) = 
\left\{\begin{matrix}
max\_value, \quad  x \geqslant max\_value\\ 
x, \quad  threshold \leqslant x < max\_value \\
alpha \times (x - threshold), \quad otherwise
\end{matrix}\right.
$$

默认情况导数

$$
f'(x) = 
\left\{\begin{matrix}
0, \quad  x < 0\\ 
1, \quad    x \geqslant 0 
\end{matrix}\right.
$$

不幸的是，ReLU在训练时可能很脆弱，可能“死亡”。例如，通过ReLU神经元的较大梯度可能导致权重更新过头，导致神经元再也不会因为任何数据点激活。如果这一情况发生了，经过这一单元的梯度从此以后将永远为零。也就是说，ReLU单元可能在训练中不可逆地死亡，因为它们被从数据流形上踢出去了。例如，你可能发现，如果学习率设置过高，40%的网络可能“死亡”（即神经元在整个训练数据集上永远不会激活）。设置一个合适的学习率可以缓解这一问题。

{% asset_img relu.png %}

### 4.1 Leaky ReLU函数

Leaky ReLU让单元未激活时能有一个很小的非零梯度。这里，很小的非零梯度是0.01。

$$
f(x) = 
\left\{\begin{matrix}
0.01x, \quad  x < 0\\ 
x, \quad    x \geqslant 0 
\end{matrix}\right.
$$

导数

$$
f'(x) = 
\left\{\begin{matrix}
0.01, \quad  x < 0\\ 
1, \quad    x \geqslant 0 
\end{matrix}\right.
$$

{% asset_img leaky.png %}

### 4.2 PReLU函数

PReLU（Parametric Rectified Linear Unit）函数类似Leaky ReLU，只不过将系数（很小的非零梯度）作为激活函数的参数，该参数和网络的其他参数一样，在训练过程中学习。

$$
f(x) = 
\left\{\begin{matrix}
\alpha x, \quad  x < 0\\ 
x, \quad    x \geqslant 0 
\end{matrix}\right.
$$

### 4.3 RReLU函数

RReLU也类似Leaky ReLU，只不过系数（较小的非零梯度）在训练中取一定范围内的随机值，在测试时固定。

$$
f(x_{ij}) =  
\left\{\begin{matrix}
\alpha_{ij} x_{ij}, \quad  x < 0\\ 
x_{ij}, \quad    x \geqslant 0 
\end{matrix}\right.
$$
{% asset_img rand.png %}

### 4.4 SReLU函数

SReLU（S-shaped Rectified Linear Activation Unit，S型修正线性激活单元）由三个分段线性函数组成。系数$t_l, a_l, t_r, a_r$作为参数，将在网络训练中学习。

$$
f_{t_l, a_l, t_r, a_r}(x) = 
\left\{\begin{matrix}
t_l + a_l(x - t_l), \quad  x \leqslant t_l\\ 
x \quad t_l < x < t_r \\
t_r + a_r(x - t_r), \quad    x \geqslant t_r
\end{matrix}\right.
$$

{% asset_img srelu.png %}

## 5. selu

```
tf.keras.activations.selu(x)
```

SELU（Scaled Exponential Linear Unit，拉伸指数线性单元）是ELU经过拉伸的版本。

$$
f(x) = scale \times elu(x, alpha)
\\
alpha = 1.6732632423543772848170429916717
\\
scale = 1.0507009873554804934193349852946
$$

## 6. sigmoid

```
tf.keras.activations.sigmoid(x)
```

sigmoid激活函数，常用于二分类任务的输出层。它有梯度消失问题。在一定epoch数目之后，网络拒绝学习，或非常缓慢地学习，因为输入$x$导致输出$y$中非常小的改动，计算sigmoid函数的导数非常简单。

就神经网络的反向传播过程而言，每层（至少）挤入四分之一的误差。因此，网络越深，越多关于数据的知识将“丢失”。某些输出层的“较大”误差可能不会影响相对较浅的层中的神经元的突触权重（“较浅”意味着接近输入层）。

$$
f(x) = \frac{1}{1 + e^{-x}}
$$

导数为

$$
f'(x) = f(x)(1-f(x))
$$

{% asset_img sigmoid.png %}

## 7. softmax

```
tf.keras.activations.softmax(
    x,
    axis=-1
)
```

softmax函数将原始值转换为后验分布，可用于衡量确定性。类似sigmoid，softmax将每个单元的输出值挤压到0和1之间。不过，softmax同时确保输出的总和等于1。

$$
\sigma(z)_j = \frac{e^{z_j}}{\sum^K_{k=1}e^{z_k}}
$$

交叉熵对参数的偏导数为

$$
\frac{\partial L}{\partial z_i} = \sigma(z)_i - y_i
$$

{% asset_img softmax.png %}

## 8. softplus

```
tf.keras.activations.softplus(x)
```

SoftPlus函数的导数为逻辑（logistic）函数。大体上，ReLU和SoftPlus很相似，只不过SoftPlus在接近零处平滑可微。另外，计算ReLU及其导数要比SoftPlus容易很多。

$$
f(x) = \ln (1+e^x)
$$

导数为

$$
f'(x) = \frac{1}{1+e^{-x}}
$$

## 9. softsign

```
tf.keras.activations.softsign(x)
```

平滑的sign函数，求导更方便。

$$
f(x) = \frac{x}{|x| + 1}
$$

## 10. tanh

```
tf.keras.activations.tanh(x)
```

tanh函数是拉伸过的sigmoid函数，以零为中心，因此导数更陡峭。tanh比sigmoid激活函数收敛得更快。

$$
f(x) = \tanh(x) = \frac{2}{1+e^{-2x}} - 1
$$

导数

$$
f'(x) = 1 - (f(x))^2
$$

{% asset_img tanh.png %}

## 11. 阶跃函数

通常只在单层感知器上有用，单层感知器是神经网络的早期形式，可用于分类线性可分的数据。这些函数可用于二元分类任务。

$$
f(x) = 
\left\{\begin{matrix}
0, \quad  x < 0\\ 
1, \quad  x \geqslant 0
\end{matrix}\right.
$$

{% asset_img jieyue.png %}

## 12. APL函数

APL（Adaptive Piecewise Linear，自适应分段线性）函数

$$
f(x) = \max(x, 0) + \sum^S_{s=1}a_i^s\max(0, -x+b^s_i)
$$

{% asset_img apl.png %}

选择激活函数时，优先选择ReLU及其变体，而不是sigmoid或tanh。同时ReLU及其变体训练起来更快。如果ReLU导致神经元死亡，使用Leaky ReLU或者ReLU的其他变体。sigmoid和tanh受到消失梯度问题的困扰，不应该在隐藏层中使用。隐藏层使用ReLU及其变体较好。使用容易求导和训练的激活函数。