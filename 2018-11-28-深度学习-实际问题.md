---
title: 深度学习-实际问题
date: 2018-11-28 13:53:04
categories:
- Deep Learning
tags:
- Theory
- Batch Normalization
- Early Stop
- Dropout
- Decay
mathjax: true
---

参考：

>《解析深度学习-语音识别实践》第4章 深度神经网络

## 1. 数据预处理

样本特征归一化和全局特征标准化

1. 如果每个样本的均值的变化与处理的问题无关，就应当将特征均值归零，减小特征相对于深度神经网络模型的变化。

2. 全局特征标准化的目标是使用全局转换缩放每维数据，使得最终的特征向量处于相似的动态范围内。通常做法是减去当前特征的均值，再除以当前特征标准差。

3. 批量标准化是在一个batch上进行特征标准化。

标准化的方式很多，对于稀疏矩阵，我们希望在标准化的同时保持矩阵稀疏性，这时的计算方式可能需要改变。具体参考[sklearn-preprocessing](http://sklearn.apachecn.org/cn/0.19.0/modules/preprocessing.html#preprocessing)

<!-- more -->

## 2. 模型初始化

模型参数初始值需要足够小才能使sigmoid函数输出值在$(0,1)$，而不是接近0或1，那样会导致梯度很小，收敛很慢。

推荐参数$b$初始为0，$w$初始值属于高斯分布$N(0, \sigma^2_{w^{l+1}})$，$\sigma_{w^{l+1}} = \frac{1}{\sqrt{N_l}}$，其中$N_l$为与权重连接的输出节点的个数。

## 3. 权重衰减

基于$L_1$范数和$L_2$范数的正则化，当DNN模型有超过1百万的参数，插值系数$\lambda$应该很小（通常在$10^{-4}$范围），当训练数据量较大时设置为0.

## 4. 丢弃法

dropout基本思想时在训练过程中随机丢弃每一个隐层中一定比例（丢弃比例$\alpha$）的神经元。当一个隐层神经元被丢弃时，它的激活值被设置为0。分析一下流程就知道，在测试阶段只需要在与dropout训练有关的所有权重上乘以$(1-\alpha)$就可以直接使用。

经验表明，在dropout比例为0.1 ~ 0.2时，识别率会有提升。与此同时，若将初始dropout比例设置为0.5，在训练过程中渐渐减小dropout比例会有更好的效果。

## 5. 批量块大小的选择

参考[`深度学习-优化器`](http://zhoutao822.coding.me/2018/11/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E5%99%A8/)。

在语音识别任务中，早期使用64到256个样本大小，后期换用1024到8096的样本大小，可以学习到一个更好的模型。

## 6. 取样随机化

参考[`深度学习-优化器`](http://zhoutao822.coding.me/2018/11/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E5%99%A8/)。

在语音识别领域中，采用滚动窗的方式每次加载一大块数据（通常为24 ~ 48小时的语音或者8.6M到17.2M个样本）进内存，然后再窗内随机取样。

## 7. 惯性系数

参考[`深度学习-优化器`](http://zhoutao822.coding.me/2018/11/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E5%99%A8/)。

类似Adadelta中的参数$\gamma$，通常取值为$0.9$ ~ $0.99^4$。定义在相同层面下惯性系数为$\rho_s$，若考虑批量块大小$M_b$条件下的惯性系数

$$
\rho = \exp(M_b\rho_s)
$$

## 8. 学习率和停止准则

参考[`深度学习-优化器`](http://zhoutao822.coding.me/2018/11/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E5%99%A8/)。

在语音识别训练任务中，可以定义每一帧的学习率（$M_b$为批量大小，动态）为

$$
\epsilon_s = \frac{\epsilon}{M_b}
$$

根据经验的学习策略为

* 首先确定批量大小以及一个大的学习率，并在此基础上训练数百个小批量数据组；
* 观察训练准则变化，然后减少批量中的数据数目、学习率或者同时减小，以使$\epsilon_sM_b$结果减半，直到训练准则获得明显改善；
* 然后学习率减半作为下一个完整数据迭代轮次的初始学习率，运行一个较大的训练数据子集，把$\epsilon_sM_b$的值增大四到八倍；
* 此时，增大$\epsilon_sM_b$并不会导致发散，而会提高训练速度。

对于从头开始训练的语音识别任务，$\epsilon_s$对深层和浅层网络分别取值$0.8e^{-4}$和$0.3e^{-3}$，在第二阶段取值$1.25e^{-2}$，在第三阶段取值$0.8e^{-6}$，效果很好。

## 9. 网络结构

一般来说，宽且浅的模型容易过拟合，深且窄的模型容易欠拟合。比较好的策略是先在只有一个隐层的神经网络上优化每层的节点个数，然后再叠加更多有相同节点个数的隐层。
在语音识别任务中，拥有5 ~ 7层，每层拥有1000 ~ 3000个节点的DNN效果很好。

## 10. 可复现性与可重启性

多次实验，每次用一个新的随机种子并记录平均结果以及标准的误差。

当训练集很大时，通常需要在中间时间停止训练并在最后的检查点继续训练。这时需要在检查点中保存模型参数、当前随机数、参数梯度、惯性系数等。

