---
title: 循环神经网络
date: 2018-11-29 09:16:15
categories:
- Deep Learning
tags:
- Theory
- RNN
mathjax: true
---

参考：

> [《深度学习》第10章 序列建模：循环和递归网络](https://github.com/exacity/deeplearningbook-chinese)

循环神经网络或RNN是一类用于处理序列数据的神经网络。正如卷积网络可以很容易地扩展到具有很大宽度和高度的图像，以及处理大小可变的图像，循环网络可以扩展到更长的序列（比不基于序列的特化网络长得多）。大多数循环网络也能处理可变长度的序列。

卷积操作允许网络跨时间共享参数，但是浅层的。卷积的输出是一个序列，其中输出中的每一项是相邻几项输入的函数。参数共享的概念体现在每个时间步中使用的相同卷积核。循环神经网络以不同的方式共享参数。输出的每一项是前一项的函数。输出的每一项对先前的输出应用相同的更新规则而产生。这种循环方式导致参数通过很深的计算图共享。

{% asset_img rnn0.png %}

## 1. 展开计算图

计算图是形式化一组计算结构的方式，如那些涉及将输入和参数映射到输出和损失的计算。

展开（unfolding）递归或循环计算得到的重复结构进行解释，这些重复结构通常对应于一个事件链。 展开（unfolding）这个计算图将导致深度网络结构中的参数共享。

使用$\boldsymbol{h}$表示网络的隐藏单元

$$
\boldsymbol{h}^{(t)} = f(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)};\boldsymbol{\theta})
$$

我们需要将任意长的序列$(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)},...,\boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)})$到以固定长度的向量$\boldsymbol{h}^{(t)}$。根据实际情况，我们往往并不需要保存全部序列的信息，所以仅仅存储足够预测句子其余部分的信息。最苛刻的情况是要求$\boldsymbol{h}^{(t)}$足够风骨，并能大致恢复输入序列。

展开图的大小取决于序列长度。用一个函数$g^{(t)}$代表经$t$步展开后的循环：

$$
\boldsymbol{h}^{(t)} = g^{(t)}(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)},...,\boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)})
\\
= f(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)};\boldsymbol{\theta})
$$

函数$g^{(t)}$将全部过去序列作为输入来生成当前状态，但是展开的循环架构允许我们将$g^{(t)}$分解为函数$f$的重复应用。因此，展开过程引入两个主要优点：

1. 无论序列的长度，学成的模型始终具有相同的输入大小，因为它指定的是从一种状态到另一种状态的转移，而不是在可变长度的历史状态上操作。
2. 我们可以在每个时间步使用相同参数的相同转移函数$f$。

这两个因素使得学习在所有时间步和所有序列长度上操作单一的模型$f$是可能的，而不需要在所有可能时间步学习独立的模型$g^{(t)}$。学习单一的共享模型允许泛化到没有见过的序列长度（没有出现在训练集中），并且估计模型所需的训练样本远远少于不带参数共享的模型。

## 2. 循环神经网络

{% asset_img rnn1.png %}

从图中可知，循环神经网络使用了三个权重矩阵$\boldsymbol{U, W, V}$，矩阵$\boldsymbol{U}$对输入进行处理，矩阵$\boldsymbol{W}$对状态转移进行处理，矩阵$\boldsymbol{V}$对输出进行处理；非常类似于马尔可夫链。

循环神经网络的设计模式包括以下几种：

1. 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络；
2. 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络；
3. 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络。

### 2.1 前向传播

假设：

* 隐藏单元的激活函数未双曲正切函数tanh；
* 输出$\boldsymbol{o}$是离散变量可能值的非标准化对数概率，如用于预测词或字符的onehot形式；
* 预测值$\hat{\boldsymbol{y}}$是对输出进行softmax处理后得到的标准化向量；
* RNN从特定的初始状态$\boldsymbol{h}^{(0)}$开始前向传播。

则从$t=1$到$t=\tau$的每个时间步，更新方程为

$$
\boldsymbol{a}^{(t)} = \boldsymbol{b} + \boldsymbol{W}\boldsymbol{h}^{(t-1)}  + \boldsymbol{U}\boldsymbol{x}^{(t)}
\\
\boldsymbol{h}^{(t)} = \tanh(\boldsymbol{a}^{(t)})
\\
\boldsymbol{o}^{(t)} = \boldsymbol{c} + \boldsymbol{V}\boldsymbol{h}^{(t)}
\\
\hat{\boldsymbol{y}}^{(t)} = softmax(\boldsymbol{o}^{(t)})
$$

偏置向量$\boldsymbol{b, c}$分别对应于输入到隐藏和隐藏到输出的连接。我们将一个输入序列映射到相同长度的输出序列。总损失就是所有时间步的损失之和。例如$L^{(t)}$为给定的$\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(t)}$后$\boldsymbol{y}^{(t)}$的负对数似然，则

$$
L(\{ \boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(\tau)} \}, \{ \boldsymbol{y}^{(1)},...,\boldsymbol{y}^{(\tau)} \})
\\
= \sum_t L^{(t)}
\\
= - \sum_t \log p_{model}(\boldsymbol{y}^{(t)}|\{ \boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(t)} \})
$$

关于各个参数计算这个损失函数的梯度是计算成本很高的操作。梯度计算涉及执行一次前向传播，接着是由右到左的反向传播。运行时间是$O(\tau)$，并且不能通过并行化来降低，因为前向传播图是固有循序的；每个时间步只能一前一后地计算。前向传播中的各个状态必须保存，直到它们反向传播中被再次使用，因此内存代价也是$O(\tau)$。应用于展开图且代价为$O(\tau)$的反向传播算法称为**通过时间反向传播BPTT**。

### 2.2 导师驱动过程和输出循环网络

{% asset_img rnn2.png %}

这个网络在一个时间步的输出和下一个时间步的隐藏单元间存在循环网络，这就导致了丢失了隐藏单元到隐藏单元间对于序列变化的信息。但是优点在于基于训练集提供的输出的理想值，我们没有必要计算前一时刻的输出，所以我们可以并行计算所有时间步的梯度。

由输出反馈到模型而产生循环连接的模型可用**导师驱动过程**进行训练。训练模型时，导师驱动过程不再使用最大似然准则，而在时刻$t+1$接收真实值$y^{(t)}$作为输入。我们可以通过检查两个时间步的序列得知这一点。条件最大似然准则是

$$
\log p(\boldsymbol{y}^{(1)},\boldsymbol{y}^{(2)}|\boldsymbol{x}^{(1)},\boldsymbol{x}^{(2)})
\\
\log p(\boldsymbol{y}^{(2)}|\boldsymbol{y}^{(1)},\boldsymbol{x}^{(1)},\boldsymbol{x}^{(2)}) + \log p(\boldsymbol{y}^{(1)}|\boldsymbol{x}^{(1)},\boldsymbol{x}^{(2)})
$$

{% asset_img rnn3.png %}

我们使用导师驱动过程的最初动机是为了在缺乏隐藏到隐藏连接的模型中避免通过时间反向传播。只要模型一个时间步的输出与下一时间步计算的值存在连接，导师驱动过程仍然可以应用到这些存在隐藏到隐藏连接的模型。然而，只要隐藏单元成为较早时间步的函数，BPTT算法是必要的。因此训练某些模型时要同时使用导师驱动过程和BPTT。

如果之后网络在开环 (open-loop) 模式下使用，即网络输出（或输出分布的样本）反馈作为输入，那么完全使用导师驱动过程进行训练的缺点就会出现。如上图所示，训练过程中的反馈与实际部署时的反馈是不同的。

在这种情况下，训练期间该网络看到的输入与测试时看到的会有很大的不同。减轻此问题的一种方法是同时使用导师驱动过程和自由运行的输入进行训练，例如在展开循环的输出到输入路径上预测几个步骤的正确目标值。

通过这种方式，网络可以学会考虑在训练时没有接触到的输入条件（如自由运行模式下，自身生成自身），以及将状态映射回使网络几步之后生成正确输出的状态。另外一种方式是通过随意选择生成值或真实的数据值作为输入以减小训练时和测试时看到的输入之间的差别。这种方法利用了课程学习策略，逐步使用更多生成值作为输入。

### 2.3 计算循环神经网络的梯度

回顾一下前向传播的公式

$$
\boldsymbol{a}^{(t)} = \boldsymbol{b} + \boldsymbol{W}\boldsymbol{h}^{(t-1)}  + \boldsymbol{U}\boldsymbol{x}^{(t)}
\\
\boldsymbol{h}^{(t)} = \tanh(\boldsymbol{a}^{(t)})
\\
\boldsymbol{o}^{(t)} = \boldsymbol{c} + \boldsymbol{V}\boldsymbol{h}^{(t)}
\\
\hat{\boldsymbol{y}}^{(t)} = softmax(\boldsymbol{o}^{(t)})
$$

我们做出的假设是：真实值和输入值都是onehot变量（对应于字或者词的字典模型），预测值是形状相同的概率输出，考虑onehot的每一位$i$，损失$L^{(t)}$为

$$
L^{(t)} = -\sum_iy_i^{(t)}\log \hat{y}^{(t)}_i
$$

再考虑整体损失

$$
L(\{ \boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(\tau)} \}, \{ \boldsymbol{y}^{(1)},...,\boldsymbol{y}^{(\tau)} \})
\\
= \sum_t L^{(t)}
$$

可以得到

$$
\frac{\partial L}{\partial L^{(t)}} = 1
\\
(\bigtriangledown_{\boldsymbol{o}^{(t)}}L)_i = \frac{\partial L}{\partial o^{(t)}_i} = \frac{\partial L}{\partial L^{(t)}}\frac{\partial L^{(t)}}{\partial o_i^{(t)}} = \hat{y}^{(t)}_i - \boldsymbol{1}_{i, y^{(t)}}
$$

上面的式子其实并不复杂，对于onehot变量来说，$\boldsymbol{y}^{(t)}$中只有一位为1，其余位都为0，也就是说，对于$\boldsymbol{o}^{(t)}$来说，梯度简单到就是预测值与真实值的差值。具体证明参考`深度学习-激活函数`。

基于上式，我们可以对最后的时间步$\tau$的$\boldsymbol{h}^{(\tau)}$进行求导计算

$$
\bigtriangledown_{\boldsymbol{h}^{(\tau)}}L = \boldsymbol{V}^T\bigtriangledown_{\boldsymbol{o}^{(\tau)}}L
$$

然后，计算从时刻$t= \tau - 1$到$t=1$反向迭代，通过时间反向传播梯度，由于$\boldsymbol{h}^{(t)}(t < \tau)$同时存在于$\boldsymbol{o}^{(t)}$和$\boldsymbol{h}^{(t+1)}$中，因此梯度计算需要考虑两个部分

$$
\bigtriangledown_{\boldsymbol{h}^{(t)}}L=(\frac{\partial \boldsymbol{h}^{(t+1)}}{\partial \boldsymbol{h}^{(t)}})^T(\bigtriangledown_{\boldsymbol{h}^{(t+1)}}L) + (\frac{\partial \boldsymbol{o}^{(t)}}{\partial \boldsymbol{h}^{(t)}})^T(\bigtriangledown_{\boldsymbol{o}^{(t)}}L)
\\
= \boldsymbol{W}^T(\bigtriangledown_{\boldsymbol{h}^{(t+1)}}L)diag(1-(\boldsymbol{h}^{(t+1)})^2) + \boldsymbol{V}^T(\bigtriangledown_{\boldsymbol{o}^{(t)}}L)
$$

$\tanh$求导参考`深度学习-激活函数`，其中$diag(1-(\boldsymbol{h}^{(t+1)})^2)$表示包含元素$1-(h_i^{(t+1)})^2$的对角矩阵。

其他参数是时间步共享的，那么需要使用$t$时刻的虚拟变量$\boldsymbol{W}^{(t)}$作为$\boldsymbol{W}$的副本，然后计算在时间步$t$权重对梯度的贡献。

$$
\bigtriangledown_\boldsymbol{c}L = \sum_t(\frac{\partial \boldsymbol{o}^{(t)}}{\partial \boldsymbol{c}})^T\bigtriangledown_{\boldsymbol{o}^{(t)}}L = \sum_t\bigtriangledown_{\boldsymbol{o}^{(t)}}L
\\
\bigtriangledown_\boldsymbol{b}L = \sum_t(\frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{b}})^T\bigtriangledown_{\boldsymbol{h}^{(t)}}L = \sum_t diag(1-(\boldsymbol{h}^{(t)})^2) \bigtriangledown_{\boldsymbol{h}^{(t)}}L
\\
\bigtriangledown_\boldsymbol{V}L = \sum_t\sum_i(\frac{\partial L}{\partial o_i^{(t)}})\bigtriangledown_{\boldsymbol{V}^{(t)}}o_i^{(t)} = \sum_t (\bigtriangledown_{\boldsymbol{o}^{(t)}}L) \boldsymbol{h}^{(t)^T}
\\
\bigtriangledown_\boldsymbol{W}L = \sum_t\sum_i(\frac{\partial L}{\partial h_i^{(t)}})\bigtriangledown_{\boldsymbol{W}^{(t)}}h_i^{(t)}
= \sum_t diag(1-(\boldsymbol{h}^{(t)})^2) (\bigtriangledown_{\boldsymbol{h}^{(t)}}L)\boldsymbol{h}^{(t-1)^T}
\\
\bigtriangledown_\boldsymbol{U}L = \sum_t\sum_i(\frac{\partial L}{\partial h_i^{(t)}})\bigtriangledown_{\boldsymbol{U}^{(t)}}h_i^{(t)}
= \sum_t diag(1-(\boldsymbol{h}^{(t)})^2) (\bigtriangledown_{\boldsymbol{h}^{(t)}}L)\boldsymbol{x}^{(t)^T}
$$

### 2.4 作为有向图模型的循环网络

本节描述没有输入$\boldsymbol{x}$的情况下，RNN如何对应于有向图模型。

与前馈网络类似，原则上循环网络几乎可以使用任何损失。但必须根据任务来选择损失。如前馈网络，我们通常希望将RNN的输出解释为一个概率分布，并且我们通常使用与分布相关联的交叉熵来定义损失。均方误差是与单位高斯分布的输出相关联的交叉熵损失，例如前馈网络中所使用的。

若不考虑输入$x$，仅考虑输出$y^{(t)}$与之前序列的关系，我们可以得到一个全连接图模型，这种情况下，计算的是链式条件概率，比较类似马尔可夫链

{% asset_img rnn4.png %}

若考虑引入隐藏的状态变量，则可以利用$\boldsymbol{h}$保存历史的信息，由于参数共享，RNN 的参数数目为$O(1)$且是序列长度的函数。我们可以调节RNN的参数数量来控制模型容量，但不用被迫与序列长度成比例。

{% asset_img rnn5.png %}

循环网络为减少的参数数目付出的代价是优化参数可能变得困难。

在循环网络中使用的参数共享的前提是相同参数可用于不同时间步的假设。也就是说，假设给定时刻$t$的变量后，时刻$t + 1$变量的条件概率分布是平稳的，这意味着之前的时间步与下个时间步之间的关系并不依赖于$t$。虽然$t$可以用作额外的输入。

为了完整描述将RNN作为图模型的观点，我们必须描述如何从模型采样。我们需要执行的主要操作是简单地从每一时间步的条件分布采样。

1. 在训练样本$\boldsymbol{x}^{(\tau)}$后增加一个特殊符号，当产生该符号时，停止训练；
2. 模型中引入一个额外的伯努利输出，表示在每个时间步决定继续生成或停止生成。新的输出单元通常使用 sigmoid 单元，并通过交叉熵训练。在这种方法中，sigmoid 被训练为最大化正确预测的对数似然，即在每个时间步序列决定结束或继续；
3. 将一个额外的输出添加到模型并预测整数$\tau$本身。模型可以采出$\tau$的值，然后采$\tau$步有价值的数据。这种方法需要在每个时间步的循环更新中增加一个额外输入，使得循环更新知道它是否是靠近所产生序列的末尾。这种额外的输入可以是$\tau$的值，也可以是$\tau - t$即剩下时间步的数量。

### 2.5 基于上下文的RNN序列建模

本节描述输入$\boldsymbol{x}$为单个向量的情况下，RNN的模型。

那么我们可以将$\boldsymbol{x}$看作产生$\boldsymbol{y}$序列RNN的额外输入。常见做法是：

1. 在每个时刻作为一个额外输入，即每个时刻都输入相同的$\boldsymbol{x}$；
2. 作为初始状态，即根据输入$\boldsymbol{x}$预测后面所有的序列；
3. 结合两种方式。

{% asset_img rnn6.png %}

对于1，输入到隐藏向量之间通过矩阵$\boldsymbol{R}$参数化，这类RNN适用于很多任务如图注，其中单个图像作为模型的输入，然后产生描述图像的词序列。

{% asset_img rnn7.png %}

将可变长度的$\boldsymbol{x}$值序列映射到相同长度的$\boldsymbol{y}$值序列上分布的条件循环神经网络。

## 3. 双向RNN

目前为止考虑的RNN有一种因果结构，即时刻$t$的状态只能从过去的序列$\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(t-1)}$以及当前输入$\boldsymbol{x}^{(t)}$捕获。或者在$\boldsymbol{y}$可用的情况下允许过去的$\boldsymbol{y}$影响当前状态的模型。

在许多应用中，输出$\boldsymbol{y}^{(t)}$的预测可能依赖于整个输入序列。可能需要从更远的未来寻找信息区分它们。比如语音识别，音素的判断。

双向RNN就是这样被发明了。双向RNN结合时间上从序列起点开始移动的RNN和另一个时间上从序列末尾开始移动的RNN。其中$\boldsymbol{h}^{(t)}$代表通过时间向前移动的子RNN的状态，$\boldsymbol{g}^{(t)}$代表通过时间向后移动的子RNN的状态。这允许输出单元$\boldsymbol{o}^{(t)}$能够计算同时依赖于过去和未来且对时刻$t$的输入值最敏感的表示，而不必指定$t$周围固定大小的窗口（这是前馈网络、卷积网络或具有固定大小的先行缓存器的常规RNN所必须要做的）。

{% asset_img rnn8.png %}

扩展这个想法到2维输入，如图象，由4个RNN组成，沿四个方向：上下左右。实际上，对于这样的RNN，前向传播公式可以写成表示使用卷积的形式，计算自底向上到每一层的输入（在整合横向相互作用的特征图的循环传播之前）。

### 4. 基于编码-解码的序列到序列架构

目的将可变长度序列映射到另一可变长度序列。

我们经常将RNN的输入称为“上下文”。我们希望产生此上下文的表示$C$。这
个上下文$C$可能是一个概括输入序列$\boldsymbol{X} = (\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(n_x)})$的向量或者向量序列。

{% asset_img rnn9.png %}

编码-解码或者序列到序列的架构，基本思路：

1. 编码器或输入RNN处理输入序列，编码器输出上下文$C$（通常是最终隐藏状态的简单函数）；
2. 解码器或输出RNN则以固定长度的向量为条件产生输出序列$\boldsymbol{Y} = (\boldsymbol{y}^{(1)},...,\boldsymbol{y}^{(n_y)})$。

序列长度$n_x,n_y$可以彼此不同。两个RNN共同训练以最大化$\log P(\boldsymbol{y}^{(1)},...,\boldsymbol{y}^{(n_y)}|\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(n_x)})$（关于训练集中所有$\boldsymbol{x}$和$\boldsymbol{y}$对的平均）。编码器RNN的最后一个状态$\boldsymbol{h}_{n_x}$通常被当作输入的表示$C$并作为解码器RNN的输入。

这里并不强制要求编码器与解码器的隐藏层具有相同的大小。

此架构的一个明显不足是，编码器RNN输出的上下文$C$的维度太小而难以适当地概括一个长序列。
让$C$成为可变长度的序列，而不是一个固定大小的向量；
将序列$C$的元素和输出序列的元素相关联的**注意力机制**。

### 5. 深度循环网络

大多数RNN中的计算可以分解为三块参数及其相关的变换：

1. 从输入到隐藏状态；
2. 从前一隐藏状态到下一隐藏状态；
3. 从隐藏状态到输出。

深度RNN，简而言之就是将MLP引入到RNN前向传播过程中与参数矩阵有关的位置。一般来说，简单的RNN的三个参数矩阵$\boldsymbol{U,W,V}$形状应该是$输入向量的维度 \times 输出向量的维度$，若需要保存更多的信息，我们只能简单的靠增加维度来实现，但是这样对运算来说是不合理的，显然，我们可以考虑将DNN引入进来，以增加参数矩阵的深度。

{% asset_img rnn10.png %}

1. 如图a，分解隐藏状态的层次；
2. 如图b，分别对三个参数矩阵应用MLP，这会导致隐藏状态到下一隐藏状态的路径加倍；
3. 如图c，引入跳跃连接缓解路径问题。

## 6. 递归神经网络

递归神经网络代表循环网络的另一个扩展，它被构造为深的树状结构而不是RNN的链状结构，因此是不同类型的计算图。

{% asset_img rnn11.png %}

这类网络的潜在用途-学习推论。递归网络已成功地应用于输入是数据结构的神经网络。
递归网络的一个明显优势是，对于具有相同长度$\tau$的序列，深度（通过非线性操作的组合数量来衡量）可以急剧地从$\tau$减小为$O(\log \tau)$，这可能有助于解决长期依赖。
一个悬而未决的问题是如何以最佳的方式构造树。一种选择是使用不依赖于数据的树结构，如平衡二叉树。在某些应用领域，外部方法可以为选择适当的树结构提供借鉴。例如，处理自然语言的句子时，用于递归网络的树结构可以被固定为句子语法分析树的结构（可以由自然语言语法分析程序提供）。理想的情况下，人们希望学习器自行发现和推断适合于任意给定输入的树结构。

## 7. 长期依赖的挑战





