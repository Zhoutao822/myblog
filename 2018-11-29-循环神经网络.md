---
title: 循环神经网络
date: 2018-11-29 09:16:15
categories:
- Deep Learning
tags:
- Theory
- RNN
mathjax: true
---

参考：

> [《深度学习》第10章 序列建模：循环和递归网络](https://github.com/exacity/deeplearningbook-chinese)

循环神经网络或RNN是一类用于处理序列数据的神经网络。正如卷积网络可以很容易地扩展到具有很大宽度和高度的图像，以及处理大小可变的图像，循环网络可以扩展到更长的序列（比不基于序列的特化网络长得多）。大多数循环网络也能处理可变长度的序列。

卷积操作允许网络跨时间共享参数，但是浅层的。卷积的输出是一个序列，其中输出中的每一项是相邻几项输入的函数。参数共享的概念体现在每个时间步中使用的相同卷积核。循环神经网络以不同的方式共享参数。输出的每一项是前一项的函数。输出的每一项对先前的输出应用相同的更新规则而产生。这种循环方式导致参数通过很深的计算图共享。

{% asset_img rnn0.png %}

## 1. 展开计算图

计算图是形式化一组计算结构的方式，如那些涉及将输入和参数映射到输出和损失的计算。

展开（unfolding）递归或循环计算得到的重复结构进行解释，这些重复结构通常对应于一个事件链。 展开（unfolding）这个计算图将导致深度网络结构中的参数共享。

使用$\boldsymbol{h}$表示网络的隐藏单元

$$
\boldsymbol{h}^{(t)} = f(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)};\boldsymbol{\theta})
$$

我们需要将任意长的序列$(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)},...,\boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)})$到以固定长度的向量$\boldsymbol{h}^{(t)}$。根据实际情况，我们往往并不需要保存全部序列的信息，所以仅仅存储足够预测句子其余部分的信息。最苛刻的情况是要求$\boldsymbol{h}^{(t)}$足够风骨，并能大致恢复输入序列。

展开图的大小取决于序列长度。用一个函数$g^{(t)}$代表经$t$步展开后的循环：

$$
\boldsymbol{h}^{(t)} = g^{(t)}(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)},...,\boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)})
\\
= f(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)};\boldsymbol{\theta})
$$

函数$g^{(t)}$将全部过去序列作为输入来生成当前状态，但是展开的循环架构允许我们将$g^{(t)}$分解为函数$f$的重复应用。因此，展开过程引入两个主要优点：

1. 无论序列的长度，学成的模型始终具有相同的输入大小，因为它指定的是从一种状态到另一种状态的转移，而不是在可变长度的历史状态上操作。
2. 我们可以在每个时间步使用相同参数的相同转移函数$f$。

这两个因素使得学习在所有时间步和所有序列长度上操作单一的模型$f$是可能的，而不需要在所有可能时间步学习独立的模型$g^{(t)}$。学习单一的共享模型允许泛化到没有见过的序列长度（没有出现在训练集中），并且估计模型所需的训练样本远远少于不带参数共享的模型。

## 2. 循环神经网络

{% asset_img rnn1.png %}

循环神经网络的设计模式包括以下几种：

1. 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络；
2. 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络；
3. 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络。






