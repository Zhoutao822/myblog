---
title: 循环神经网络
date: 2018-11-29 09:16:15
categories:
- Deep Learning
tags:
- Theory
- RNN
mathjax: true
---

参考：

> [《深度学习》第10章 序列建模：循环和递归网络](https://github.com/exacity/deeplearningbook-chinese)
> [回声状态网络(ESN)教程](https://blog.csdn.net/cassiePython/article/details/80389394)

循环神经网络或RNN是一类用于处理序列数据的神经网络。正如卷积网络可以很容易地扩展到具有很大宽度和高度的图像，以及处理大小可变的图像，循环网络可以扩展到更长的序列（比不基于序列的特化网络长得多）。大多数循环网络也能处理可变长度的序列。

卷积操作允许网络跨时间共享参数，但是浅层的。卷积的输出是一个序列，其中输出中的每一项是相邻几项输入的函数。参数共享的概念体现在每个时间步中使用的相同卷积核。循环神经网络以不同的方式共享参数。输出的每一项是前一项的函数。输出的每一项对先前的输出应用相同的更新规则而产生。这种循环方式导致参数通过很深的计算图共享。

{% asset_img rnn0.png %}

<!-- more -->

## 1. 展开计算图

计算图是形式化一组计算结构的方式，如那些涉及将输入和参数映射到输出和损失的计算。

展开（unfolding）递归或循环计算得到的重复结构进行解释，这些重复结构通常对应于一个事件链。 展开（unfolding）这个计算图将导致深度网络结构中的参数共享。

使用$\boldsymbol{h}$表示网络的隐藏单元

$$
\boldsymbol{h}^{(t)} = f(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)};\boldsymbol{\theta})
$$

我们需要将任意长的序列$(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)},...,\boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)})$到以固定长度的向量$\boldsymbol{h}^{(t)}$。根据实际情况，我们往往并不需要保存全部序列的信息，所以仅仅存储足够预测句子其余部分的信息。最苛刻的情况是要求$\boldsymbol{h}^{(t)}$足够风骨，并能大致恢复输入序列。

展开图的大小取决于序列长度。用一个函数$g^{(t)}$代表经$t$步展开后的循环：

$$
\boldsymbol{h}^{(t)} = g^{(t)}(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)},...,\boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)})
\\
= f(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)};\boldsymbol{\theta})
$$

函数$g^{(t)}$将全部过去序列作为输入来生成当前状态，但是展开的循环架构允许我们将$g^{(t)}$分解为函数$f$的重复应用。因此，展开过程引入两个主要优点：

1. 无论序列的长度，学成的模型始终具有相同的输入大小，因为它指定的是从一种状态到另一种状态的转移，而不是在可变长度的历史状态上操作。
2. 我们可以在每个时间步使用相同参数的相同转移函数$f$。

这两个因素使得学习在所有时间步和所有序列长度上操作单一的模型$f$是可能的，而不需要在所有可能时间步学习独立的模型$g^{(t)}$。学习单一的共享模型允许泛化到没有见过的序列长度（没有出现在训练集中），并且估计模型所需的训练样本远远少于不带参数共享的模型。

## 2. 循环神经网络

{% asset_img rnn1.png %}

从图中可知，循环神经网络使用了三个权重矩阵$\boldsymbol{U, W, V}$，矩阵$\boldsymbol{U}$对输入进行处理，矩阵$\boldsymbol{W}$对状态转移进行处理，矩阵$\boldsymbol{V}$对输出进行处理；非常类似于马尔可夫链。

循环神经网络的设计模式包括以下几种：

1. 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络；
2. 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络；
3. 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络。

### 2.1 前向传播

假设：

* 隐藏单元的激活函数未双曲正切函数tanh；
* 输出$\boldsymbol{o}$是离散变量可能值的非标准化对数概率，如用于预测词或字符的onehot形式；
* 预测值$\hat{\boldsymbol{y}}$是对输出进行softmax处理后得到的标准化向量；
* RNN从特定的初始状态$\boldsymbol{h}^{(0)}$开始前向传播。

则从$t=1$到$t=\tau$的每个时间步，更新方程为

$$
\boldsymbol{a}^{(t)} = \boldsymbol{b} + \boldsymbol{W}\boldsymbol{h}^{(t-1)}  + \boldsymbol{U}\boldsymbol{x}^{(t)}
\\
\boldsymbol{h}^{(t)} = \tanh(\boldsymbol{a}^{(t)})
\\
\boldsymbol{o}^{(t)} = \boldsymbol{c} + \boldsymbol{V}\boldsymbol{h}^{(t)}
\\
\hat{\boldsymbol{y}}^{(t)} = softmax(\boldsymbol{o}^{(t)})
$$

偏置向量$\boldsymbol{b, c}$分别对应于输入到隐藏和隐藏到输出的连接。我们将一个输入序列映射到相同长度的输出序列。总损失就是所有时间步的损失之和。例如$L^{(t)}$为给定的$\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(t)}$后$\boldsymbol{y}^{(t)}$的负对数似然，则

$$
L(\{ \boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(\tau)} \}, \{ \boldsymbol{y}^{(1)},...,\boldsymbol{y}^{(\tau)} \})
\\
= \sum_t L^{(t)}
\\
= - \sum_t \log p_{model}(\boldsymbol{y}^{(t)}|\{ \boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(t)} \})
$$

关于各个参数计算这个损失函数的梯度是计算成本很高的操作。梯度计算涉及执行一次前向传播，接着是由右到左的反向传播。运行时间是$O(\tau)$，并且不能通过并行化来降低，因为前向传播图是固有循序的；每个时间步只能一前一后地计算。前向传播中的各个状态必须保存，直到它们反向传播中被再次使用，因此内存代价也是$O(\tau)$。应用于展开图且代价为$O(\tau)$的反向传播算法称为**通过时间反向传播BPTT**。

### 2.2 导师驱动过程和输出循环网络

{% asset_img rnn2.png %}

这个网络在一个时间步的输出和下一个时间步的隐藏单元间存在循环网络，这就导致了丢失了隐藏单元到隐藏单元间对于序列变化的信息。但是优点在于基于训练集提供的输出的理想值，我们没有必要计算前一时刻的输出，所以我们可以并行计算所有时间步的梯度。

由输出反馈到模型而产生循环连接的模型可用**导师驱动过程**进行训练。训练模型时，导师驱动过程不再使用最大似然准则，而在时刻$t+1$接收真实值$y^{(t)}$作为输入。我们可以通过检查两个时间步的序列得知这一点。条件最大似然准则是

$$
\log p(\boldsymbol{y}^{(1)},\boldsymbol{y}^{(2)}|\boldsymbol{x}^{(1)},\boldsymbol{x}^{(2)})
\\
\log p(\boldsymbol{y}^{(2)}|\boldsymbol{y}^{(1)},\boldsymbol{x}^{(1)},\boldsymbol{x}^{(2)}) + \log p(\boldsymbol{y}^{(1)}|\boldsymbol{x}^{(1)},\boldsymbol{x}^{(2)})
$$

{% asset_img rnn3.png %}

我们使用导师驱动过程的最初动机是为了在缺乏隐藏到隐藏连接的模型中避免通过时间反向传播。只要模型一个时间步的输出与下一时间步计算的值存在连接，导师驱动过程仍然可以应用到这些存在隐藏到隐藏连接的模型。然而，只要隐藏单元成为较早时间步的函数，BPTT算法是必要的。因此训练某些模型时要同时使用导师驱动过程和BPTT。

如果之后网络在开环 (open-loop) 模式下使用，即网络输出（或输出分布的样本）反馈作为输入，那么完全使用导师驱动过程进行训练的缺点就会出现。如上图所示，训练过程中的反馈与实际部署时的反馈是不同的。

在这种情况下，训练期间该网络看到的输入与测试时看到的会有很大的不同。减轻此问题的一种方法是同时使用导师驱动过程和自由运行的输入进行训练，例如在展开循环的输出到输入路径上预测几个步骤的正确目标值。

通过这种方式，网络可以学会考虑在训练时没有接触到的输入条件（如自由运行模式下，自身生成自身），以及将状态映射回使网络几步之后生成正确输出的状态。另外一种方式是通过随意选择生成值或真实的数据值作为输入以减小训练时和测试时看到的输入之间的差别。这种方法利用了课程学习策略，逐步使用更多生成值作为输入。

### 2.3 计算循环神经网络的梯度

回顾一下前向传播的公式

$$
\boldsymbol{a}^{(t)} = \boldsymbol{b} + \boldsymbol{W}\boldsymbol{h}^{(t-1)}  + \boldsymbol{U}\boldsymbol{x}^{(t)}
\\
\boldsymbol{h}^{(t)} = \tanh(\boldsymbol{a}^{(t)})
\\
\boldsymbol{o}^{(t)} = \boldsymbol{c} + \boldsymbol{V}\boldsymbol{h}^{(t)}
\\
\hat{\boldsymbol{y}}^{(t)} = softmax(\boldsymbol{o}^{(t)})
$$

我们做出的假设是：真实值和输入值都是onehot变量（对应于字或者词的字典模型），预测值是形状相同的概率输出，考虑onehot的每一位$i$，损失$L^{(t)}$为

$$
L^{(t)} = -\sum_iy_i^{(t)}\log \hat{y}^{(t)}_i
$$

再考虑整体损失

$$
L(\{ \boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(\tau)} \}, \{ \boldsymbol{y}^{(1)},...,\boldsymbol{y}^{(\tau)} \})
\\
= \sum_t L^{(t)}
$$

可以得到

$$
\frac{\partial L}{\partial L^{(t)}} = 1
\\
(\bigtriangledown_{\boldsymbol{o}^{(t)}}L)_i = \frac{\partial L}{\partial o^{(t)}_i} = \frac{\partial L}{\partial L^{(t)}}\frac{\partial L^{(t)}}{\partial o_i^{(t)}} = \hat{y}^{(t)}_i - \boldsymbol{1}_{i, y^{(t)}}
$$

上面的式子其实并不复杂，对于onehot变量来说，$\boldsymbol{y}^{(t)}$中只有一位为1，其余位都为0，也就是说，对于$\boldsymbol{o}^{(t)}$来说，梯度简单到就是预测值与真实值的差值。具体证明参考`深度学习-激活函数`。

基于上式，我们可以对最后的时间步$\tau$的$\boldsymbol{h}^{(\tau)}$进行求导计算

$$
\bigtriangledown_{\boldsymbol{h}^{(\tau)}}L = \boldsymbol{V}^T\bigtriangledown_{\boldsymbol{o}^{(\tau)}}L
$$

然后，计算从时刻$t= \tau - 1$到$t=1$反向迭代，通过时间反向传播梯度，由于$\boldsymbol{h}^{(t)}(t < \tau)$同时存在于$\boldsymbol{o}^{(t)}$和$\boldsymbol{h}^{(t+1)}$中，因此梯度计算需要考虑两个部分

$$
\bigtriangledown_{\boldsymbol{h}^{(t)}}L=(\frac{\partial \boldsymbol{h}^{(t+1)}}{\partial \boldsymbol{h}^{(t)}})^T(\bigtriangledown_{\boldsymbol{h}^{(t+1)}}L) + (\frac{\partial \boldsymbol{o}^{(t)}}{\partial \boldsymbol{h}^{(t)}})^T(\bigtriangledown_{\boldsymbol{o}^{(t)}}L)
\\
= \boldsymbol{W}^T(\bigtriangledown_{\boldsymbol{h}^{(t+1)}}L)diag(1-(\boldsymbol{h}^{(t+1)})^2) + \boldsymbol{V}^T(\bigtriangledown_{\boldsymbol{o}^{(t)}}L)
$$

$\tanh$求导参考`深度学习-激活函数`，其中$diag(1-(\boldsymbol{h}^{(t+1)})^2)$表示包含元素$1-(h_i^{(t+1)})^2$的对角矩阵。

其他参数是时间步共享的，那么需要使用$t$时刻的虚拟变量$\boldsymbol{W}^{(t)}$作为$\boldsymbol{W}$的副本，然后计算在时间步$t$权重对梯度的贡献。

$$
\bigtriangledown_\boldsymbol{c}L = \sum_t(\frac{\partial \boldsymbol{o}^{(t)}}{\partial \boldsymbol{c}})^T\bigtriangledown_{\boldsymbol{o}^{(t)}}L = \sum_t\bigtriangledown_{\boldsymbol{o}^{(t)}}L
\\
\bigtriangledown_\boldsymbol{b}L = \sum_t(\frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{b}})^T\bigtriangledown_{\boldsymbol{h}^{(t)}}L = \sum_t diag(1-(\boldsymbol{h}^{(t)})^2) \bigtriangledown_{\boldsymbol{h}^{(t)}}L
\\
\bigtriangledown_\boldsymbol{V}L = \sum_t\sum_i(\frac{\partial L}{\partial o_i^{(t)}})\bigtriangledown_{\boldsymbol{V}^{(t)}}o_i^{(t)} = \sum_t (\bigtriangledown_{\boldsymbol{o}^{(t)}}L) \boldsymbol{h}^{(t)^T}
\\
\bigtriangledown_\boldsymbol{W}L = \sum_t\sum_i(\frac{\partial L}{\partial h_i^{(t)}})\bigtriangledown_{\boldsymbol{W}^{(t)}}h_i^{(t)}
= \sum_t diag(1-(\boldsymbol{h}^{(t)})^2) (\bigtriangledown_{\boldsymbol{h}^{(t)}}L)\boldsymbol{h}^{(t-1)^T}
\\
\bigtriangledown_\boldsymbol{U}L = \sum_t\sum_i(\frac{\partial L}{\partial h_i^{(t)}})\bigtriangledown_{\boldsymbol{U}^{(t)}}h_i^{(t)}
= \sum_t diag(1-(\boldsymbol{h}^{(t)})^2) (\bigtriangledown_{\boldsymbol{h}^{(t)}}L)\boldsymbol{x}^{(t)^T}
$$

### 2.4 作为有向图模型的循环网络

本节描述没有输入$\boldsymbol{x}$的情况下，RNN如何对应于有向图模型。

与前馈网络类似，原则上循环网络几乎可以使用任何损失。但必须根据任务来选择损失。如前馈网络，我们通常希望将RNN的输出解释为一个概率分布，并且我们通常使用与分布相关联的交叉熵来定义损失。均方误差是与单位高斯分布的输出相关联的交叉熵损失，例如前馈网络中所使用的。

若不考虑输入$x$，仅考虑输出$y^{(t)}$与之前序列的关系，我们可以得到一个全连接图模型，这种情况下，计算的是链式条件概率，比较类似马尔可夫链

{% asset_img rnn4.png %}

若考虑引入隐藏的状态变量，则可以利用$\boldsymbol{h}$保存历史的信息，由于参数共享，RNN 的参数数目为$O(1)$且是序列长度的函数。我们可以调节RNN的参数数量来控制模型容量，但不用被迫与序列长度成比例。

{% asset_img rnn5.png %}

循环网络为减少的参数数目付出的代价是优化参数可能变得困难。

在循环网络中使用的参数共享的前提是相同参数可用于不同时间步的假设。也就是说，假设给定时刻$t$的变量后，时刻$t + 1$变量的条件概率分布是平稳的，这意味着之前的时间步与下个时间步之间的关系并不依赖于$t$。虽然$t$可以用作额外的输入。

为了完整描述将RNN作为图模型的观点，我们必须描述如何从模型采样。我们需要执行的主要操作是简单地从每一时间步的条件分布采样。

1. 在训练样本$\boldsymbol{x}^{(\tau)}$后增加一个特殊符号，当产生该符号时，停止训练；
2. 模型中引入一个额外的伯努利输出，表示在每个时间步决定继续生成或停止生成。新的输出单元通常使用 sigmoid 单元，并通过交叉熵训练。在这种方法中，sigmoid 被训练为最大化正确预测的对数似然，即在每个时间步序列决定结束或继续；
3. 将一个额外的输出添加到模型并预测整数$\tau$本身。模型可以采出$\tau$的值，然后采$\tau$步有价值的数据。这种方法需要在每个时间步的循环更新中增加一个额外输入，使得循环更新知道它是否是靠近所产生序列的末尾。这种额外的输入可以是$\tau$的值，也可以是$\tau - t$即剩下时间步的数量。

### 2.5 基于上下文的RNN序列建模

本节描述输入$\boldsymbol{x}$为单个向量的情况下，RNN的模型。

那么我们可以将$\boldsymbol{x}$看作产生$\boldsymbol{y}$序列RNN的额外输入。常见做法是：

1. 在每个时刻作为一个额外输入，即每个时刻都输入相同的$\boldsymbol{x}$；
2. 作为初始状态，即根据输入$\boldsymbol{x}$预测后面所有的序列；
3. 结合两种方式。

{% asset_img rnn6.png %}

对于1，输入到隐藏向量之间通过矩阵$\boldsymbol{R}$参数化，这类RNN适用于很多任务如图注，其中单个图像作为模型的输入，然后产生描述图像的词序列。

{% asset_img rnn7.png %}

将可变长度的$\boldsymbol{x}$值序列映射到相同长度的$\boldsymbol{y}$值序列上分布的条件循环神经网络。

## 3. 双向RNN

目前为止考虑的RNN有一种因果结构，即时刻$t$的状态只能从过去的序列$\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(t-1)}$以及当前输入$\boldsymbol{x}^{(t)}$捕获。或者在$\boldsymbol{y}$可用的情况下允许过去的$\boldsymbol{y}$影响当前状态的模型。

在许多应用中，输出$\boldsymbol{y}^{(t)}$的预测可能依赖于整个输入序列。可能需要从更远的未来寻找信息区分它们。比如语音识别，音素的判断。

双向RNN就是这样被发明了。双向RNN结合时间上从序列起点开始移动的RNN和另一个时间上从序列末尾开始移动的RNN。其中$\boldsymbol{h}^{(t)}$代表通过时间向前移动的子RNN的状态，$\boldsymbol{g}^{(t)}$代表通过时间向后移动的子RNN的状态。这允许输出单元$\boldsymbol{o}^{(t)}$能够计算同时依赖于过去和未来且对时刻$t$的输入值最敏感的表示，而不必指定$t$周围固定大小的窗口（这是前馈网络、卷积网络或具有固定大小的先行缓存器的常规RNN所必须要做的）。

{% asset_img rnn8.png %}

扩展这个想法到2维输入，如图象，由4个RNN组成，沿四个方向：上下左右。实际上，对于这样的RNN，前向传播公式可以写成表示使用卷积的形式，计算自底向上到每一层的输入（在整合横向相互作用的特征图的循环传播之前）。

### 4. 基于编码-解码的序列到序列架构

目的将可变长度序列映射到另一可变长度序列。

我们经常将RNN的输入称为“上下文”。我们希望产生此上下文的表示$C$。这
个上下文$C$可能是一个概括输入序列$\boldsymbol{X} = (\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(n_x)})$的向量或者向量序列。

{% asset_img rnn9.png %}

编码-解码或者序列到序列的架构，基本思路：

1. 编码器或输入RNN处理输入序列，编码器输出上下文$C$（通常是最终隐藏状态的简单函数）；
2. 解码器或输出RNN则以固定长度的向量为条件产生输出序列$\boldsymbol{Y} = (\boldsymbol{y}^{(1)},...,\boldsymbol{y}^{(n_y)})$。

序列长度$n_x,n_y$可以彼此不同。两个RNN共同训练以最大化$\log P(\boldsymbol{y}^{(1)},...,\boldsymbol{y}^{(n_y)}|\boldsymbol{x}^{(1)},...,\boldsymbol{x}^{(n_x)})$（关于训练集中所有$\boldsymbol{x}$和$\boldsymbol{y}$对的平均）。编码器RNN的最后一个状态$\boldsymbol{h}_{n_x}$通常被当作输入的表示$C$并作为解码器RNN的输入。

这里并不强制要求编码器与解码器的隐藏层具有相同的大小。

此架构的一个明显不足是，编码器RNN输出的上下文$C$的维度太小而难以适当地概括一个长序列。
让$C$成为可变长度的序列，而不是一个固定大小的向量；
将序列$C$的元素和输出序列的元素相关联的**注意力机制**。

### 5. 深度循环网络

大多数RNN中的计算可以分解为三块参数及其相关的变换：

1. 从输入到隐藏状态；
2. 从前一隐藏状态到下一隐藏状态；
3. 从隐藏状态到输出。

深度RNN，简而言之就是将MLP引入到RNN前向传播过程中与参数矩阵有关的位置。一般来说，简单的RNN的三个参数矩阵$\boldsymbol{U,W,V}$形状应该是$输入向量的维度 \times 输出向量的维度$，若需要保存更多的信息，我们只能简单的靠增加维度来实现，但是这样对运算来说是不合理的，显然，我们可以考虑将DNN引入进来，以增加参数矩阵的深度。

{% asset_img rnn10.png %}

1. 如图a，分解隐藏状态的层次；
2. 如图b，分别对三个参数矩阵应用MLP，这会导致隐藏状态到下一隐藏状态的路径加倍；
3. 如图c，引入跳跃连接缓解路径问题。

## 6. 递归神经网络

递归神经网络代表循环网络的另一个扩展，它被构造为深的树状结构而不是RNN的链状结构，因此是不同类型的计算图。

{% asset_img rnn11.png %}

这类网络的潜在用途-学习推论。递归网络已成功地应用于输入是数据结构的神经网络。
递归网络的一个明显优势是，对于具有相同长度$\tau$的序列，深度（通过非线性操作的组合数量来衡量）可以急剧地从$\tau$减小为$O(\log \tau)$，这可能有助于解决长期依赖。
一个悬而未决的问题是如何以最佳的方式构造树。一种选择是使用不依赖于数据的树结构，如平衡二叉树。在某些应用领域，外部方法可以为选择适当的树结构提供借鉴。例如，处理自然语言的句子时，用于递归网络的树结构可以被固定为句子语法分析树的结构（可以由自然语言语法分析程序提供）。理想的情况下，人们希望学习器自行发现和推断适合于任意给定输入的树结构。

## 7. 长期依赖的挑战

长期依赖的挑战根本问题是，经过许多阶段传播后的梯度倾向于消失（大部分情况）或爆炸（很少，但对优化过程影响很大）。

循环神经网络所使用的函数组合有点像矩阵乘法，可以认为

$$
\boldsymbol{h}^{(t)} = \boldsymbol{W}^T\boldsymbol{h}^{(t-1)}
$$

是一个简单的、缺少非线性激活函数和输入的循环神经网络。根据递推关系

$$
\boldsymbol{h}^{(t)} = (\boldsymbol{W}^t)^T\boldsymbol{h}^{(0)}
$$

而当$\boldsymbol{W}$符合下列形式的特征分解

$$
\boldsymbol{W} = \boldsymbol{Q\Lambda Q}^T
$$

其中$\boldsymbol{Q}$正交，循环型可进一步简化为

$$
\boldsymbol{h}^{(t)} = \boldsymbol{Q}^T\boldsymbol{\Lambda}^t\boldsymbol{Qh}^{(0)}
$$

显然，$\boldsymbol{\Lambda}$中小于1的特征值会衰减到0，大于1的特征值会激增。任何不与最大特征向量对齐的$\boldsymbol{h}^{(0)}$的部分将最终被丢弃。

如果每个时刻使用不同的权重$w^{(t)}$，情况就不同了。假设$w^{(t)}$的值是随机生成的，各自独立且均值为0方差为$v$，乘积的方差为$O(v^n)$。如果我们希望最终结果的方差为$v^*$，我们就可以选择单个方差为$v=\sqrt[n]{v^*}$的权重。因此，非常深的前馈网络通过精心设计的比例可以避免梯度消失和爆炸问题。

为了储存记忆并对小扰动具有鲁棒性，RNN必须进入参数空间中的梯度消失区域。具体来说，每当模型能够表示长期依赖时，长期相互作用的梯度幅值就会变得指数小（相比短期相互作用的梯度幅值）。这并不意味着这是不可能学习的，由于长期依赖关系的信号很容易被短期相关性产生的最小波动隐藏，因而学习长期依赖可能需要很长的时间。

## 8. 回声状态网络

从$\boldsymbol{h}^{(t-1)}$到$\boldsymbol{h}^{(t)}$的循环权重映射以及从$\boldsymbol{x}^{(t)}$到$\boldsymbol{h}^{(t)}$的输入权重映射是循环网络中最难学习的参数。
避免这种困难的方法是设定循环隐藏单元，使其能很好地捕捉过去输入历史，并且只学习输出权重。

回声状态网络ESN以及流体状态机就是基于上述思想的模型。ESN和流体状态机都被称为**储层计算**，因为隐藏单元形成了可能捕获输入历史不同方面的临时特征池。

储层计算循环网络类似于核机器，这是思考它们的一种方式：它们将任意长度的序列（到时刻$t$的输入历史）映射为一个长度固定的向量（循环状态$\boldsymbol{h}^{(t)}$），之后可以施加一个线性预测算子（通常是一个线性回归）以解决感兴趣的问题。训练准则就可以很容易地设计为输出权重的凸函数。例如，如果输出是从隐藏单元到输出目标的线性回归，训练准则就是均方误差，由于是凸的就可以用简单的学习算法可靠地解决。

{% asset_img rnn12.png %}

网络结构依次是输入层、储备池和输出层，所谓的储备池就是中间的部分。这个储备池的特点是: (1)储备池中神经元的连接状态是随机的，即神经元之间是否建立连接并不是我们人工确定的；(2)储备池中的连接权重是固定的，不像传统的MLP网络使用梯度下降进行权重的更新。这样做的好处是：(1)大大降低了训练的计算量；(2)一定程度上避免了梯度下降的优化算法中出现的局部极小情况；(3)此外，在很多问题上确实有着不错的建模能力。ESN的基本思想就是由储备池生成一个随输入不断变化的复杂动态空间，当这个状态空间足够复杂时，就可以利用这些内部状态, 线性地组合处所需要的对应输出~(实际上就是传统的MLP拟合的能力)。

最初的想法是使状态到状态转换函数的Jacobian矩阵的特征值接近1。循环网络的一个重要特征就是Jacobian矩阵的特征值谱$\boldsymbol{J}^{(t)} = \frac{\partial s^{(t)}}{\partial s^{(t-1)}}$。特别重要的是$\boldsymbol{J}^{(t)}$的**谱半径**，定义为特征值的最大绝对值。

同理，在矩阵$\boldsymbol{J}$不随$t$改变的简单情况下，矩阵$\boldsymbol{J}$大于1的的特征值会导致偏差指数增大，小于1的会导致偏差变得指数小。

当非线性存在时，非线性的导数将在许多时间步后接近0，并有助于防止因过大的谱半径而导致的爆炸。事实上，关于回声状态网络的最近工作提倡使用远大于1的谱半径。

需要注意的是，$\boldsymbol{W}$和$\boldsymbol{J}$都不需要是对称的（尽管它们是实方阵），因此它们可能有复的特征值和特征向量，其中虚数分量对应于潜在的振荡行为（如果迭代地应用同一Jacobian）。即使$\boldsymbol{h}^{(t)}$或$\boldsymbol{h}^{(t)}$中有趣的小变化在反向传播中是实值的，它们仍可以用这样的复数基表示。

非线性映射情况时，Jacobian会在每一步任意变化。因此，动态量变得更加复杂。然而，一个小的初始变化多步之后仍然会变成一个大的变化。纯线性和非线性情况的一个不同之处在于使用压缩非线性（如$\tanh$）可以使循环动态量有界。注意，即使前向传播动态量有界，反向传播的动态量仍然可能无界，例如，当$\tanh$序列都在它们状态中间的线性部分，并且由谱半径大于1的权重矩阵连接。然而，所有$\tanh$单元同时位于它们的线性激活点是非常罕见的。

回声状态网络的策略是简单地固定权重使其具有一定的谱半径如3，其中信息通过时间前向传播，但会由于饱和非线性单元（如$\tanh$）的稳定作用而不会爆炸。

**ESN的构造过程**

{% asset_img rnn13.png %}

## 9. 渗漏单元和其他多时间尺度的策略

处理长期依赖的一种方法是设计工作在多个时间尺度的模型，使模型的某些部分在细粒度时间尺度上操作并能处理小细节，而其他部分在粗时间尺度上操作并能把遥远过去的信息更有效地传递过来。存在多种同时构建粗细时间尺度的策略。这些策略包括在时间轴增加跳跃连接，“渗漏单元”使用不同时间常数整合信号，并去除一些用于建模细粒度时间尺度的连接。

### 9.1 时间维度的跳跃连接

将循环网络延时，从$t$到$t+1$延长到从$t$到$t+d$，那么梯度下降的速度与$\frac{\tau}{d}$相关而不是$\tau$。虽然仍然存在指数爆炸的可能性，但是时间跨度变长了，算法捕捉到更长的依赖性。

### 9.2 渗漏单元和一系列不同时间尺度

渗漏单元：参数更新方式为$\mu^{(t)} \leftarrow \alpha \mu^{(t-1)} + (1-\alpha)v^{(t)}$。其目的是通过一个累积的滑动平均值$\mu^{(t)}$模拟滑动平均，从而达到记住过去的信息，当$\alpha$接近1时，滑动平均值能记住过去很长一段时间的信息，当$\alpha$接近0时，关于过去的信息被迅速丢弃。

一般通过两种基本策略设置渗漏单元使用的时间常数。一种策略是手动将其固定为常数，例如在初始化时从某些分布采样它们的值。另一种策略是使时间常数成为自由变量，并学习出来。在不同时间尺度使用这样的渗漏单元似乎能帮助学习长期依赖。

### 9.3 删除连接

处理长期依赖另一种方法是在多个时间尺度组织RNN状态的想法，信息在较慢的时间尺度上更容易长距离流动。

这个想法与之前讨论的时间维度上的跳跃连接不同，因为它涉及主动删除长度为一的连接并用更长的连接替换它们。以这种方式修改的单元被迫在长时间尺度上运作。而通过时间跳跃连接是添加边。收到这种新连接的单元，可以学习在长时间尺度上运作，但也可以选择专注于自己其他的短期连接。

强制一组循环单元在不同时间尺度上运作有不同的方式。一种选择是使循环单元变成渗漏单元，但不同的单元组关联不同的固定时间尺度。另一种选择是使显式且离散的更新发生在不同的时间，不同的单元组有不同的频率。

## 10. 长短期记忆和其他门控RNN

实际应用中常使用基于**长短期记忆**和基于**门控循环单元**的网络。

像渗漏单元一样，门控RNN想法也是基于生成通过时间的路径，其中导数既不消失也不发生爆炸。渗漏单元通过手动选择常量的连接权重或参数化的连接权重来达到这一目的。门控RNN将其推广为在每个时间步都可能改变的连接权重。

渗漏单元允许网络在较长持续时间内积累信息（诸如用于特定特征或类的线索）。然而，一旦该信息被使用，让神经网络遗忘旧的状态可能是有用的。例如，如果一个序列是由子序列组成，我们希望渗漏单元能在各子序列内积累线索，我们需要将状态设置为0以忘记旧状态的机制。我们希望神经网络学会决定何时清除状态，而不是手动决定。这就是门控RNN要做的事。

### 10.1 LSTM

长短期记忆，通过引入自循环以产生梯度长时间持续流动的路径。同时空过另一个隐藏单元实现门控自循环的权重，累积的时间尺度可以动态地改变。

{% asset_img rnn15.png %}

LSTM循环网络除了外部的RNN循环外，还具有内部的LSTM细胞循环，因此LSTM不是简单地向输入和循环单元的仿射变换之后施加一个逐元素的非线性。

{% asset_img rnn14.png %}

* 最下方的`input, input gate, forget gate, output gate`的输入都是相同的，一个是当前时刻的输入$\boldsymbol{x}^{(t)}$，另一个是前一时刻的隐藏单元$\boldsymbol{h}^{(t-1)}$；
* 最重要的部分是状态单元$s_i^{(t)}$，也就是`self-loop`对应的位置，此处自环的权重（或相关联的时间常数）由遗忘门$f_i^{(t)}$控制，由sigmoid单元将权重设置为0和1之间的值：
$$
f_i^{(t)} = \sigma(b_i^f + \sum_jU^f_{i,j}x_j^{(t)} + \sum_jW^f_{i,j}h_j^{(t-1)})
$$
* 其中$\boldsymbol{x}^{(t)}$是当前输入向量，$\boldsymbol{h}^t$是当前隐藏层向量，$\boldsymbol{b}^f, \boldsymbol{U}^f, \boldsymbol{W}^f$分别是偏置、输入权重和遗忘门的循环权重；
* 状态单元更新方式为：

$$
s_i^{(t)} = f_i^{(t)}s_i^{(t-1)} + g_i^{(t)}\sigma(b_i + \sum_j U_{i,j}x_j^{(t)} + \sum_j W_{i,j}h_j^{(t-1)})
$$
* 外部输入门单元$g_i^{(t)}$的更新方式：

$$
g_i^{(t)} = \sigma(b_i^g + \sum_j U^g_{i,j}x_j^{(t)} + \sum_j W^g_{i,j}h_j^{(t-1)})
$$
* 输出门$q_i^{(t)}$控制LSTM细胞的输出$h_i^{(t)}$：

$$
h_i^{(t)} = \tanh(s_i^{(t)})q_i^{(t)}
\\
q_i^{(t)} = \sigma(b_i^o + \sum_jU^o_{i,j}x_j^{(t)} + \sum_jW^o_{i,j}h_j^{(t-1)})
$$
* 在LSTM变体中，可以选择使用状态单元$s_i^{(t)}$作为额外的输入，输入到第$i$个单元的三个门，同时也需要额外的三个参数。

LSTM网络比简单的循环架构更易于学习长期依赖。

### 10.2 其他门控RNN

门控RNN的单元被称为门控循环单元GRU。与LSTM的主要区别是，单个门控单元同时控制遗忘因子和更新状态单元的决定。

{% asset_img rnn16.jpg %}

$$
h_i^{(t)} = u_i^{(t-1)}h_i^{(t-1)} + (1-u_i^{t-1})\sigma(b_i + \sum_jU_{i,j}x_j^{(t)} + \sum_jW_{i,j}r_j^{(t-1)}h_j^{(t-1)})
$$

其中$\boldsymbol{u}$代表更新门，$\boldsymbol{r}$表示复位门（重置门）

$$
u_i^{(t)} = \sigma(b_i^u + \sum_jU^u_{i,j}x_j^{(t)} + \sum_jW^u_{i,j}h_j^{(t)})
\\
r_i^{(t)} = \sigma(b_i^r + \sum_jU^r_{i,j}x_j^{(t)} + \sum_jW^r_{i,j}h_j^{(t)})
$$

复位和更新门能独立地“忽略”状态向量的一部分。更新门像条件渗漏累积器一样可以线性门控任意维度，从而选择将它复制（在sigmoid的一个极端）或完全由新的“目标状态”值（朝向渗漏累积器的收敛方向）替换并完全忽略它（在另一个极端）。复位门控制当前状态中哪些部分用于计算下一个目标状态，在过去状态和未来状态之间引入了附加的非线性效应。

围绕这一主题可以设计更多的变种。例如复位门（或遗忘门）的输出可以在多个隐藏单元间共享。或者，全局门的乘积（覆盖一整组的单元，例如整一层）和一个局部门（每单元）可用于结合全局控制和局部控制。然而，一些调查发现这些LSTM和GRU架构的变种，在广泛的任务中难以明显地同时击败这两个原始架构。

## 11. 优化长期依赖

二阶导数可能在一阶导数消失的同时消失。二阶优化算法可以大致被理解为将一阶导数除以二阶导数（在更高维数，由梯度乘以Hessian的逆）。如果二阶导数与一阶导数以类似的速率收缩，那么一阶和二阶导数的比率可保持相对恒定。不幸的是，二阶方法有许多缺点，包括高的计算成本、需要一个大的小批量、并且倾向于被吸引到鞍点。

使用较简单的方法可以达到类似的结果，例如经过谨慎初始化的Nesterov动量法。应用于LSTM时，这两种方法在很大程度上会被单纯的 SGD（甚至没有动量）取代。这是机器学习中一个延续的主题，设计一个易于优化模型通常比设计出更加强大的优化算法更容易。

### 11.1 截断梯度

{% asset_img rnn17.png %}

由于RNN中存在$\boldsymbol{h}^{(t)}$的循环，因此在梯度下降时很容易遇到临界值为1的情况，当参数大于1时，梯度下降很大，导致参数更新步伐很大。因此，需要在这样的悬崖使用更小的学习率以中和梯度下降的速度。

一个简单的解决方案是**截断梯度**。一种方式是在参数更新之前，逐元素地截断小批量产生地参数梯度；另一种是在参数更新之前截断梯度$\boldsymbol{g}$地范数$||\boldsymbol{g}||$

$$
if \quad ||\boldsymbol{g}|| > v
\\
\boldsymbol{g} \leftarrow \frac{\boldsymbol{g}v}{||\boldsymbol{g}||}
$$

其中$v$是范数上界，$\boldsymbol{g}$用来更新参数。

如果爆炸非常严重，梯度数值上为$Inf$或$Nan$（无穷大或不是一个数字），则可以采取大小为$v$的随机一步，通常会离开数值不稳定的状态。截断每小批量梯度范数不会改变单个小批量的梯度方向。然而，许多小批量使用范数截断梯度后的平均值不等同于截断真实梯度（使用所有的实例所形成的梯度）的范数。大导数范数的样本，和像这样的出现在同一小批量的样本，其对最终方向的贡献将消失。不像传统小批量梯度下降，其中真实梯度的方向是等于所有小批量梯度的平均。换句话说，传统的随机梯度下降使用梯度的无偏估计，而与使用范数截断的梯度下降引入了经验上是有用的启发式偏置。通过逐元素截断，更新的方向与真实梯度或小批量的梯度不再对齐，但是它仍然是一个下降方向。

### 11.2 引导信息流的正则化

梯度截断有助于处理爆炸的梯度，但它无助于消失的梯度。为了解决消失的梯度问题并更好地捕获长期依赖，我们讨论了如下想法：在展开循环架构的计算图中，沿着与弧边相关联的梯度乘积接近1的部分创建路径。

实现这一点的一种方法是使用LSTM以及其他自循环和门控机制。另一个想法是正则化或约束参数，以引导“信息流”。

特别是即使损失函数只对序列尾部的输出作惩罚，我们也希望梯度向量$\bigtriangledown_{\boldsymbol{h}^{(t)}}L$在反向传播时能维持其幅度。形式上，我们要使

$$
(\bigtriangledown_{\boldsymbol{h}^{(t)}}L)\frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{h}^{(t-1)}}
$$

与

$$
\bigtriangledown_{\boldsymbol{h}^{(t)}}L
$$

一样大。在这个目标下，提出了以下正则项

$$
\Omega = \sum_t(\frac{||(\bigtriangledown_{\boldsymbol{h}^{(t)}}L)\frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{h}^{(t-1)}}||}{||\bigtriangledown_{\boldsymbol{h}^{(t)}}L||} - 1)^2
$$

计算这一梯度的正则项可能会出现困难，可以将后向传播向量$\bigtriangledown_{\boldsymbol{h}^{(t)}}L$考虑为恒值作为近似（为了计算正则化的目的，没有必要通过它们向后传播）。

## 12. 外显记忆

神经网络擅长存储隐性知识，但是他们很难记住事实。被存储在神经网络参数中之前，随机梯度下降需要多次提供相同的输入，即使如此，该输入也不会被特别精确地存储。Graves et al. (2014) 推测这是因为神经网络缺乏工作存储 (workingmemory) 系统，即类似人类为实现一些目标而明确保存和操作相关信息片段的系统。这种外显记忆组件将使我们的系统不仅能够快速“故意”地存储和检索具体的事实，也能利用他们循序推论。神经网络处理序列信息的需要，改变了每个步骤向网络注入输入的方式，长期以来推理能力被认为是重要的，而不是对输入做出自动的、直观的反应。

为了解决这一难题，引入了**记忆网络**，其中包括一组可以通过寻址机制来访问的记忆单元。记忆网络原本需要监督信号指示他们如何使用自己的记忆单元。需要明确的监督指示采取哪些行动而能学习从记忆单元读写任意内容，并通过使用基于内容的软注意机制，允许端到端的训练。这种软寻址机制已成为其他允许基于梯度优化的模拟算法机制的相关架构的标准。

每个记忆单元可以被认为是 LSTM 和 GRU 中记忆单元的扩展。不同的是，网络输出一个内部状态来选择从哪个单元读取或写入，正如数字计算机读取或写入到特定地址的内存访问。

{% asset_img rnn18.png %}

无论是软（允许反向传播）或随机硬性的，用于选择一个地址的机制与先前在机器翻译的背景下引入的注意力机制形式相同。甚至更早之前，注意力机制的想法就被引入了神经网络，在手写生成的情况下，有一个被约束为通过序列只向前移动的注意力机制。在机器翻译和记忆网络的情况下，每个步骤中关注的焦点可以移动到一个完全不同的地方 (相比之前的步骤)。