---
title: 支持向量机
date: 2018-11-07 17:39:31
categories:
- Machine Learning
tags:
- Theory
- SVM
- SVR
mathjax: true
---

**Super!!!!**

参考：

> 西瓜书第6章 支持向量机

## 1. 间隔与支持向量

给定训练样本集$D = \{ (\boldsymbol{x}_1, y_1), (\boldsymbol{x}_2, y_2),...,(\boldsymbol{x}_m, y_m) \}, y_i \in \{ -1, +1 \}$，显然这是一个二分类问题，支持向量机的设想是在训练集$D$的样本空间中找到一个划分超平面，将不同类别的样本分开。显然，在空间中这样的划分超平面有无穷多，我们需要寻找到最合适的那一个。

{% asset_img svm0.png 多个划分超平面 %}

从直觉上我们直到，“正中间”的划分超平面应该会是很合适的，因为处于正中间的划分超平面距离两种分类的样本的距离是最远的，因此用作预测时鲁棒性很好，泛化能力强。如何定义这样的超平面呢？

<!-- more -->

在样本空间中，划分超平面可以定义为

$$
\boldsymbol{w}^T\boldsymbol{x} + b=0
$$

其中$\boldsymbol{w}=(w_1;w_2;...;w_d)$为法向量，决定了超平面的方向；$b$为位移项，决定超平面与原点之间的距离。那么样本空间中任意点$\boldsymbol{x}$到超平面$(\boldsymbol{w},b)$的距离为

$$
r = \frac{|\boldsymbol{w}^T\boldsymbol{x}+b|}{||\boldsymbol{w}||}
$$

假设超平面$(\boldsymbol{w}, b)$能将训练样本正确分类，即对于$(\boldsymbol{x}_i, y_i) \in D$，若$y_i = +1$，则有$\boldsymbol{w}^T\boldsymbol{x}_i +b > 0$；若$y_i = -1$，则有$\boldsymbol{w}^T\boldsymbol{x}_i +b < 0$。令

$$
\left\{\begin{matrix}
\boldsymbol{w}^T\boldsymbol{x}_i + b \geqslant +1, \quad y_i = +1 \\ 
\boldsymbol{w}^T\boldsymbol{x}_i + b \leqslant -1, \quad y_i = -1
\end{matrix}\right.
$$

这里有两个问题：

1. 为什么不等式可以映射到$\pm 1$，而且为什么不是其他数字；
2. 为什么划分超平面必须是“正中间”，而不能对一种类别有偏好，即两个不等式右边的数字绝对值不等。

* 第一个问题的回答，由于左右两边可以同时缩放，因此可以缩放到为$\pm 1$的条件，这是为了后面计算更加方便；
* 均等分割也是为了后面公式计算更加方便，至于偏好，我想应该有其它方法基于SVM进行了拓展实现呃不均等分割。

{% asset_img svm1.jpg 支持向量与间隔 %}

如图所示，距离超平面最近的几个训练样本可以使等号成立，它们被称为支持向量，两个异类支持向量到超平面的距离之和为

$$
\gamma = \frac{2}{||\boldsymbol{w}||}
$$

$\gamma$被称为间隔，双竖线表示$\boldsymbol{w}$的各成分的平方和再开方，若里面是两个向量，物理意义表示两个向量的欧拉距离。

我们的目标是找到最大间隔，即

$$
\underset{\boldsymbol{w}, b}{\max} \frac{2}{||\boldsymbol{w}||}
\\
s.t. \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \geqslant 1, \quad i = 1,2,...,m
$$

最大化$||\boldsymbol{w}||^{-1}$等效于最小化$||\boldsymbol{w}||^2$，加平方是为了抵消开方，简化后面的计算，于是可以重写为

$$
\underset{\boldsymbol{w},b}{\min} \frac{1}{2} ||\boldsymbol{w}||^2
\\
s.t. \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \geqslant 1, \quad i = 1,2,...,m
$$

这就是支持向量机（SVM）的基本型。

## 2. 对偶问题

我们希望求解上式得到最大间隔划分超平面所对应的模型

$$
f(\boldsymbol{x}) = \boldsymbol{w}^T\boldsymbol{x} + b
$$

这个问题可以使用拉格朗日乘子法求解，首先对约束条件添加拉格朗日乘子$\alpha_i \geqslant 0$

$$
L(\boldsymbol{w}, b, \boldsymbol{\alpha}) = \frac{1}{2}||\boldsymbol{w}||^2 + \sum^m_{i=1} \alpha(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))
$$

其中$\boldsymbol{\alpha} = (\alpha_1;\alpha_2;...;\alpha_m)$，令$L(\boldsymbol{w},b,\boldsymbol{\alpha})$对$\boldsymbol{w}$和$b$的偏导数为0可得

$$
\boldsymbol{w} = \sum^m_{i=1} \alpha_iy_i\boldsymbol{x}_i
\\
0=\sum^m_{i=1}\alpha_iy_i
$$

注意$||\boldsymbol{w}||^2 = \boldsymbol{w}^T\boldsymbol{w}$。

将上式代入到$L$中消去$\boldsymbol{w}$和$b$，我们就得到基本型的对偶问题

$$
\underset{\boldsymbol{\alpha}}{\max} \sum^m_{i=1}\alpha_i - \frac{1}{2} \sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j
\\
s.t. \quad \sum^m_{i=1}\alpha_iy_i = 0,
\\
\alpha_i \geqslant 0, \quad i = 1,2,...,m
$$

对偶问题怎么得到的，我们可以看作对于$L$有三个参数$(\boldsymbol{w},b,\boldsymbol{\alpha})$，我们通过拉格朗日法求解出其中两个参数的表示形式$(\boldsymbol{w},b)$，那么$L$就只与$\boldsymbol{\alpha}$相关，与$\boldsymbol{\alpha}$相关的部分是$\sum^m_{i=1} \alpha(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))$，而由于约束条件可知$1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \leqslant 0$，所以系数$\alpha_i$越大，$L$越小，所以我们就得到了对偶问题。

如果我们求解出$\boldsymbol{\alpha}$后，再解出$(\boldsymbol{w},b)$，我们的模型就可以确定了

$$
f(\boldsymbol{x}) = \boldsymbol{w}^T\boldsymbol{x} + b
\\
= \sum^m_{i=1} \alpha_iy_i\boldsymbol{x}_i^T\boldsymbol{x} +b
$$

---

下面我们研究如何求解出$\boldsymbol{\alpha}$，每一个拉格朗日乘子$\alpha_i$对应一个样本$(\boldsymbol{x}_i, y_i)$。由于基本型的约束条件，所以求解目标满足KKT条件

$$
\left\{\begin{matrix}
\alpha_i \geqslant 0 \\ 
y_if(\boldsymbol{x}_i)-1 \geqslant 0 \\ 
\alpha_i(y_if(\boldsymbol{x}_i)-1) = 0
\end{matrix}\right.
$$

对KKT的简单认知为：不等式约束条件与拉格朗日乘子的乘积等于0，且这个条件决定了训练样本的分布位置，若$\alpha_i > 0$，则必有$y_if(\boldsymbol{x}_i)=1$，这样的点是支持向量；若$\alpha_i = 0$，则该样本不会在模型表达式求和中出现，该点的位置属于正常分类，但是不在间隔边界上。

**SVM性质：训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。**

基于上述约束条件，为了求解，使用SMO算法，其基本思路是先固定$\alpha_i$之外的所有参数，然后求$\alpha_i$上的极值，由于存在约束条件$\sum^m_{i=1}\alpha_iy_i = 0$，一个参数可以由其他固定的参数导出，所以需要选择两个变量，固定其他的参数。

于是，SMO每次选择两个变量$\alpha_i$和$\alpha_j$，并固定其他参数。在参数初始化后，不断执行如下两个步骤直至收敛：

* 选取一对需要更新的变量$\alpha_i$和$\alpha_j$；
* 固定$\alpha_i$和$\alpha_j$以外的参数，求解$\max \alpha_i + \alpha_j - \lambda \alpha_i \alpha_j$的方程获得更新后的$\alpha_i$和$\alpha_j$（这里忽略掉其他常数）。

很容易发现，在参数更新过程中$\max$目标函数会越来越大，而且倘若选取的两个参数$(\alpha_i, \alpha_j)$中只要有一个不满足KKT条件，那么目标函数增幅会更大，而且KKT条件违背程度越大，增幅越大。于是我们可以考虑先选取违背KKT条件的参数$\alpha$进行更新，具体实现是选取两变量对应的样本之间的间隔最大。

我们选取$(\alpha_i, \alpha_j)$，约束条件重写为

$$
\alpha_iy_i + \alpha_jy_j = c, \quad \alpha_i \geqslant 0, \alpha_j \geqslant 0
$$

其中

$$
c = -\sum_{k \neq i,j}\alpha_ky_k
$$

而且$\max$可以化简为$\alpha_i + \alpha_j - \lambda \alpha_i \alpha_j$，这就是一个简单的二次规划问题，直接公式求解。

如何确定偏移项$b$，因为对任意支持向量$(\boldsymbol{x}_s, y_s)$有$y_sf(\boldsymbol{x}_s)=1$，即

$$
y_s(\sum_{i\in S}\alpha_iy_i\boldsymbol{x}^T_i\boldsymbol{x}_s +b) =1
$$

$S$为所有支持向量下标集，理论上$(\boldsymbol{x}_s, y_s)$可以取任意支持向量来求解$b$，这样会得到很多组$b$，实际上采用均值实现更鲁棒

$$
b= \frac{1}{|S|}\sum_{s\in S}(\frac{1}{y_s} - \sum_{i\in S}\alpha_iy_i\boldsymbol{x}^T_i\boldsymbol{x}_s)
$$

## 3. 核函数

{% asset_img svm2.jpg 映射到更高维度 %}

上面在求解SVM时，我们默认数据样本在其样本空间是可分的，但是实际上由于维度过高，我们并不清楚数据是否存在这样的超平面，这时，我们可以将数据从原始样本空间映射到一个更高维的特征空间（再生核希尔伯特空间），使得样本在这个特征空间内可分。研究表明，只要原始空间维度有限，那么必定存在一个更高维的空间是样本可分。

令$\phi(\boldsymbol{x})$表示映射后的特征向量，于是划分超平面模型为

$$
f(\boldsymbol{x}) = \boldsymbol{w}^T \phi(\boldsymbol{x}) + b
$$

同理修改约束条件和SVM基本型，那么对偶问题变成了

$$
\underset{\boldsymbol{\alpha}}{\max} \sum^m_{i=1}\alpha_i - \frac{1}{2} \sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_j\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)
\\
s.t. \quad \sum^m_{i=1}\alpha_iy_i = 0,
\\
\alpha_i \geqslant 0, \quad i = 1,2,...,m
$$

但是由于直接计算高维$\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)$，通常计算量大而且很困难，为了解决这个问题，设想一个函数：

$$
\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j) = \left \langle \phi(\boldsymbol{x}_i), \phi(\boldsymbol{x}_j) \right \rangle = \phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)
$$

也就是说我们使用原始空间的函数计算特征空间的内积，感觉很奇妙。重写上面各种式子，得到模型

$$
f(\boldsymbol{x}) = \boldsymbol{w}^T \phi(\boldsymbol{x}) + b
\\
= \sum^m_{i=1}\alpha_iy_i\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}) + b
\\
= \sum^m_{i=1}\alpha_iy_i\kappa(\boldsymbol{x},\boldsymbol{x}_i) + b
$$

这里$\kappa(\cdot, \cdot)$就是核函数。显然，若已知映射$\phi(\cdot)$的具体形式，那么可以写出核函数$\kappa(\cdot, \cdot)$，但是实际问题中我们很难知道高维映射$\phi(\cdot)$的具体形式，如何寻找合适的核函数呢？有定理：

$\kappa(\cdot, \cdot)$是定义在$\chi \times \chi$（$\chi$为输入空间）上的对称函数，则$\kappa$是核函数，且对于任意数据$D=\{ \boldsymbol{x}_1, \boldsymbol{x}_2,...,\boldsymbol{x}_m \}$，核矩阵$\boldsymbol{K}$总是半正定：

$$
\boldsymbol{K}_{i,j} = \kappa(\boldsymbol{x}_i, \boldsymbol{x}_j)
$$

*半正定：设$A$是$n$阶方阵，如果对任何非零向量$X$，都有$X^TAX \geqslant 0$，其中$X^T$表示$X$的转置，就称$A$为半正定矩阵。*

**所以只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。**

{% asset_img svm3.png 常用核函数 %}

此外，函数组合也可以得到，若$\kappa_1,\kappa_2$为核函数，例如：

* 对于任意正数$\gamma_1, \gamma_2$，线性组合：
$$
\gamma_1\kappa_1 + \gamma_2\kappa_2
$$
* 核函数直积：
$$
\kappa_1(\boldsymbol{x}, \boldsymbol{z})\kappa_2(\boldsymbol{x}, \boldsymbol{z})
$$
* 任意函数$g(\boldsymbol{x})$：
$$
g(\boldsymbol{x})\kappa_1(\boldsymbol{x}, \boldsymbol{z})g(\boldsymbol{z})
$$
都是核函数。

## 4. 软间隔与正则化

在上面的讨论中，我们始终假设划分超平面是存在的，实际上往往很难确定一个核函数使得样本线性可分，在此基础上我们可以适当放宽间隔的条件，允许一定的样本出错，为此，引入软间隔概念。

{% asset_img svm4.png 软间隔示意 %}

因此某些样本不满足约束

$$
y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \geqslant 1
$$

那么优化目标修正为

$$
\underset{\boldsymbol{w},b}{\min} \frac{1}{2} ||\boldsymbol{w}||^2 + C \sum^m_{i=1}l_{0/1}(y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) -1)
$$

其中$C > 0$是一个常数，通过$C$的取值大小控制对错误样本的容忍度，$C$越大，对错误样本的容忍度越小，对约束条件越严格，当$C$取有限值，允许一些样本不满足约束。$l_{0/1}$是“0/1损失函数”

$$
l_{0/1}(z) = \left\{\begin{matrix}
1, \quad if \quad z < 0  \\ 
0, \quad otherwise
\end{matrix}\right.
$$

然而$l_{0/1}$非凸、非连续，数学性质不太好，实际中常用其他函数替代，称为替代损失函数，它们通常是凸的连续函数且是$l_{0/1}$的上界。三种常用的替代损失函数：

1. hinge损失：$l_{hinge}(z) = max(0, 1-z)$
2. 指数损失：$l_{exp}(z) = exp(-z)$
3. 对率损失：$l_{log}(z) = log(1+exp(-z))$

{% asset_img svm5.png 常见替代损失函数 %}

若要替换，则$z = y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)$。若引入松弛变量$\xi_i \geqslant 0$，则优化目标重写为

$$
\underset{\boldsymbol{w},b,\xi_i}{\min} \frac{1}{2}||\boldsymbol{w}||^2 + C\sum^m_{i=1}\xi_i
\\
s.t. \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b) \geqslant 1 - \xi_i
\\
\xi_i \geqslant 0, \quad i = 1,2,...,m
$$

这就是常用的软间隔支持向量机。

显然，每一个样本对应一个松弛变量，用以表征该样本不满足约束的程度。于是同样拉格朗日法得到

$$
L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi}, \boldsymbol{\mu}) = \frac{1}{2}||\boldsymbol{w}||^2 + C\sum^m_{i=1}\xi_i 
\\
+ \sum^m_{i=1}\alpha_i(1-\xi_i -y_i(\boldsymbol{w}^T\boldsymbol{x}_i + b)) - \sum^m_{i=1}\mu_i\xi_i
$$

其中拉格朗日乘子$\alpha_i \geqslant 0, \mu_i \geqslant 0$。

令$L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi}, \boldsymbol{\mu})$对$(\boldsymbol{w}, b,\xi_i)$偏导数为0可得

$$
\boldsymbol{w} = \sum^m_{i=1}\alpha_iy_i\boldsymbol{x}_i
\\
0 = \sum^m_{i=1}\alpha_iy_i
\\
C = \alpha_i + \mu_i
$$

同样得到对偶问题

$$
\underset{\boldsymbol{\alpha}}{\max} \sum^m_{i=1}\alpha_i - \frac{1}{2} \sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j
\\
s.t. \quad \sum^m_{i=1}\alpha_iy_i = 0,
\\
0 \leqslant \alpha_i \leqslant C, \quad i = 1,2,...,m
$$

显然，这与最开始的对偶问题唯一的区别是$\alpha_i$的范围。

类似的KKT条件为

$$
\left\{\begin{matrix}
\alpha_i \geqslant 0 , \quad \mu_i \geqslant 0\\ 
y_if(\boldsymbol{x}_i)-1+ \xi_i \geqslant 0 \\ 
\alpha_i(y_if(\boldsymbol{x}_i)-1+ \xi_i) = 0 \\
\xi_i \geqslant 0, \quad \mu_i\xi_i = 0
\end{matrix}\right. 
$$

对任意训练样本$(\boldsymbol{x}_i, y_i)$，总有$\alpha_i = 0$或$y_if(\boldsymbol{x}_i) = 1 - \xi_i$，而且：

* 若$\alpha_i=0$，则该样本不会对$f(\boldsymbol{x})$有任何影响；
* 若$\alpha_i > 0$，则必有$y_if(\boldsymbol{x}_i) = 1 - \xi_i$，即支持向量；
* 若$\alpha_i < C$，则$\mu_i > 0$，进而有$\xi_i=0$，即该样本恰在最大间隔边界上；
* 若$\alpha_i = C$，则有$\mu_i = 0$，此时若$\xi_i \leqslant 1$则该样本落在最大间隔内部，若$\xi_i > 1$则样本被错误分类。

由此看出，软间隔支持向量机的最终模型仅与支持向量有关，即通过采用hinge损失函数仍保持了稀疏性。

使用对率损失函数$l_{log}$替换，几乎得到了对率回归模型。

将优化目标写为一般形式

$$
\underset{f}{\min} \Omega(f) + C \sum^m_{i=1}l(f(\boldsymbol{x}_i), y_i)
$$

其中$\Omega(f)$称为结构风险，用于描述模型$f$的某些性质；第二项$\sum^m_{i=1}l(f(\boldsymbol{x}_i), y_i)$称为经验风险，用于描述模型与训练数据的契合程度；$C$对二者进行折中。从正则化角度看，$\Omega(f)$称为正则化项，$C$称为正则化常数。也就意味着可以使用$L_p$范数进行调整，$L_2$范数倾向于$\boldsymbol{w}$的分量取值尽量均衡，即非零分量个数尽量稠密，而$L_0$或$L_1$范数则倾向与$\boldsymbol{w}$的分量尽量稀疏，即非零分量个数尽量少。

## 5. 支持向量回归

支持向量机用于回归问题，希望学得的模型$f(\boldsymbol{x})$与$y$尽可能接近。

传统回归模型直接基于模型输出$f(\boldsymbol{x})$与真实输出$y$之间的差别来计算损失，当且仅当两者完全相同时，损失才为0.与此不同，支持向量回归SVR允许我们容忍$f(\boldsymbol{x})$与$y$之间最多有$\epsilon$的偏差，仅当两者之间的差别绝对值大于$\epsilon$时才计算损失。这相当于以$f(\boldsymbol{x})$为中心构建了一个宽度为$2\epsilon$的间隔带，只有落入间隔带的样本被认为是预测正确的。

{% asset_img svr0.png 支持向量回归 %}

于是SVR问题可形式化为

$$
\underset{\boldsymbol{w},b}{\min} \frac{1}{2}||\boldsymbol{w}||^2 + C\sum^m_{i=1}l_{\epsilon}(f(\boldsymbol{x}_i)-y_i)
$$

其中$C$为正则化常数，$l_\epsilon$为$\epsilon$-不敏感损失函数

$$
l_\epsilon(z) = \left\{\begin{matrix}
0, \quad if |z| \leqslant \epsilon \\ 
|z| -\epsilon , \quad otherwise
\end{matrix}\right.
$$

{% asset_img svr1.png 不敏感损失函数 %}

引入松弛变量$\xi_i$和$\hat{\xi}_i$，重写上式为

$$
\underset{\boldsymbol{w},b,\xi_i, \hat{\xi}_i}{\min} \frac{1}{2}||\boldsymbol{w}||^2 + C\sum^m_{i=1}(\xi_i + \hat{\xi}_i)
\\
s.t. \quad f(\boldsymbol{x}_i) - y_i \leqslant \epsilon + \xi_i
\\
y_i - f(\boldsymbol{x}_i) \leqslant \epsilon + \hat{\xi}_i
\\
\xi_i \geqslant 0, \hat{\xi}_i \geqslant 0, \quad i = 1,2,...,m
$$

同理引入拉格朗日乘子$(\mu_i, \hat{\mu}_i, \alpha_i, \hat{\alpha}_i) \geqslant 0$

$$
L(\boldsymbol{w},b,\boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}, \boldsymbol{\xi}, \hat{\boldsymbol{\xi}},\boldsymbol{\mu}, \hat{\boldsymbol{\mu}})
\\
= \frac{1}{2}||\boldsymbol{w}||^2 + C\sum^m_{i=1}(\xi_i+\hat{\xi}_i) - \sum^m_{i=1}\mu_i\xi_i - \sum^m_{i=1}\hat{\mu}_i\hat{\xi}_i
\\
+ \sum^m_{i=1}\alpha_i(f(\boldsymbol{x}_i) -y_i -\epsilon -\xi_i) + \sum^m_{i=1}\hat{\alpha}_i(y_i - f(\boldsymbol{x}_i)-\epsilon -\hat{\xi}_i)
$$

令$L(\boldsymbol{w},b,\boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}, \boldsymbol{\xi}, \hat{\boldsymbol{\xi}},\boldsymbol{\mu}, \hat{\boldsymbol{\mu}})$对$\boldsymbol{w},b,\xi_i,\hat{\xi}_i$偏导数为0

$$
\boldsymbol{w} = \sum^m_{i=1}(\hat{\alpha}_i - \alpha_i)\boldsymbol{x}_i
\\
0 = \sum^m_{i=1}(\hat{\alpha}_i - \alpha_i)
\\
C = \alpha_i + \mu_i
\\
C = \hat{\alpha}_i + \hat{\alpha}_i
$$

代入，得到SVR对偶问题

$$
\underset{\boldsymbol{\alpha}, \hat{\boldsymbol{\alpha}}}{\max} \sum^m_{i=1}y_i(\hat{\alpha}_i - \alpha_i) - \epsilon(\hat{\alpha}_i + \alpha_i)
\\
- \frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}(\hat{\alpha}_i - \alpha_i)(\hat{\alpha}_j - \alpha_j)\boldsymbol{x}_i^T\boldsymbol{x}_j
\\
s.t. \quad \sum^m_{i=1}(\hat{\alpha}_i - \alpha_i) = 0
\\
0 \leqslant \alpha_i, \hat{\alpha}_i \leqslant C
$$

上述过程需满足KKT条件，即

$$
\left\{\begin{matrix}
\alpha_i(f(\boldsymbol{x}_i)-y_i-\epsilon-\xi_i) = 0 \\ 
\hat{\alpha}_i(y_i - f(\boldsymbol{x}_i)-\epsilon-\xi_i) = 0 \\ 
\alpha_i\hat{\alpha}_i = 0, \xi_i\hat{\xi}_i = 0\\ 
(C - \alpha_i) \xi_i = 0,(C-\hat{\alpha}_i)\hat{\xi}_i = 0
\end{matrix}\right.
$$

由上式可知，当且仅当样本$(\boldsymbol{x}_i,y_i)$不落入$\epsilon$-间隔带中，相应的$\alpha_i$和$\hat{\alpha}_i$才能取非零值。此外约束$f(\boldsymbol{x}_i)-y_i-\epsilon-\xi_i = 0$和$y_i - f(\boldsymbol{x}_i)-\epsilon-\xi_i = 0$不能同时成立，因此$\alpha_i$和$\hat{\alpha}_i$中至少有一个为0。

同理，SVR的解形如

$$
f(\boldsymbol{x}) = \sum^m_{i=1}(\hat{\alpha}_i - \alpha_i)\boldsymbol{x}_i^T\boldsymbol{x} + b
$$

其中使$(\hat{\alpha}_i - \alpha_i) \neq 0$的样本即为SVR的支持向量，它们必定落在间隔带之外。显然，也是稀疏解。

由KKT条件可知，对每个样本$(\boldsymbol{x}_i,y_i)$都有$(C - \alpha_i) \xi_i = 0$且$\alpha_i(f(\boldsymbol{x}_i)-y_i-\epsilon-\xi_i) = 0$。于是，在得到$\alpha_i$后，若$0 < \alpha_i < C$，则必有$\xi_i = 0$，进而有

$$
b = y_i + \epsilon - \sum^m_{j=1}(\hat{\alpha}_j - \alpha_j)\boldsymbol{x}_j^T\boldsymbol{x}_i
$$

理论上对于任意满足$0<\alpha_i<C$的样本都可以求得$b$。也就是多组解，实践中通常选取多个或所有满足的样本求解平均值，参考SVM。

若考虑特征映射，则

$$
\boldsymbol{w} = \sum^m_{i=1}(\hat{\alpha}_i - \alpha_i)\phi(\boldsymbol{x}_i)
$$

同理，带核函数的SVR表示形式改变为

$$
f(\boldsymbol{x}) = \sum^m_{i=1}(\hat{\alpha}_i - \alpha_i)\kappa(\boldsymbol{x}, \boldsymbol{x}_i) + b
$$

## 6. 核方法

观察SVM和SVR的模型表示形式（考虑核函数），若不考虑偏移项$b$，最终模型总能表达成核函数的线性组合。一个更一般的结论称为表示定理：

> **表示定理** 令$\mathbb{H}$为核函数$\kappa$对应的再生核希尔伯特空间，$||h||_\mathbb{H}$表示$\mathbb{H}$空间中关于$h$的范数，对于任意单调递增函数$\Omega:[0, \infty] \rightarrow \mathbb{R}$和任意非负损失函数$l:\mathbb{R}^m \rightarrow [0, \infty]$，优化问题

$$
\underset{h\in \mathbb{H}}{\min} F(h) = \Omega(||h||_\mathbb{H}) + l(h(\boldsymbol{x}_1), h(\boldsymbol{x}_2),...,h(\boldsymbol{x}_m))
$$

的解总可写为

$$
h^*(\boldsymbol{x}) = \sum^m_{i=1}\alpha_i \kappa(\boldsymbol{x}, \boldsymbol{x}_i)
$$

表示定理对损失函数没有限制，对正则化项$\Omega$仅要求单调递增，甚至不要求$\Omega$为凸函数，这意味着对于一般的损失函数和正则化项，优化问题的最优解都可以表示为核函数的线性组合。

---

下面介绍通过引入核函数拓展[线性判别分析LDA](http://zhoutao822.coding.me/2018/11/07/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/)为非线性学习器，从而得到核线性判别分析KLDA。

先假设有映射$\phi:\chi \rightarrow \mathbb{F}$将样本映射到一个特征空间，然后在$\mathbb{F}$中执行线性判别分析，以求得

$$
h(\boldsymbol{x}) = \boldsymbol{w}^T\phi(\boldsymbol{x})
$$

类似LDA的类内散度矩阵$\boldsymbol{S}^\phi_b$和类间散度矩阵$\boldsymbol{S}^\phi_w$，KLDA的学习目标是

$$
\underset{\boldsymbol{w}}{\max} J(\boldsymbol{w}) = \frac{\boldsymbol{w}^T\boldsymbol{S}^{\phi}_b\boldsymbol{w}}{\boldsymbol{w}^T\boldsymbol{S}^{\phi}_w\boldsymbol{w}}
$$

令$X_i$表示第$i \in \{ 0,1 \}$类样本的集合，其样本数为$m_i$，样本总数为$m = m_0+m_1$。第$i$类样本在特征空间$\mathbb{F}$中的均值为

$$
\boldsymbol{\mu}^\phi_i = \frac{1}{m_i}\sum_{\boldsymbol{x}\in X_i}\phi(\boldsymbol{x})
$$

两个散度矩阵分别为

$$
\boldsymbol{S}_b^\phi = (\boldsymbol{\mu}^\phi_1 - \boldsymbol{\mu}^\phi_0)(\boldsymbol{\mu}^\phi_1 - \boldsymbol{\mu}^\phi_0)^T
\\
\boldsymbol{S}_w^\phi = \sum^1_{i=0}\sum_{\boldsymbol{x}\in X_i}(\phi(\boldsymbol{x})-\boldsymbol{\mu}^\phi_i)(\phi(\boldsymbol{x})-\boldsymbol{\mu}^\phi_i)^T
$$

同理使用核函数$\kappa(\boldsymbol{x}, \boldsymbol{x}_i) = \phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x})$隐式表达映射$\phi$和特征空间$\mathbb{F}$。把$J(\boldsymbol{w})$作为损失函数$l$，再令$\Omega \equiv 0$，由表示定理，函数$h(\boldsymbol{x})$可写为

$$
h(\boldsymbol{x}) = \sum^m_{i=1}\alpha_i \kappa(\boldsymbol{x}, \boldsymbol{x}_i)
\\
\boldsymbol{w} = \sum^m_{i=1}\alpha_i\phi(\boldsymbol{x}_i)
$$

令$\boldsymbol{K} \in \mathbb{R}^{m \times m}$为核函数$\kappa$对应的核矩阵，$(\boldsymbol{K})_{ij} = \kappa(\boldsymbol{x}_i, \boldsymbol{x}_j)$。令$\boldsymbol{1}_i \in \{ 1,0 \}^{m \times 1}$为第$i$类样本的指示向量，即$\boldsymbol{1}_i$的第$j$个分量为1当且仅当$\boldsymbol{x}_j \in X_i$，否则第$j$个分量为0。再令

$$
\boldsymbol{\hat{\mu}}_0 = \frac{1}{m_0}\boldsymbol{K}\boldsymbol{1}_0
\\
\boldsymbol{\hat{\mu}}_1 = \frac{1}{m_1}\boldsymbol{K}\boldsymbol{1}_1
\\
\boldsymbol{M} = (\boldsymbol{\hat{\mu}}_0 - \boldsymbol{\hat{\mu}}_1)(\boldsymbol{\hat{\mu}}_0 - \boldsymbol{\hat{\mu}}_1)^T
\\
\boldsymbol{N} = \boldsymbol{K}\boldsymbol{K}^T - \sum^1_{i=0}m_i\boldsymbol{\hat{\mu}}_i\boldsymbol{\hat{\mu}}_i^T
$$

于是$\max$等价为

$$
\underset{\boldsymbol{\alpha}}{\max} J(\boldsymbol{\alpha}) = \frac{\boldsymbol{\alpha}^T\boldsymbol{M}\boldsymbol{\alpha}}{\boldsymbol{\alpha}^T\boldsymbol{N}\boldsymbol{\alpha}}
$$

根据上式再是同线性判别分析求解方法即可得到$\boldsymbol{\alpha}$，进而得到投影函数$h(\boldsymbol{x})$，具体参考[`线性模型`](http://zhoutao822.coding.me/2018/11/07/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/)。
