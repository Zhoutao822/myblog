---
title: 神经网络
date: 2018-11-07 17:37:59
categories:
- deep learning
tags:
- theory
- neural network
mathjax: true
---

> 神经网络可太懒了

参考：西瓜书第5章 神经网络

## 1. 神经元模型

学过一点高中生物知识的我们都知道，对于有机生物来说，都是通过神经来控制躯体或者思维，电信号在神经上传导，并且要经过神经元的控制，只有被激活的神经元才能继续传递电信号。神经网络模型就类似于这种形式，在这个模型中，神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将于神经元的阈值进行比较，然后通过激活函数处理以产生神经元的输出。

{% asset_img mp.png M-P神经元模型 %}

理想视为激活函数应该是阶跃函数，只有“0/1”状态，但是阶跃函数不连续、不光滑，因此实际常用Sigmoid函数作为激活函数。

{% asset_img activate.png 典型的神经元激活函数%}

把这些神经元按一定层次结构连接起来就得到神经网络。

## 2. 感知机与多层网络

感知机由两层神经元组成，输入层接收外界信号传递给输出层，输出层是M-P神经元，感知机能容易地实现逻辑与、或、非。

{% asset_img perceptron.jpg 感知机%}

一般的，给定训练数据集，权重$w_i(i=1,2,...,n)$以及阈值$\theta$可以通过学习得到。若将$\theta$看作一个固定输入，对训练样本$(\mathbb{x}, y)$，若当前感知机的输出为$\hat{y}$，则感知机权重调整：

$$
wi \leftarrow w_i + \Delta w_i
\\
\Delta w_i = \eta(y - \hat{y})x_i
$$

其中$\eta \in (0, 1)$称为学习率。若预测正确，则感知机不会变化，否则根据错误程度进行权重调整。感知机的求解形式非常类似线性模型中的分类，但是感知机无法实现非线性的问题求解，比如异或。

因此在输入层与输出层之间增加隐含层（至少为1），隐含层和输出层神经元都是拥有激活函数的功能神经元，这样就能对非线性问题进行拟合。

{% asset_img nn.jpg 多层前馈神经网络%}

## 3. 误差逆传播算法BP

给定训练集$D = \{ (\mathbb{x}_1, \mathbb{y}_1) , (\mathbb{x}_2, \mathbb{y}_2),...,(\mathbb{x}_m, \mathbb{y}_m)\}$，$\mathbb{x}_i \in \mathbb{R}^d ，\mathbb{y}_i \in \mathbb{R}^l$，即输入示例由$d$个属性描述，输出$l$维实值变量。

{% asset_img bp.jpg 多层前馈神经网络%}

如图神经网络结构：

* $d$个输入神经元，$l$个输出神经元，$q$个隐层神经元；
* 输出层第$j$个神经元的阈值用$\theta_j$表示，隐层第$h$个神经元的阈值用$\gamma_h$表示；
* 输入层第$i$个神经元与隐层第$h$个神经元之间的连接权为$v_{ih}$，隐层第$h$个神经元与输出层第$j$个神经元之间的连接权为$w_{hj}$；
* 隐层第$h$个神经元接收到的输入为$\alpha_h = \sum^d_{i=1}v_{ih}x_i$，输出层第$j$个神经元接收到的输入为$\beta_j = \sum^q_{h=1}w_{hj}b_h$，其中$b_h$为隐层第$h$个神经元的输出。假设激活函数均为Sigmoid。

---
对训练例$(\mathbb{x}_k, \mathbb{y}_k)$，假定神经网络的输出为$\hat{\mathbb{y}}_k = (\hat{y}^k_1,\hat{y}^k_2,...,\hat{y}^k_l)$，即

$$
\hat{y}^k_j = f(\beta_j-\theta_j)
$$
则网络在$(\mathbb{x}_k, \mathbb{y}_k)$上的均方误差MSE为

$$
E_k = \frac{1}{2}\sum^l_{j=1}(\hat{y}^k_j - y^k_j)^2，这里除以2是为了后面求导更简洁
$$

那么上图中的神经网络一共有$(d + l + 1)q + l$个参数需要确定：

* 输入层到隐层的$d \times q$个权值；
* 隐层到输出层的$q \times l$个权值；
* $q$个隐层神经元的阈值；
* $l$个输出层神经元的阈值。

同感知机的参数更新方式，对神经网络中的任意参数$v$：

$$
v \leftarrow v + \Delta v
$$

---
以隐层到输出层的连接权$w_{hj}$为例进行推导。BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整。对误差$E_k$，给定学习率$\eta$，有

$$
\Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}}
$$

根据链式法则有

$$
\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y}^k_j} \cdot \frac{\partial \hat{y}^k_j}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial w_{hj}}
$$

根据$\beta_j$的定义，显然有

$$
\frac{\partial \beta_j}{\partial w_{hj}} = b_h
$$

又因为Sigmoid函数有一个很好的性质：

$$
f'(x) = f(x) (1-f(x))
$$

于是令

$$
g_j = - \frac{\partial E_k}{\partial \hat{y}^k_j} \cdot \frac{\partial \hat{y}^k_j}{\partial \beta_j}
\\
= -(\hat{y}^k_j - y^k_j)f'(\beta_j - \theta_j)
\\
= \hat{y}^k_j(1- \hat{y}^k_j)(y^k_j - \hat{y}^k_j)
$$

代入上式，关于$w_{hj}$的更新公式

$$
\Delta w_{hj} = \eta g_j b_h
$$

类似可得

$$
\Delta \theta_j = - \eta g_j
\\
\Delta v_{ih} = \eta e_h x_i
\\
\Delta \gamma_h = - \eta e_h
$$

其中
$$
e_h = - \frac{\partial E_k}{\partial b_h} \cdot \frac{\partial b_h}{\partial \alpha_h}
\\
= - \sum^l_{j=1}\frac{\partial E_k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} f'(\alpha_h - \gamma_h)
\\
= \sum^l_{j=1}w_{hj}g_jf'(\alpha_h - \gamma_h)
\\
= b_h(1 - b_h)\sum^l_{j=1}w_{hj}g_j
$$

学习率$\eta$控制算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛过慢。

## 4. 全局最小与局部最小


## 5. 其他常见神经网络


## 6. 深度学习


