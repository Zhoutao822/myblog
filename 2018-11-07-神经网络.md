---
title: 神经网络
date: 2018-11-07 17:37:59
categories:
- deep learning
tags:
- theory
- neural network
mathjax: true
---

> 神经网络可太懒了

参考：西瓜书第5章 神经网络

## 1. 神经元模型

学过一点高中生物知识的我们都知道，对于有机生物来说，都是通过神经来控制躯体或者思维，电信号在神经上传导，并且要经过神经元的控制，只有被激活的神经元才能继续传递电信号。神经网络模型就类似于这种形式，在这个模型中，神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将于神经元的阈值进行比较，然后通过激活函数处理以产生神经元的输出。

{% asset_img mp.png M-P神经元模型 %}

理想视为激活函数应该是阶跃函数，只有“0/1”状态，但是阶跃函数不连续、不光滑，因此实际常用Sigmoid函数作为激活函数。

{% asset_img activate.png 典型的神经元激活函数%}

把这些神经元按一定层次结构连接起来就得到神经网络。

## 2. 感知机与多层网络

感知机由两层神经元组成，输入层接收外界信号传递给输出层，输出层是M-P神经元，感知机能容易地实现逻辑与、或、非。

{% asset_img perceptron.jpg 感知机%}

一般的，给定训练数据集，权重$w_i(i=1,2,...,n)$以及阈值$\theta$可以通过学习得到。若将$\theta$看作一个固定输入，对训练样本$(\mathbb{x}, y)$，若当前感知机的输出为$\hat{y}$，则感知机权重调整：

$$
wi \leftarrow w_i + \Delta w_i
\\
\Delta w_i = \eta(y - \hat{y})x_i
$$

其中$\eta \in (0, 1)$称为学习率。若预测正确，则感知机不会变化，否则根据错误程度进行权重调整。感知机的求解形式非常类似线性模型中的分类，但是感知机无法实现非线性的问题求解，比如异或。

因此在输入层与输出层之间增加隐含层（至少为1），隐含层和输出层神经元都是拥有激活函数的功能神经元，这样就能对非线性问题进行拟合。

{% asset_img nn.jpg 多层前馈神经网络%}

## 3. 误差逆传播算法BP

给定训练集$D = \{ (\mathbb{x}_1, \mathbb{y}_1) , (\mathbb{x}_2, \mathbb{y}_2),...,(\mathbb{x}_m, \mathbb{y}_m)\}$，$\mathbb{x}_i \in \mathbb{R}^d ，\mathbb{y}_i \in \mathbb{R}^l$，即输入示例由$d$个属性描述，输出$l$维实值变量。

{% asset_img bp.jpg 多层前馈神经网络%}

如图神经网络结构：

* $d$个输入神经元，$l$个输出神经元，$q$个隐层神经元；
* 输出层第$j$个神经元的阈值用$\theta_j$表示，隐层第$h$个神经元的阈值用$\gamma_h$表示；
* 输入层第$i$个神经元与隐层第$h$个神经元之间的连接权为$v_{ih}$，隐层第$h$个神经元与输出层第$j$个神经元之间的连接权为$w_{hj}$；
* 隐层第$h$个神经元接收到的输入为$\alpha_h = \sum^d_{i=1}v_{ih}x_i$，输出层第$j$个神经元接收到的输入为$\beta_j = \sum^q_{h=1}w_{hj}b_h$，其中$b_h$为隐层第$h$个神经元的输出。假设激活函数均为Sigmoid。

---
对训练例$(\mathbb{x}_k, \mathbb{y}_k)$，假定神经网络的输出为$\hat{\mathbb{y}}_k = (\hat{y}^k_1,\hat{y}^k_2,...,\hat{y}^k_l)$，即

$$
\hat{y}^k_j = f(\beta_j-\theta_j)
$$
则网络在$(\mathbb{x}_k, \mathbb{y}_k)$上的均方误差MSE为

$$
E_k = \frac{1}{2}\sum^l_{j=1}(\hat{y}^k_j - y^k_j)^2，这里除以2是为了后面求导更简洁
$$

那么上图中的神经网络一共有$(d + l + 1)q + l$个参数需要确定：

* 输入层到隐层的$d \times q$个权值；
* 隐层到输出层的$q \times l$个权值；
* $q$个隐层神经元的阈值；
* $l$个输出层神经元的阈值。

同感知机的参数更新方式，对神经网络中的任意参数$v$：

$$
v \leftarrow v + \Delta v
$$

---
以隐层到输出层的连接权$w_{hj}$为例进行推导。BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整。对误差$E_k$，给定学习率$\eta$，有

$$
\Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}}
$$

根据链式法则有

$$
\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y}^k_j} \cdot \frac{\partial \hat{y}^k_j}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial w_{hj}}
$$

根据$\beta_j$的定义，显然有

$$
\frac{\partial \beta_j}{\partial w_{hj}} = b_h
$$

又因为Sigmoid函数有一个很好的性质：

$$
f'(x) = f(x) (1-f(x))
$$

于是令

$$
g_j = - \frac{\partial E_k}{\partial \hat{y}^k_j} \cdot \frac{\partial \hat{y}^k_j}{\partial \beta_j}
\\
= -(\hat{y}^k_j - y^k_j)f'(\beta_j - \theta_j)
\\
= \hat{y}^k_j(1- \hat{y}^k_j)(y^k_j - \hat{y}^k_j)
$$

代入上式，关于$w_{hj}$的更新公式

$$
\Delta w_{hj} = \eta g_j b_h
$$

类似可得

$$
\Delta \theta_j = - \eta g_j
\\
\Delta v_{ih} = \eta e_h x_i
\\
\Delta \gamma_h = - \eta e_h
$$

其中
$$
e_h = - \frac{\partial E_k}{\partial b_h} \cdot \frac{\partial b_h}{\partial \alpha_h}
\\
= - \sum^l_{j=1}\frac{\partial E_k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} f'(\alpha_h - \gamma_h)
\\
= \sum^l_{j=1}w_{hj}g_jf'(\alpha_h - \gamma_h)
\\
= b_h(1 - b_h)\sum^l_{j=1}w_{hj}g_j
$$

学习率$\eta$控制算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛过慢，最终目的是使训练误差达到一个可以接受的较小的值。

对上面的推导来说，我们只考虑了一个样本，BP算法的目标是最小化训练集$D$上的累积误差

$$
E = \frac{1}{m}\sum^m_{k=1}E_k
$$

---
* 若每次参数更新都针对单个样例，则称为标准BP算法，缺点是参数更新频繁，需要计算量大（反向传播）；
* 若在读取整个训练集$D$一遍后才对参数进行更新，则称为累计BP算法，缺点是在一段时间后，梯度下降会非常缓慢，同样也会增大计算量（迭代次数增加）；
* 一般实际中采用的方式是将数据集$D$划分为多个batch，一个batch只包含固定数量的数据，这个batch的大小视具体情况而定，我们针对一个batch更新参数，这样就能避免仅使用标准BP算法和仅使用累计BP算法的缺点。

> 理论上已经证明，一个包含足够多神经元的隐层的多层前馈神经网络能以任意精度逼近任意复杂度的连续函数。然而，确定隐层神经元的个数是个未决问题，实际中使用“试错法”调整。

既然BP神经网络有如此强大的拟合能力，那么必然会遇到过拟合的问题，解决方法是

1. 划分出验证集，根据训练集更新参数，验证集计算误差，当训练集误差下降而验证集误差上升时停止训练，并将参数返回；
2. 正则化，令误差目标函数为

$$
E = \lambda \frac{1}{m} \sum^m_{k=1}E_k + (1-\lambda)\sum_i w_i^2
\\
\lambda \in (0,1)，用于对经验误差与网络复杂度进行折中，常通过交叉验证法来估计
$$

## 4. 全局最小与局部最小

由于神经网络考虑的参数数量非常大，那么很容易会遇到全局最小与局部极小的问题，我们可以将其想象成在一个坑坑洼洼的地面，我们需要找到最深的那个坑，最深的点即是最低的损失，即全局最小，构成这个坑的参数即是我们的神经网络的最优参数；

问题在于，梯度下降对初始值很敏感，如果我们在一个坑的边缘，那么梯度下降会沿着坑的最快下降梯度进行更新，即我们会沿着这个坑掉下去，若这个坑并不是最深的，那么我们就得不到最优解，最终陷入局部极小；

在现实任务中我们可以尝试跳出局部极小：

* 多组不同的参数初始化神经网络，取最小误差的参数作为最终参数。相当于我们从不同的坑开始下降，看哪个坑最终的深度最大；
* “模拟退火”技术，即在每一步都以一定概率接受比当前解更差的结果，并且在迭代过程中，接受“次优解”的概率要随着时间的推移逐渐降低，从而保证算法稳定。相当于我们在坑中不仅仅只是单纯的下降，我们需要以一定的概率往其他方向走，这个方向可以是深度不变的平移，也可以是深度增加的上升，类似于在坑中跳动，这样可以有几率跳出局部最小；
* 随机梯度下降，在计算梯度时加入了随机因素，即便陷入局部极小点，它计算出的梯度仍可能不为0，因此有机会跳出局部极小继续搜索。

## 5. 其他常见神经网络


## 6. 深度学习


