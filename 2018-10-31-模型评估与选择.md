---
title: 模型评估与选择
date: 2018-10-31 21:44:17
categories:
- machine learning
tags:
- theory
mathjax: true
---

**论如何成为一个优秀的瓜农**

参考：

> 西瓜书第2章 模型评估与选择

## 1. 经验误差与过拟合

* 错误率（error rate）：在m个样本中有a个样本分类错误，即错误率`E = a/m`；
* 精度（accuracy）：`1 - a/m`；
* 误差（error）：模型的预测输出与样本的真实输出之间的差异；
* 训练误差（training error）：训练集上的误差；
* 泛化误差（generalization error）：新样本上的误差，或者说测试集上的误差；

* 过拟合（overfitting）：学习器把训练集中的某些特征当作一般特征，导致泛化性能下降，表现为模型在训练集上表现非常好，在测试集上表现不好；
* 欠拟合（underfitting）：学习器尚未习得样本数据集的特性，导致无法正确的预测，表现为模型在训练集和测试集上表现都不好；

**解决方法：**
* 过拟合：减少训练轮数或剪枝处理，选择泛化性能更强的学习器，很难处理；
* 欠拟合：增加训练轮数或增加分支，容易解决；

<!-- more -->

## 2. 评估方法

### 2.1 目标

> 训练集用于训练学习器，测试集用于测试学习器泛化性能，训练集与测试集不相交，且样本分布应大致相同。

### 2.2 原理
> 如果测试集与训练集相同或者是训练集的子集，那么学习器只需要朝着过拟合的方向发展就可以实现在测试集上很好的效果，但是面对新的样本，那么过拟合就成为了必然的缺陷，所以划分测试集是为了模拟面对新样本的状况，能够更好的体现学习器的泛化性能。

### 2.3 划分技巧

我们有一个包含了m个样例的数据集$D = {(x_1, y_1), (x_2, y_2),...,(x_m, y_m)}$，通过对D进行适当的处理，从中产生训练集S和测试集T。

**1. 留出法**

直接将数据集D划分为两个互斥的集合T/S，T，S互斥。一般`T : S = 5 : 1 ~ 3 : 1`。

为了尽可能的保持数据分布的一致性，我们还需要进行采样：
采样方式为“分层采样”，即在正反样本中采样比例相同，或者是多分类问题中各个种类采样比例相同。

在上面的采样中，仍然会出现问题，比如选取的样本是前XX个还是后XX个，解决方法是重复进行若干次划分，例如进行100次划分，每次都产生一个训练/测试集用于实验评估，得到的100个结果取平均。

**2. 交叉验证法**

将数据集$D$划分为k个大小相似的互斥子集$D_1~D_k$，每个子集$D_i$都尽可能的保持数据分布一致性，即从D中通过分层采样得到。每次用k - 1个子集的并集训练，剩下的那一个子集用于测试；这样就可以进行k次训练和测试，最后取均值。k常取10，也叫“k折交叉验证”

同样在分层采样的过程中也常常需要重复若干次划分，同上，因此常见的有“10次10折交叉验证”。

交叉验证的特例：留一法

* 优点：评估结果最准确；

* 缺点：训练的模型数等于数据集样本数，如果数据集包括百万数据，那么需要训练百万个模型，训练成本难以忍受。

**3. 自助法**

给定包含m个样本的数据集D，每次从D中随机挑选一个样本放入$D'$（复制不是剪切），重复执行m次，得到一个包含m个样本的数据集$D'$，我们可以将$D'$作为训练集，D\$D'$作为测试集。

* $D'$中的样本有重复
* D中大约有36.8%的样本未出现在数据集$D'$中
* 自助法在数据集较小/难以有效划分训练/测试集时很有用，但是自助法产生的数据集改变了初始数据集的分布，会引入估计偏差，因此，在数据量足够时，留出法和交叉验证法更常用

**4. 调参**

* 对每一个参数选定一个范围和变化步长
* 模型评估阶段需要划分测试集，最终提交给用户的模型在训练过程中不需要划分测试集，直接使用全部数据
* 在研究阶段，将数据集划分为训练集、验证集和测试集，测试集用于评估模型在实际使用时的泛化能力，验证集用于模型选择和调参

## 3. 性能度量

### 3.1 常见性能度量函数

**1. 均方误差MSE（Mean Squard Error）**

$$
E(f; D)) = \frac{1}{m} \sum_{i=1}^{m} (f(\boldsymbol{x}_i) - y_i) ^ 2
$$

对于数据分布$D$和概率密度函数$p(·)$，均方误差为
$$ 
E(f; D) = \int_{\boldsymbol{x} \sim  D}^{ } (f(\boldsymbol{x}) - y)^2p(\boldsymbol{x})d\boldsymbol{x}
$$

**2. 均方根误差RMSE（Root Mean Squard Error）**

$$ 
E(f; D)) = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (f(\boldsymbol{x}_i) - y_i) ^ 2}
$$

**3. 平均绝对误差MAE（Mean Absolute Error）**

$$  
E(f; D)) = \frac{1}{m} \sum_{i=1}^{m} \left | f(\boldsymbol{x}_i) - y_i \right |
$$

区别：
* MSE和RMSE放大数据波动，预测效果不好的值产生的残差会被放大；
* RMSE和MAE的数量级和单位与数据相同，因此可以直观的显示模型性能，但是RMSE更关注错误情况。

### 3.2 查准率、查全率与F1

分类结果混淆矩阵

|真实情况/预测结果|正例|反例|
|----|----|----|
|正例|TP（真正例）|FN（假反例）|
|反例|FP（假正例）|TN（真反例）|

* 查准率P（precision）

$$
P = \frac{TP}{ TP + FP}
$$

* 查全率R（recall）

$$  
R = \frac{TP}{TP + FN}
$$

<!-- ![](images/pr.png) -->
{% asset_img pr.png model %}

查准率与查全率是一对矛盾的度量，如果一条曲线可以将另一条曲线完全“包裹”，则可以判断后者的模型性能比前者弱；如果两条曲线相交，那么可以使用“平衡点”（BEP），即当$P = R$时的值，谁大谁更优，但是BEP还是过于简单，更常用的时F1度量：

$$  
F1 = \frac{2 \times P \times R}{P + R} = \frac{2 \times TP}{样例总数 + TP - TN}
$$

上式中我们对查准率和查全率没有偏向，但是在实际使用中，我们可能会更关注两者之一，那么就需要对其进行加权， $\beta > 1$时查全率有更大影响；$\beta < 1$时查准率有更大影响：

$$  
F_{\beta} = \frac{(1 + \beta ^2) \times P \times R}{(\beta ^2 \times P) + R}
$$

### 3.3 ROC与AUC

很多学习器为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则反类；根据阈值设定的大小，我们决定更加偏向于查准率还是查全率，比如，如果更重视查准率，则提高分类阈值。ROC曲线则是从这个角度出发来研究学习器泛化性能的有力工具。

ROC纵轴“真正例率”（TPR）

$$  
TPR = \frac{TP}{TP + FN}
$$

ROC横轴“假正例率”（FPR）

$$  
FPR = \frac{FP}{TN + FP}
$$

当ROC曲线越接近左边和上边时，模型性能越好。

<!-- ![](images/roc.jpg) -->
{% asset_img roc.jpg model %}

实际过程中的ROC曲线不是平滑的，绘制过程：给定$m^+$个正例和$m^-$个反例，根据学习器预测结果排序，然后将分类阈值设置为最大，然后分类阈值依次设为每个样例的预测值，设前一个标记点的坐标为$(x, y)$，若当前为真正例，则对应的标记点坐标为$(x, y + \frac{1}{m^+})$，若当前为假正例，则标记点为$(x + \frac{1}{m^-}, y)$；分类阈值最大时对应所有样例均预测为反例，TPR和FPR均为0

AUC为ROC曲线下的面积

$$  
AUC = \frac{1}{2} \sum_{i=1}^{m-1}(x_{i+1} - x_i) \cdot (y_i + y_{i+1})
$$

形式化的看，AUC考虑的是样本预测的排序质量，因此它与排序误差有紧密联系。给定$m^+$个正例和$m^-$个反例，令$D^+$和$D^-$分别表示正、反例集合，则排序损失定义为：

$$  
l_{rank} = \frac{1}{m^+m^-} \sum_{\boldsymbol{x}^+ \in D^+}\sum_{\boldsymbol{x}^- \in D^-}(\mathbb{I}(f(\boldsymbol{x}^+) < f(\boldsymbol{x}^-)) + \frac{1}{2}\mathbb{I}(f(\boldsymbol{x}^+) = f(\boldsymbol{x}^-)))
$$

即考虑每一对正、反例，若正例的预测值小于反例，则记一个罚分，若相等，记0.5个罚分。因此$l_{rank}$对应ROC曲线之上的面积

$$  
AUC = 1 - l_{rank}
$$

### 3.4 代价敏感错误率与代价曲线

在实际分类问题中，把健康人诊断为患者和把患者诊断为健康人的代价是完全不同的，因此需要一个代价矩阵对分类错误加权。

二分类代价矩阵

|真实类别/预测类别|第0类|第1类|
|----|----|----|
|第0类|0|cost01|
|第1类|cost10|0|

代价敏感错误率为

$$  
E(f;D;cost) = \frac{1}{m}(\sum_{\boldsymbol{x}_i \in D^+}\mathbb{I}(f(\boldsymbol{x}_i) \neq y_i) \times cost_{01} + \sum_{\boldsymbol{x}_i \in D^-}\mathbb{I}(f(\boldsymbol{x}_i) \neq y_i) \times cost_{10})
$$

在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，而“代价曲线”则可达到该目的，代价曲线图的横轴是取值为$[0, 1]$的正例率代价

$$  
P(+)cost = \frac{p \times cost_{01}}{p \times cost_{01} + (1 - p) \times cost_{10}}
$$

其中p是样例为正例的概率；纵轴是取值为$[0, 1]$的归一化代价

$$  
cost_{norm} = \frac{FNR \times p \times cost_{01} + FPR \times (1 - p) \times cost_{10}}{p \times cost_{01} + (1-p) \times cost_{10}}
$$

FPR是假正例率，$FNR = 1 - TPR$是假反例率

<!-- ![](images/cost.jpg) -->
{% asset_img cost.jpg model %}


## 4. 偏差与方差

学习算法的期望预测为

$$  
\overline{f}(\boldsymbol{x}) = \mathbb{E}_D[f(\boldsymbol{x};D)]
$$

使用样本数相同的不同训练集产生的方差为

$$  
var(\boldsymbol{x}) = \mathbb{E}_D[(f(\boldsymbol{x};D) - \overline{f}(\boldsymbol{x}))^2]
$$

噪声为

$$  
\varepsilon ^2 = \mathbb{E}_D[(y_D - y)^2]
$$

期望输出与真实标记的差别为偏差（bias）

$$  
bias^2(\boldsymbol{x}) = (\overline{f}(\boldsymbol{x}) - y)^2
$$

假定噪声期望为0，即$\mathbb{E}[y_D - y] = 0$，通过简单的计算可对算法的期望泛化误差进行分解：

$$  
E(f;D) = \mathbb{E}_D[(f(\boldsymbol{x};D) - y_D)^2]
$$
$$  
= \mathbb{E}_D[(f(\boldsymbol{x};D) - \overline{f}(\boldsymbol{x}) + \overline{f}(\boldsymbol{x}) - y_D)^2]
$$
$$  
...
$$
$$  
= \mathbb{E}_D[(f(\boldsymbol{x};D) - \overline{f}(\boldsymbol{x}))^2] + (\overline{f}(\boldsymbol{x}) - y)^2 + \mathbb{E}[(y_D - y)^2]
$$

于是，
$$  
E(f;D) = bias^2(\boldsymbol{x}) + var(\boldsymbol{x}) + \varepsilon^2
$$

也就是说，泛化误差可分解为偏差、方差和噪声之和。

* 偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；
* 方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；
* 噪声则表达了学习问题本身的难度

<!-- ![](images/error.jpg) -->
{% asset_img error.jpg model %}
